{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual implementation of `fwd()` to extract LayerNorm inputs & outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:55:34.939249Z",
     "start_time": "2024-04-26T20:54:09.526700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Apply Top-K by magniture to LayerNorm and Residual connection - the densifying operations in a Transformer\n",
    "class TopKLayerNormGPT2Block(GPT2Block):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__(config, layer_idx=None)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states  # Identical debug to GPT2Block so far\n",
    "        hidden_states = self.ln_1(hidden_states) # Also identical\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]  # This is where differences start\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GPT2TopKModel(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([TopKLayerNormGPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Correctness against stock GPT-2 at same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Token Embeddings output match\n",
      "State Dicts match\n",
      "Attention parameters match\n",
      "First Hidden State output match\n",
      "original_output:  tensor([[[-1.5082,  0.6068,  1.0091,  ...,  0.6132, -0.0694, -0.1872],\n",
      "         [-0.0204,  0.4613,  0.2522,  ...,  0.7203,  0.0367,  0.0335],\n",
      "         [-0.3271,  1.8532, -0.1924,  ..., -0.4243, -0.1610, -0.5711],\n",
      "         [-0.5064,  0.1800, -1.0481,  ...,  0.4531, -0.6576, -0.7522],\n",
      "         [-0.3700,  0.9147, -1.0633,  ...,  0.0647,  0.0035, -0.0753],\n",
      "         [-0.5584,  1.6094, -0.6073,  ...,  0.5091,  0.6707, -0.1499]]])\n",
      "modified_output:  tensor([[[-1.5082,  0.6068,  1.0091,  ...,  0.6132, -0.0694, -0.1872],\n",
      "         [-0.0204,  0.4613,  0.2522,  ...,  0.7203,  0.0367,  0.0335],\n",
      "         [-0.3271,  1.8532, -0.1924,  ..., -0.4243, -0.1610, -0.5711],\n",
      "         [-0.5064,  0.1800, -1.0481,  ...,  0.4531, -0.6576, -0.7522],\n",
      "         [-0.3700,  0.9147, -1.0633,  ...,  0.0647,  0.0035, -0.0753],\n",
      "         [-0.5584,  1.6094, -0.6073,  ...,  0.5091,  0.6707, -0.1499]]])\n"
     ]
    }
   ],
   "source": [
    "def compare_model_params(model1, model2):\n",
    "    # Disable non-deterministic behavior\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Ensure both models start with the same weights\n",
    "    model2.load_state_dict(model1.state_dict())\n",
    "    model2.h[0].load_state_dict(model1.h[0].state_dict())\n",
    "\n",
    "    # Compare all named parameters\n",
    "    params_dict_original = {name: param for name, param in model1.named_parameters()}\n",
    "    params_dict_modified = {name: param for name, param in model2.named_parameters()}\n",
    "\n",
    "    # Now you can access by name\n",
    "    for name in params_dict_original:\n",
    "        assert torch.allclose(params_dict_original[name], params_dict_modified[name], atol=1e-6), f\"Mismatch found in parameter: {name}\"\n",
    "\n",
    "\n",
    "    def compare_state_dicts(dict1, dict2):\n",
    "        # Check if both dictionaries have the same set of keys (parameter names)\n",
    "        assert dict1.keys() == dict2.keys(), \"State dicts have different sets of parameters.\"\n",
    "        \n",
    "        # Check if all tensors in both dictionaries are close\n",
    "        for key in dict1:\n",
    "            assert torch.allclose(dict1[key], dict2[key], atol=1e-6), f\"Mismatch found in parameter: {key}\"\n",
    "\n",
    "    # Prepare input\n",
    "    input_ids = torch.tensor([[12100, 242, 508, 318, 13, 198]])\n",
    "\n",
    "    hidden_states_original = model1.wte(input_ids)\n",
    "    hidden_states_modified = model2.wte(input_ids)\n",
    "    assert torch.allclose(hidden_states_original, hidden_states_modified, atol=1e-6), \"Embedding output mismatch\"\n",
    "    print(\"Word Token Embeddings output match\")\n",
    "\n",
    "    # modified_model.load_state_dict(original_model.state_dict())\n",
    "    # modified_model.h[0].load_state_dict(original_model.h[0].state_dict())\n",
    "\n",
    "    # Compare state dicts\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    compare_state_dicts(state_dict1, state_dict2)\n",
    "    print(\"State Dicts match\")\n",
    "\n",
    "\n",
    "    def compare_attention_params(model1, model2, layer_index=0):\n",
    "        attn1 = model1.h[layer_index].attn\n",
    "        attn2 = model2.h[layer_index].attn\n",
    "        \n",
    "        # Convert generators to dictionaries\n",
    "        params1 = dict(attn1.named_parameters())\n",
    "        params2 = dict(attn2.named_parameters())\n",
    "\n",
    "        # Now params1 and params2 are dictionaries that can be accessed by keys\n",
    "        for name in params1:\n",
    "            param1 = params1[name]\n",
    "            param2 = params2[name]\n",
    "            if not torch.allclose(param1, param2, atol=1e-6):\n",
    "                print(f\"Mismatch in {name}\")\n",
    "                print(f\"Model1: {param1}\")\n",
    "                print(f\"Model2: {param2}\")\n",
    "\n",
    "    # Call this function with your models\n",
    "    for i in range(12):\n",
    "        compare_attention_params(model1, model2, i)\n",
    "    print(\"Attention parameters match\")\n",
    "\n",
    "\n",
    "    original_h0 = model1.h[0](hidden_states_original)[0]\n",
    "    modified_h0 = model2.h[0](hidden_states_modified)[0]  # Input is the same. These are still different\n",
    "    # modified_h0 = modified_model.h[0](hidden_states_modified)[0]\n",
    "    assert torch.allclose(original_h0, modified_h0, atol=1e-6), \"First Hidden State output mismatch\"\n",
    "    print(\"First Hidden State output match\")\n",
    "\n",
    "    model1.h[0].attn.c_attn.weight\n",
    "\n",
    "\n",
    "    # Get outputs from both models\n",
    "    with torch.no_grad():\n",
    "        original_output = model1(input_ids)[0]\n",
    "        modified_output = model2(input_ids)[0]\n",
    "\n",
    "    # at = model2(input_ids)#.attn\n",
    "    # Compare outputs\n",
    "    # print(\"Difference between original and modified model outputs:\", torch.sum((original_output - modified_output) ** 2))\n",
    "    print(\"original_output: \", original_output)\n",
    "    print(\"modified_output: \", modified_output)\n",
    "\n",
    "    assert torch.allclose(original_output, modified_output, atol=1e-6), \"Final output mismatch\"\n",
    "    print(\"Final output matches\")\n",
    "\n",
    "\n",
    "config = GPT2Config()\n",
    "model1 = GPT2Model(config).eval()\n",
    "# modified_model = GPT2Model(config).eval()  # Assigning h[0] = CustomGPT2Block does not work!\n",
    "model2 = GPT2TopKModel(config).eval()\n",
    "\n",
    "compare_model_params(model1, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example model loading and modification\n",
    "# from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# config = GPT2Config()\n",
    "# model = GPT2Model(config)\n",
    "# model.h[0] = TopKLayerNormGPT2Block(config)  # Replace the first block for demonstration\n",
    "\n",
    "# # Example forward pass\n",
    "# input_ids = tokenizer.encode(\"ariel is a cs phd student\", return_tensors=\"pt\")\n",
    "# outputs = model(input_ids)\n",
    "\n",
    "# # Extract LayerNorm inputs\n",
    "# ln_inputs = outputs[-1][-1]  # Assuming only the first block was replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_top_n_tiny_shakespeare\n",
    "\n",
    "longest_shakespeare = get_top_n_tiny_shakespeare(1, mode=\"longest\")[0]['Text']\n",
    "longest_shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(longest_shakespeare, return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract LayerNorm inputs\n",
    "ln_inputs = outputs[-1][-1]\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
