{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ChatGPT-4\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model\n",
    "import torch\n",
    "\n",
    "class EarlyExitingGPT2Model(GPT2Model):\n",
    "    def __init__(self, config, exit_layer):\n",
    "        super().__init__(config)\n",
    "        self.exit_layer = exit_layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_states = self.wte(input_ids)  # Word token embeddings\n",
    "        for i, block in enumerate(self.h):\n",
    "            hidden_states = block(hidden_states)[0]\n",
    "            if i == self.exit_layer - 1:\n",
    "                break\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hiddenstates_attn(text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    print(\"Nr. of input tokens: \", len(input_ids))\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids)\n",
    "\n",
    "    hidden_states = output.hidden_states\n",
    "    attentions = output.attentions\n",
    "    return hidden_states, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration and create a new model instance\n",
    "config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "# early_exiting_model = EarlyExitingGPT2Model(config, exit_layer=12)\n",
    "gpt2_medium_model = GPT2Model.from_pretrained('gpt2-medium', output_attentions=True, output_hidden_states=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "text = \"Hello, my dog is cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 1024,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 24,\n",
       "  \"n_positions\": 1024,\n",
       "  \"n_special\": 0,\n",
       "  \"predict_special_tokens\": true,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.40.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "GPT2Model                                     --\n",
       "├─Embedding: 1-1                              51,463,168\n",
       "├─Embedding: 1-2                              1,048,576\n",
       "├─Dropout: 1-3                                --\n",
       "├─ModuleList: 1-4                             --\n",
       "│    └─GPT2Block: 2-1                         --\n",
       "│    │    └─LayerNorm: 3-1                    2,048\n",
       "│    │    └─GPT2Attention: 3-2                4,198,400\n",
       "│    │    └─LayerNorm: 3-3                    2,048\n",
       "│    │    └─GPT2MLP: 3-4                      8,393,728\n",
       "│    └─GPT2Block: 2-2                         --\n",
       "│    │    └─LayerNorm: 3-5                    2,048\n",
       "│    │    └─GPT2Attention: 3-6                4,198,400\n",
       "│    │    └─LayerNorm: 3-7                    2,048\n",
       "│    │    └─GPT2MLP: 3-8                      8,393,728\n",
       "│    └─GPT2Block: 2-3                         --\n",
       "│    │    └─LayerNorm: 3-9                    2,048\n",
       "│    │    └─GPT2Attention: 3-10               4,198,400\n",
       "│    │    └─LayerNorm: 3-11                   2,048\n",
       "│    │    └─GPT2MLP: 3-12                     8,393,728\n",
       "│    └─GPT2Block: 2-4                         --\n",
       "│    │    └─LayerNorm: 3-13                   2,048\n",
       "│    │    └─GPT2Attention: 3-14               4,198,400\n",
       "│    │    └─LayerNorm: 3-15                   2,048\n",
       "│    │    └─GPT2MLP: 3-16                     8,393,728\n",
       "│    └─GPT2Block: 2-5                         --\n",
       "│    │    └─LayerNorm: 3-17                   2,048\n",
       "│    │    └─GPT2Attention: 3-18               4,198,400\n",
       "│    │    └─LayerNorm: 3-19                   2,048\n",
       "│    │    └─GPT2MLP: 3-20                     8,393,728\n",
       "│    └─GPT2Block: 2-6                         --\n",
       "│    │    └─LayerNorm: 3-21                   2,048\n",
       "│    │    └─GPT2Attention: 3-22               4,198,400\n",
       "│    │    └─LayerNorm: 3-23                   2,048\n",
       "│    │    └─GPT2MLP: 3-24                     8,393,728\n",
       "│    └─GPT2Block: 2-7                         --\n",
       "│    │    └─LayerNorm: 3-25                   2,048\n",
       "│    │    └─GPT2Attention: 3-26               4,198,400\n",
       "│    │    └─LayerNorm: 3-27                   2,048\n",
       "│    │    └─GPT2MLP: 3-28                     8,393,728\n",
       "│    └─GPT2Block: 2-8                         --\n",
       "│    │    └─LayerNorm: 3-29                   2,048\n",
       "│    │    └─GPT2Attention: 3-30               4,198,400\n",
       "│    │    └─LayerNorm: 3-31                   2,048\n",
       "│    │    └─GPT2MLP: 3-32                     8,393,728\n",
       "│    └─GPT2Block: 2-9                         --\n",
       "│    │    └─LayerNorm: 3-33                   2,048\n",
       "│    │    └─GPT2Attention: 3-34               4,198,400\n",
       "│    │    └─LayerNorm: 3-35                   2,048\n",
       "│    │    └─GPT2MLP: 3-36                     8,393,728\n",
       "│    └─GPT2Block: 2-10                        --\n",
       "│    │    └─LayerNorm: 3-37                   2,048\n",
       "│    │    └─GPT2Attention: 3-38               4,198,400\n",
       "│    │    └─LayerNorm: 3-39                   2,048\n",
       "│    │    └─GPT2MLP: 3-40                     8,393,728\n",
       "│    └─GPT2Block: 2-11                        --\n",
       "│    │    └─LayerNorm: 3-41                   2,048\n",
       "│    │    └─GPT2Attention: 3-42               4,198,400\n",
       "│    │    └─LayerNorm: 3-43                   2,048\n",
       "│    │    └─GPT2MLP: 3-44                     8,393,728\n",
       "│    └─GPT2Block: 2-12                        --\n",
       "│    │    └─LayerNorm: 3-45                   2,048\n",
       "│    │    └─GPT2Attention: 3-46               4,198,400\n",
       "│    │    └─LayerNorm: 3-47                   2,048\n",
       "│    │    └─GPT2MLP: 3-48                     8,393,728\n",
       "│    └─GPT2Block: 2-13                        --\n",
       "│    │    └─LayerNorm: 3-49                   2,048\n",
       "│    │    └─GPT2Attention: 3-50               4,198,400\n",
       "│    │    └─LayerNorm: 3-51                   2,048\n",
       "│    │    └─GPT2MLP: 3-52                     8,393,728\n",
       "│    └─GPT2Block: 2-14                        --\n",
       "│    │    └─LayerNorm: 3-53                   2,048\n",
       "│    │    └─GPT2Attention: 3-54               4,198,400\n",
       "│    │    └─LayerNorm: 3-55                   2,048\n",
       "│    │    └─GPT2MLP: 3-56                     8,393,728\n",
       "│    └─GPT2Block: 2-15                        --\n",
       "│    │    └─LayerNorm: 3-57                   2,048\n",
       "│    │    └─GPT2Attention: 3-58               4,198,400\n",
       "│    │    └─LayerNorm: 3-59                   2,048\n",
       "│    │    └─GPT2MLP: 3-60                     8,393,728\n",
       "│    └─GPT2Block: 2-16                        --\n",
       "│    │    └─LayerNorm: 3-61                   2,048\n",
       "│    │    └─GPT2Attention: 3-62               4,198,400\n",
       "│    │    └─LayerNorm: 3-63                   2,048\n",
       "│    │    └─GPT2MLP: 3-64                     8,393,728\n",
       "│    └─GPT2Block: 2-17                        --\n",
       "│    │    └─LayerNorm: 3-65                   2,048\n",
       "│    │    └─GPT2Attention: 3-66               4,198,400\n",
       "│    │    └─LayerNorm: 3-67                   2,048\n",
       "│    │    └─GPT2MLP: 3-68                     8,393,728\n",
       "│    └─GPT2Block: 2-18                        --\n",
       "│    │    └─LayerNorm: 3-69                   2,048\n",
       "│    │    └─GPT2Attention: 3-70               4,198,400\n",
       "│    │    └─LayerNorm: 3-71                   2,048\n",
       "│    │    └─GPT2MLP: 3-72                     8,393,728\n",
       "│    └─GPT2Block: 2-19                        --\n",
       "│    │    └─LayerNorm: 3-73                   2,048\n",
       "│    │    └─GPT2Attention: 3-74               4,198,400\n",
       "│    │    └─LayerNorm: 3-75                   2,048\n",
       "│    │    └─GPT2MLP: 3-76                     8,393,728\n",
       "│    └─GPT2Block: 2-20                        --\n",
       "│    │    └─LayerNorm: 3-77                   2,048\n",
       "│    │    └─GPT2Attention: 3-78               4,198,400\n",
       "│    │    └─LayerNorm: 3-79                   2,048\n",
       "│    │    └─GPT2MLP: 3-80                     8,393,728\n",
       "│    └─GPT2Block: 2-21                        --\n",
       "│    │    └─LayerNorm: 3-81                   2,048\n",
       "│    │    └─GPT2Attention: 3-82               4,198,400\n",
       "│    │    └─LayerNorm: 3-83                   2,048\n",
       "│    │    └─GPT2MLP: 3-84                     8,393,728\n",
       "│    └─GPT2Block: 2-22                        --\n",
       "│    │    └─LayerNorm: 3-85                   2,048\n",
       "│    │    └─GPT2Attention: 3-86               4,198,400\n",
       "│    │    └─LayerNorm: 3-87                   2,048\n",
       "│    │    └─GPT2MLP: 3-88                     8,393,728\n",
       "│    └─GPT2Block: 2-23                        --\n",
       "│    │    └─LayerNorm: 3-89                   2,048\n",
       "│    │    └─GPT2Attention: 3-90               4,198,400\n",
       "│    │    └─LayerNorm: 3-91                   2,048\n",
       "│    │    └─GPT2MLP: 3-92                     8,393,728\n",
       "│    └─GPT2Block: 2-24                        --\n",
       "│    │    └─LayerNorm: 3-93                   2,048\n",
       "│    │    └─GPT2Attention: 3-94               4,198,400\n",
       "│    │    └─LayerNorm: 3-95                   2,048\n",
       "│    │    └─GPT2MLP: 3-96                     8,393,728\n",
       "├─LayerNorm: 1-5                              2,048\n",
       "======================================================================\n",
       "Total params: 354,823,168\n",
       "Trainable params: 354,823,168\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary as ti_summary\n",
    "\n",
    "ti_summary(gpt2_medium_model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of input tokens:  1\n"
     ]
    }
   ],
   "source": [
    "# Attn is the score just before the *V\n",
    "hidden_states, attn = get_hiddenstates_attn(text, gpt2_medium_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention is $n\\times n$ since it's a pairwise importance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 6, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:55:34.939249Z",
     "start_time": "2024-04-26T20:54:09.526700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm Inputs: (tensor([[[[ 9.0784e-01, -1.3208e+00,  3.7702e-01,  ...,  7.3676e-01,\n",
      "            6.8704e-01,  2.1352e-01],\n",
      "          [ 4.5032e-01, -1.3441e+00,  4.0242e-02,  ...,  7.3500e-01,\n",
      "            1.1475e+00,  1.2623e-01],\n",
      "          [ 9.5168e-01, -1.5631e+00, -1.9853e-01,  ...,  1.1555e-01,\n",
      "            1.7924e+00, -6.9798e-02],\n",
      "          ...,\n",
      "          [ 1.4890e-01, -9.8778e-01,  7.9761e-01,  ...,  5.7417e-02,\n",
      "            1.3985e+00,  8.3555e-02],\n",
      "          [ 9.7711e-02, -4.5645e-01,  9.0713e-01,  ..., -1.0180e-01,\n",
      "            1.0865e+00,  5.5014e-01],\n",
      "          [-2.8885e-01, -1.3738e-01,  1.0511e+00,  ..., -3.1056e-02,\n",
      "            4.5944e-01, -1.9303e-01]],\n",
      "\n",
      "         [[ 2.1683e-01, -4.5429e-01,  1.8653e-01,  ..., -1.4401e-01,\n",
      "           -8.6371e-02, -6.9943e-01],\n",
      "          [-2.1272e-01, -1.7721e-01, -9.4751e-02,  ..., -2.9946e-01,\n",
      "           -4.1352e-01, -1.2405e+00],\n",
      "          [ 4.8012e-01, -3.2333e-01, -5.4602e-01,  ...,  4.0491e-01,\n",
      "           -5.1088e-02, -5.5999e-01],\n",
      "          ...,\n",
      "          [ 8.0977e-01, -4.9210e-01,  6.7420e-01,  ..., -7.0718e-02,\n",
      "           -4.4300e-01,  6.9049e-01],\n",
      "          [ 5.8146e-01,  1.8711e-01,  3.3056e-01,  ..., -1.2232e-01,\n",
      "           -3.8621e-01,  4.8170e-01],\n",
      "          [ 6.7005e-01, -5.0606e-01,  3.0707e-01,  ..., -5.7002e-02,\n",
      "           -2.9961e-01,  7.4849e-01]],\n",
      "\n",
      "         [[-3.7159e-01,  2.3225e-01,  1.9105e-01,  ...,  5.0492e-01,\n",
      "            1.6226e-01,  5.9979e-01],\n",
      "          [-8.0046e-02,  2.9587e-01,  1.2674e-01,  ...,  8.6551e-01,\n",
      "            6.6510e-01,  1.0679e+00],\n",
      "          [ 3.1182e-01, -7.1385e-01, -4.2835e-01,  ...,  9.0021e-01,\n",
      "            2.3316e-01,  7.0111e-01],\n",
      "          ...,\n",
      "          [ 2.8860e-01, -3.0813e-01,  4.5846e-01,  ...,  3.9210e-01,\n",
      "           -7.5594e-01,  8.2363e-01],\n",
      "          [-3.1472e-01, -2.4203e-01,  5.0158e-01,  ...,  5.7281e-01,\n",
      "           -8.4047e-01,  3.2549e-01],\n",
      "          [-1.1575e+00, -9.1383e-01,  4.8295e-02,  ...,  3.5111e-01,\n",
      "           -1.2638e+00,  8.6432e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7892e-01, -1.3646e-01,  1.0804e-01,  ...,  2.1709e-01,\n",
      "           -1.7648e-02, -8.5688e-01],\n",
      "          [-2.7067e-01,  1.8808e-01, -2.2818e-02,  ..., -5.5542e-01,\n",
      "           -3.3874e-01, -3.5068e-01],\n",
      "          [ 7.3328e-02,  1.4137e-01, -4.0103e-01,  ..., -5.3428e-03,\n",
      "           -2.6450e-01,  7.4736e-03],\n",
      "          ...,\n",
      "          [-5.9486e-02,  1.0440e-01,  5.2965e-01,  ...,  3.8454e-01,\n",
      "           -3.9710e-01, -7.3070e-01],\n",
      "          [ 2.3745e-01, -5.2887e-01,  5.0799e-01,  ...,  3.8746e-02,\n",
      "           -1.5419e-01, -3.6217e-01],\n",
      "          [-1.1898e-02, -4.1996e-01,  9.4840e-01,  ..., -7.3906e-02,\n",
      "            3.2321e-01, -6.6459e-01]],\n",
      "\n",
      "         [[ 3.4228e-01, -1.6258e-01, -1.2701e-01,  ..., -4.5535e-01,\n",
      "            4.5504e-01,  3.3971e-01],\n",
      "          [-8.9315e-02,  2.7674e-02,  4.1755e-01,  ..., -2.9795e-01,\n",
      "            6.3227e-01,  5.3884e-01],\n",
      "          [ 4.4046e-02, -8.8886e-02,  6.0083e-01,  ..., -5.5132e-01,\n",
      "            1.5692e-01,  1.5348e-01],\n",
      "          ...,\n",
      "          [-6.3702e-01, -5.1478e-01,  2.2325e-01,  ...,  6.8936e-02,\n",
      "            3.3062e-01, -4.0013e-02],\n",
      "          [-1.1902e-01, -6.5818e-01,  3.1679e-01,  ...,  8.4324e-02,\n",
      "           -2.4435e-02,  6.5980e-01],\n",
      "          [-6.4748e-01, -1.8383e-01,  3.6148e-01,  ...,  5.9361e-01,\n",
      "            1.5586e-01,  1.0571e+00]],\n",
      "\n",
      "         [[-6.1262e-01, -6.9303e-01,  8.6686e-01,  ...,  4.1541e-01,\n",
      "            7.8708e-01,  2.9665e-01],\n",
      "          [-7.0302e-01, -3.3967e-01,  3.4078e-01,  ...,  6.5264e-01,\n",
      "            1.8367e-01,  2.8275e-01],\n",
      "          [-1.8161e-01, -7.0732e-01,  2.7464e-01,  ..., -9.5650e-01,\n",
      "           -9.6778e-04, -2.6492e-01],\n",
      "          ...,\n",
      "          [-5.1473e-02,  3.0891e-02, -1.0479e+00,  ..., -2.5811e-01,\n",
      "           -2.9872e-01, -2.8209e-01],\n",
      "          [-4.5351e-01, -2.6492e-01,  1.3038e-01,  ..., -2.8337e-01,\n",
      "           -1.3297e-01,  4.5490e-01],\n",
      "          [-8.6348e-02,  3.0818e-01, -2.2141e-01,  ...,  3.6184e-01,\n",
      "           -1.2852e-01,  9.6304e-02]]]], grad_fn=<PermuteBackward0>), tensor([[[[-1.2684e+00,  4.6373e-01, -1.0136e+00,  ..., -6.5740e-01,\n",
      "            8.6075e-01, -6.2183e-02],\n",
      "          [ 3.9639e-01,  7.4882e-01, -5.8444e-01,  ..., -7.8031e-01,\n",
      "           -2.5525e-01, -8.2121e-01],\n",
      "          [-1.0360e+00, -2.6341e-01, -5.8624e-01,  ..., -7.7996e-01,\n",
      "            2.2134e-01, -1.0314e+00],\n",
      "          ...,\n",
      "          [ 2.4534e-01, -1.0971e-01, -5.2543e-01,  ..., -1.2850e+00,\n",
      "           -2.1493e-02, -7.5781e-01],\n",
      "          [-3.8376e-01, -3.5432e-01, -4.7100e-01,  ..., -1.1210e+00,\n",
      "            3.1904e-01, -8.4911e-02],\n",
      "          [ 2.1420e-01,  2.0044e-02, -8.4885e-01,  ..., -9.6841e-01,\n",
      "           -7.5227e-01, -9.8046e-04]],\n",
      "\n",
      "         [[ 7.2822e-01,  4.4375e-01, -8.3650e-01,  ...,  9.0382e-01,\n",
      "            9.4080e-02, -1.7193e-01],\n",
      "          [ 3.5007e-01,  6.7440e-02, -2.7417e-01,  ...,  2.5640e-01,\n",
      "            1.5303e-01, -8.6335e-02],\n",
      "          [ 4.5951e-01,  6.5302e-01,  7.0046e-01,  ...,  4.1431e-01,\n",
      "            6.1619e-01, -6.7604e-01],\n",
      "          ...,\n",
      "          [-3.4837e-01,  6.2422e-01,  1.1359e+00,  ..., -3.2485e-01,\n",
      "            3.9157e-01,  2.5632e-01],\n",
      "          [-4.1625e-02,  7.5939e-01,  4.6621e-01,  ..., -6.3106e-01,\n",
      "            6.3114e-01,  1.1867e-01],\n",
      "          [-3.2608e-01,  5.7141e-01, -1.2460e-01,  ..., -3.5410e-01,\n",
      "           -2.5944e-01,  1.9083e-02]],\n",
      "\n",
      "         [[ 4.9568e-01, -4.0004e-01,  3.1007e-01,  ...,  6.4937e-01,\n",
      "            2.6166e-01, -6.6341e-01],\n",
      "          [ 3.7954e-01, -4.7239e-01, -9.6902e-02,  ...,  1.3886e+00,\n",
      "            2.5890e-01, -7.3431e-01],\n",
      "          [ 8.7012e-01, -3.5112e-02,  4.2006e-02,  ...,  6.3071e-01,\n",
      "            9.2639e-01, -9.7282e-01],\n",
      "          ...,\n",
      "          [ 9.8669e-01,  2.9047e-01, -2.2067e-01,  ..., -2.9842e-01,\n",
      "            1.2120e+00, -7.0004e-01],\n",
      "          [ 1.0680e+00,  3.6572e-01, -4.3494e-01,  ..., -6.6667e-01,\n",
      "            9.1979e-01,  3.2266e-01],\n",
      "          [ 9.9495e-01,  4.5058e-01, -2.4170e-01,  ...,  6.2075e-02,\n",
      "            8.0412e-01, -2.1705e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3049e-02, -6.4659e-02,  1.1155e-01,  ..., -1.3061e+00,\n",
      "           -1.2188e-01,  7.5201e-01],\n",
      "          [ 1.0970e-01, -9.2243e-01, -2.5592e-01,  ..., -7.1549e-01,\n",
      "           -5.3742e-01,  1.3174e+00],\n",
      "          [-1.2551e-02, -9.6695e-01, -2.8424e-01,  ..., -7.9219e-01,\n",
      "           -9.9027e-01,  1.9379e+00],\n",
      "          ...,\n",
      "          [-3.6519e-01, -1.1976e-01, -4.0104e-01,  ..., -2.0382e-01,\n",
      "           -1.3956e+00,  1.0244e+00],\n",
      "          [-7.2304e-01, -1.0304e-01, -9.3977e-02,  ...,  2.6825e-01,\n",
      "           -2.4261e-01,  2.7062e-01],\n",
      "          [-3.4870e-01,  4.6112e-02, -2.7729e-02,  ..., -6.4395e-01,\n",
      "           -1.3689e+00,  9.9252e-01]],\n",
      "\n",
      "         [[-2.8316e-01,  7.9706e-02, -4.8593e-01,  ...,  6.4356e-02,\n",
      "           -2.9007e-01, -3.3506e-01],\n",
      "          [-1.8025e-02,  7.3791e-02,  2.0384e-01,  ...,  2.7313e-01,\n",
      "           -3.6763e-01, -7.5573e-01],\n",
      "          [ 1.3713e-01, -6.0403e-01,  3.4449e-01,  ..., -4.7342e-02,\n",
      "           -3.5916e-01, -8.2728e-01],\n",
      "          ...,\n",
      "          [-2.1407e-01, -7.6407e-01,  2.4528e-01,  ..., -1.7845e-01,\n",
      "           -1.0088e-01, -4.8279e-01],\n",
      "          [-3.6300e-01, -7.5588e-01,  1.0135e-01,  ..., -1.3617e-01,\n",
      "            2.7873e-01, -6.4305e-01],\n",
      "          [ 6.8881e-02, -7.2858e-01, -3.0407e-01,  ...,  1.4490e-01,\n",
      "            4.4497e-01, -7.4734e-01]],\n",
      "\n",
      "         [[-1.3675e-01,  2.7267e-01,  1.0185e+00,  ...,  9.6297e-01,\n",
      "           -2.0606e-01, -2.6067e-01],\n",
      "          [-8.0682e-01, -3.3127e-01,  8.8625e-01,  ...,  1.9249e-01,\n",
      "           -2.0455e-01,  4.9271e-01],\n",
      "          [-9.2509e-01,  6.2157e-01,  8.6716e-01,  ...,  6.3873e-01,\n",
      "            2.2033e-01,  2.2148e-01],\n",
      "          ...,\n",
      "          [-5.4014e-01,  2.0864e-01,  8.2949e-01,  ...,  3.8471e-01,\n",
      "            3.2159e-01,  5.0366e-01],\n",
      "          [ 2.9761e-01, -4.9418e-01, -2.4334e-01,  ...,  6.1997e-01,\n",
      "            6.7966e-01,  1.2807e+00],\n",
      "          [-2.9135e-01,  5.1828e-01, -4.4467e-01,  ...,  5.1472e-01,\n",
      "            4.4164e-01,  1.0498e+00]]]], grad_fn=<PermuteBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# From GPT-4: https://chat.openai.com/share/a99ee779-f8c8-4bad-9511-483606ba0cca\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CustomGPT2Block(GPT2Block):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False, **kwargs):\n",
    "        # Before self-attention LayerNorm\n",
    "        ln1_input = hidden_states\n",
    "        attn_outputs = super().forward(\n",
    "            hidden_states, layer_past, attention_mask, head_mask, use_cache=use_cache, output_attentions=output_attentions, **kwargs\n",
    "        )\n",
    "        # Output from self-attention layer\n",
    "        hidden_states = attn_outputs[0]\n",
    "\n",
    "        # Before MLP LayerNorm\n",
    "        ln2_input = hidden_states\n",
    "\n",
    "        # Continue the normal process\n",
    "        feed_forward_hidden_states = self.mlp(ln2_input)\n",
    "        # Add the feed-forward output to the attention output\n",
    "        hidden_states = hidden_states + feed_forward_hidden_states\n",
    "        \n",
    "        # Final LayerNorm input\n",
    "        ln3_input = hidden_states\n",
    "\n",
    "        return (hidden_states,) + attn_outputs[1:] + ((ln1_input, ln2_input, ln3_input),)\n",
    "\n",
    "# Example model loading and modification\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "config = GPT2Config()\n",
    "model = GPT2Model(config)\n",
    "model.h[0] = CustomGPT2Block(config)  # Replace the first block for demonstration\n",
    "\n",
    "# Example forward pass\n",
    "input_ids = tokenizer.encode(\"ariel is a cs phd student\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract LayerNorm inputs\n",
    "ln_inputs = outputs[-1][-1]  # Assuming only the first block was replaced\n",
    "print(\"LayerNorm Inputs:\", ln_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
