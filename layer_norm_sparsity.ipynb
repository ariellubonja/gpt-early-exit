{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ChatGPT-4\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model\n",
    "import torch\n",
    "\n",
    "class EarlyExitingGPT2Model(GPT2Model):\n",
    "    def __init__(self, config, exit_layer):\n",
    "        super().__init__(config)\n",
    "        self.exit_layer = exit_layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_states = self.wte(input_ids)  # Word token embeddings\n",
    "        for i, block in enumerate(self.h):\n",
    "            hidden_states = block(hidden_states)[0]\n",
    "            if i == self.exit_layer - 1:\n",
    "                break\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hiddenstates_attn(text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    print(\"Nr. of input tokens: \", len(input_ids))\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids)\n",
    "\n",
    "    hidden_states = output.hidden_states\n",
    "    attentions = output.attentions\n",
    "    return hidden_states, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration and create a new model instance\n",
    "config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "# early_exiting_model = EarlyExitingGPT2Model(config, exit_layer=12)\n",
    "gpt2_medium_model = GPT2Model.from_pretrained('gpt2-medium', output_attentions=True, output_hidden_states=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "text = \"Hello, my dog is cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 1024,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 24,\n",
       "  \"n_positions\": 1024,\n",
       "  \"n_special\": 0,\n",
       "  \"predict_special_tokens\": true,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.40.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "GPT2Model                                     --\n",
       "├─Embedding: 1-1                              51,463,168\n",
       "├─Embedding: 1-2                              1,048,576\n",
       "├─Dropout: 1-3                                --\n",
       "├─ModuleList: 1-4                             --\n",
       "│    └─GPT2Block: 2-1                         --\n",
       "│    │    └─LayerNorm: 3-1                    2,048\n",
       "│    │    └─GPT2Attention: 3-2                4,198,400\n",
       "│    │    └─LayerNorm: 3-3                    2,048\n",
       "│    │    └─GPT2MLP: 3-4                      8,393,728\n",
       "│    └─GPT2Block: 2-2                         --\n",
       "│    │    └─LayerNorm: 3-5                    2,048\n",
       "│    │    └─GPT2Attention: 3-6                4,198,400\n",
       "│    │    └─LayerNorm: 3-7                    2,048\n",
       "│    │    └─GPT2MLP: 3-8                      8,393,728\n",
       "│    └─GPT2Block: 2-3                         --\n",
       "│    │    └─LayerNorm: 3-9                    2,048\n",
       "│    │    └─GPT2Attention: 3-10               4,198,400\n",
       "│    │    └─LayerNorm: 3-11                   2,048\n",
       "│    │    └─GPT2MLP: 3-12                     8,393,728\n",
       "│    └─GPT2Block: 2-4                         --\n",
       "│    │    └─LayerNorm: 3-13                   2,048\n",
       "│    │    └─GPT2Attention: 3-14               4,198,400\n",
       "│    │    └─LayerNorm: 3-15                   2,048\n",
       "│    │    └─GPT2MLP: 3-16                     8,393,728\n",
       "│    └─GPT2Block: 2-5                         --\n",
       "│    │    └─LayerNorm: 3-17                   2,048\n",
       "│    │    └─GPT2Attention: 3-18               4,198,400\n",
       "│    │    └─LayerNorm: 3-19                   2,048\n",
       "│    │    └─GPT2MLP: 3-20                     8,393,728\n",
       "│    └─GPT2Block: 2-6                         --\n",
       "│    │    └─LayerNorm: 3-21                   2,048\n",
       "│    │    └─GPT2Attention: 3-22               4,198,400\n",
       "│    │    └─LayerNorm: 3-23                   2,048\n",
       "│    │    └─GPT2MLP: 3-24                     8,393,728\n",
       "│    └─GPT2Block: 2-7                         --\n",
       "│    │    └─LayerNorm: 3-25                   2,048\n",
       "│    │    └─GPT2Attention: 3-26               4,198,400\n",
       "│    │    └─LayerNorm: 3-27                   2,048\n",
       "│    │    └─GPT2MLP: 3-28                     8,393,728\n",
       "│    └─GPT2Block: 2-8                         --\n",
       "│    │    └─LayerNorm: 3-29                   2,048\n",
       "│    │    └─GPT2Attention: 3-30               4,198,400\n",
       "│    │    └─LayerNorm: 3-31                   2,048\n",
       "│    │    └─GPT2MLP: 3-32                     8,393,728\n",
       "│    └─GPT2Block: 2-9                         --\n",
       "│    │    └─LayerNorm: 3-33                   2,048\n",
       "│    │    └─GPT2Attention: 3-34               4,198,400\n",
       "│    │    └─LayerNorm: 3-35                   2,048\n",
       "│    │    └─GPT2MLP: 3-36                     8,393,728\n",
       "│    └─GPT2Block: 2-10                        --\n",
       "│    │    └─LayerNorm: 3-37                   2,048\n",
       "│    │    └─GPT2Attention: 3-38               4,198,400\n",
       "│    │    └─LayerNorm: 3-39                   2,048\n",
       "│    │    └─GPT2MLP: 3-40                     8,393,728\n",
       "│    └─GPT2Block: 2-11                        --\n",
       "│    │    └─LayerNorm: 3-41                   2,048\n",
       "│    │    └─GPT2Attention: 3-42               4,198,400\n",
       "│    │    └─LayerNorm: 3-43                   2,048\n",
       "│    │    └─GPT2MLP: 3-44                     8,393,728\n",
       "│    └─GPT2Block: 2-12                        --\n",
       "│    │    └─LayerNorm: 3-45                   2,048\n",
       "│    │    └─GPT2Attention: 3-46               4,198,400\n",
       "│    │    └─LayerNorm: 3-47                   2,048\n",
       "│    │    └─GPT2MLP: 3-48                     8,393,728\n",
       "│    └─GPT2Block: 2-13                        --\n",
       "│    │    └─LayerNorm: 3-49                   2,048\n",
       "│    │    └─GPT2Attention: 3-50               4,198,400\n",
       "│    │    └─LayerNorm: 3-51                   2,048\n",
       "│    │    └─GPT2MLP: 3-52                     8,393,728\n",
       "│    └─GPT2Block: 2-14                        --\n",
       "│    │    └─LayerNorm: 3-53                   2,048\n",
       "│    │    └─GPT2Attention: 3-54               4,198,400\n",
       "│    │    └─LayerNorm: 3-55                   2,048\n",
       "│    │    └─GPT2MLP: 3-56                     8,393,728\n",
       "│    └─GPT2Block: 2-15                        --\n",
       "│    │    └─LayerNorm: 3-57                   2,048\n",
       "│    │    └─GPT2Attention: 3-58               4,198,400\n",
       "│    │    └─LayerNorm: 3-59                   2,048\n",
       "│    │    └─GPT2MLP: 3-60                     8,393,728\n",
       "│    └─GPT2Block: 2-16                        --\n",
       "│    │    └─LayerNorm: 3-61                   2,048\n",
       "│    │    └─GPT2Attention: 3-62               4,198,400\n",
       "│    │    └─LayerNorm: 3-63                   2,048\n",
       "│    │    └─GPT2MLP: 3-64                     8,393,728\n",
       "│    └─GPT2Block: 2-17                        --\n",
       "│    │    └─LayerNorm: 3-65                   2,048\n",
       "│    │    └─GPT2Attention: 3-66               4,198,400\n",
       "│    │    └─LayerNorm: 3-67                   2,048\n",
       "│    │    └─GPT2MLP: 3-68                     8,393,728\n",
       "│    └─GPT2Block: 2-18                        --\n",
       "│    │    └─LayerNorm: 3-69                   2,048\n",
       "│    │    └─GPT2Attention: 3-70               4,198,400\n",
       "│    │    └─LayerNorm: 3-71                   2,048\n",
       "│    │    └─GPT2MLP: 3-72                     8,393,728\n",
       "│    └─GPT2Block: 2-19                        --\n",
       "│    │    └─LayerNorm: 3-73                   2,048\n",
       "│    │    └─GPT2Attention: 3-74               4,198,400\n",
       "│    │    └─LayerNorm: 3-75                   2,048\n",
       "│    │    └─GPT2MLP: 3-76                     8,393,728\n",
       "│    └─GPT2Block: 2-20                        --\n",
       "│    │    └─LayerNorm: 3-77                   2,048\n",
       "│    │    └─GPT2Attention: 3-78               4,198,400\n",
       "│    │    └─LayerNorm: 3-79                   2,048\n",
       "│    │    └─GPT2MLP: 3-80                     8,393,728\n",
       "│    └─GPT2Block: 2-21                        --\n",
       "│    │    └─LayerNorm: 3-81                   2,048\n",
       "│    │    └─GPT2Attention: 3-82               4,198,400\n",
       "│    │    └─LayerNorm: 3-83                   2,048\n",
       "│    │    └─GPT2MLP: 3-84                     8,393,728\n",
       "│    └─GPT2Block: 2-22                        --\n",
       "│    │    └─LayerNorm: 3-85                   2,048\n",
       "│    │    └─GPT2Attention: 3-86               4,198,400\n",
       "│    │    └─LayerNorm: 3-87                   2,048\n",
       "│    │    └─GPT2MLP: 3-88                     8,393,728\n",
       "│    └─GPT2Block: 2-23                        --\n",
       "│    │    └─LayerNorm: 3-89                   2,048\n",
       "│    │    └─GPT2Attention: 3-90               4,198,400\n",
       "│    │    └─LayerNorm: 3-91                   2,048\n",
       "│    │    └─GPT2MLP: 3-92                     8,393,728\n",
       "│    └─GPT2Block: 2-24                        --\n",
       "│    │    └─LayerNorm: 3-93                   2,048\n",
       "│    │    └─GPT2Attention: 3-94               4,198,400\n",
       "│    │    └─LayerNorm: 3-95                   2,048\n",
       "│    │    └─GPT2MLP: 3-96                     8,393,728\n",
       "├─LayerNorm: 1-5                              2,048\n",
       "======================================================================\n",
       "Total params: 354,823,168\n",
       "Trainable params: 354,823,168\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary as ti_summary\n",
    "\n",
    "ti_summary(gpt2_medium_model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of input tokens:  1\n"
     ]
    }
   ],
   "source": [
    "# Attn is the score just before the *V\n",
    "hidden_states, attn = get_hiddenstates_attn(text, gpt2_medium_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention is $n\\times n$ since it's a pairwise importance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 6, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual implementation of `fwd()` to extract LayerNorm inputs & outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:55:34.939249Z",
     "start_time": "2024-04-26T20:54:09.526700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Don't need this. Use modeling_gpt2.py:GPT2Block, debug before/after LayerNorm\n",
    "# TODO edit this to perform Top-K LayerNorm\n",
    "# From GPT-4: https://chat.openai.com/share/a99ee779-f8c8-4bad-9511-483606ba0cca\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "class TopKLayerNormGPT2Block(GPT2Block):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example model loading and modification\n",
    "# from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# config = GPT2Config()\n",
    "# model = GPT2Model(config)\n",
    "# model.h[0] = TopKLayerNormGPT2Block(config)  # Replace the first block for demonstration\n",
    "\n",
    "# # Example forward pass\n",
    "# input_ids = tokenizer.encode(\"ariel is a cs phd student\", return_tensors=\"pt\")\n",
    "# outputs = model(input_ids)\n",
    "\n",
    "# # Extract LayerNorm inputs\n",
    "# ln_inputs = outputs[-1][-1]  # Assuming only the first block was replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Correctness against stock GPT-2 at same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Token Embeddings output match\n",
      "State Dicts match\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Custom block class definition here (as previously defined)\n",
    "# from your_custom_module import CustomGPT2Block\n",
    "\n",
    "def compare_models():\n",
    "    torch.manual_seed(43)\n",
    "\n",
    "    config = GPT2Config()\n",
    "    original_model = GPT2Model(config).eval()\n",
    "    modified_model = GPT2Model(config).eval()\n",
    "\n",
    "    # new_h0 = TopKLayerNormGPT2Block(config)\n",
    "    # # Replace the first block of the modified model with the custom block\n",
    "    # modified_model.h[0] = new_h0\n",
    "    modified_model.h[0] = TopKLayerNormGPT2Block(config)\n",
    "\n",
    "    # Ensure both models start with the same weights\n",
    "    modified_model.load_state_dict(original_model.state_dict())\n",
    "    modified_model.h[0].load_state_dict(original_model.h[0].state_dict())\n",
    "\n",
    "    def compare_state_dicts(dict1, dict2):\n",
    "        # Check if both dictionaries have the same set of keys (parameter names)\n",
    "        assert dict1.keys() == dict2.keys(), \"State dicts have different sets of parameters.\"\n",
    "        \n",
    "        # Check if all tensors in both dictionaries are close\n",
    "        for key in dict1:\n",
    "            assert torch.allclose(dict1[key], dict2[key], atol=1e-6), f\"Mismatch found in parameter: {key}\"\n",
    "\n",
    "    # assert torch.allclose(original_model.state_dict(), hidden_states_modified, atol=1e-6), \"Embedding output mismatch\"\n",
    "\n",
    "    # Prepare input\n",
    "    input_ids = torch.tensor([[12100, 242, 508, 318, 13, 198]])\n",
    "\n",
    "    hidden_states_original = original_model.wte(input_ids)\n",
    "    hidden_states_modified = modified_model.wte(input_ids)\n",
    "    assert torch.allclose(hidden_states_original, hidden_states_modified, atol=1e-6), \"Embedding output mismatch\"\n",
    "    print(\"Word Token Embeddings output match\")\n",
    "\n",
    "    modified_model.load_state_dict(original_model.state_dict())\n",
    "    modified_model.h[0].load_state_dict(original_model.h[0].state_dict())\n",
    "\n",
    "    # Compare state dicts\n",
    "    state_dict1 = original_model.state_dict()\n",
    "    state_dict2 = modified_model.state_dict()\n",
    "\n",
    "    compare_state_dicts(state_dict1, state_dict2)\n",
    "    print(\"State Dicts match\")\n",
    "\n",
    "    original_h0 = original_model.h[0](hidden_states_original)[0]\n",
    "    modified_h0 = modified_model.h[0](hidden_states_modified)[0]\n",
    "    assert torch.allclose(original_h0, modified_h0, atol=1e-6), \"First Hidden State output mismatch\"\n",
    "    print(\"First Hidden State output match\")\n",
    "\n",
    "\n",
    "    # Get outputs from both models\n",
    "    with torch.no_grad():\n",
    "        original_output = original_model(input_ids)[0]\n",
    "        modified_output = modified_model(input_ids)[0]\n",
    "\n",
    "    # Compare outputs\n",
    "    # print(\"Difference between original and modified model outputs:\", torch.sum((original_output - modified_output) ** 2))\n",
    "    print(\"original_output: \", original_output)\n",
    "    print(\"modified_output: \", modified_output)\n",
    "\n",
    "compare_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Fails</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.2464,  1.4542,  0.2508,  ...,  0.7829, -0.3028,  1.5124],\n",
       "         [ 1.4531,  0.1231,  0.5521,  ..., -0.6331,  0.9294,  0.8452],\n",
       "         [ 1.3695, -1.6607,  0.5966,  ...,  0.2552,  0.5448, -0.1715],\n",
       "         ...,\n",
       "         [ 1.7666, -2.1220, -0.8764,  ...,  0.4726,  1.6389, -0.1402],\n",
       "         [ 1.1706, -0.9836,  0.4577,  ...,  0.3069,  0.3220, -0.8828],\n",
       "         [ 0.0530, -1.2568,  0.0125,  ...,  0.6572,  1.3371, -0.9808]]],\n",
       "       grad_fn=<ViewBackward0>), past_key_values=((tensor([[[[-0.0194, -0.2735, -0.4559,  ...,  0.0875, -0.6530,  0.6981],\n",
       "          [-0.3479,  0.2874, -0.0608,  ..., -0.3080,  0.5808, -0.8519],\n",
       "          [-0.2116, -0.6707, -0.0784,  ..., -0.0107,  0.4759, -0.0268],\n",
       "          ...,\n",
       "          [ 0.5499, -0.9602, -0.0313,  ..., -0.0875,  0.0368,  0.6028],\n",
       "          [ 0.0415, -0.1072,  0.4097,  ...,  0.5615, -0.6842,  0.8332],\n",
       "          [-0.3610,  0.0154,  0.2739,  ..., -0.2877, -0.4979, -0.4553]],\n",
       "\n",
       "         [[ 0.7418, -0.8026,  0.3746,  ..., -0.3108, -0.2023,  0.3757],\n",
       "          [ 0.5789, -0.0654, -0.3420,  ..., -0.2007,  0.3349, -0.5231],\n",
       "          [-0.0658, -0.0128, -0.7644,  ...,  1.0357, -0.5766,  0.6113],\n",
       "          ...,\n",
       "          [-0.1091,  0.6895, -0.3818,  ..., -0.0596, -0.3487, -0.7129],\n",
       "          [-0.6531,  0.2369,  1.0401,  ..., -0.1795, -0.1919,  0.5599],\n",
       "          [-0.0293, -0.2301,  0.4037,  ...,  1.0104,  0.4037, -0.1530]],\n",
       "\n",
       "         [[ 0.5739,  0.4759,  0.1803,  ...,  1.0471, -0.0585,  0.1344],\n",
       "          [-0.1929, -0.3688,  0.2537,  ..., -0.7223,  0.2397,  0.8519],\n",
       "          [-0.2073, -0.4857, -0.4572,  ..., -0.8403, -0.3492, -0.5162],\n",
       "          ...,\n",
       "          [-0.1089, -0.0483,  0.0563,  ..., -0.0602, -1.5375, -0.2259],\n",
       "          [ 0.0206, -0.9524,  0.3232,  ...,  0.2035,  0.8142,  0.0852],\n",
       "          [-0.3488, -0.3969, -0.3941,  ..., -0.8870,  0.3447, -0.2021]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1365,  0.1826, -0.7175,  ...,  0.8716, -0.9493,  0.6117],\n",
       "          [ 0.4005, -0.2301, -0.2785,  ...,  0.7704, -0.7335, -0.2005],\n",
       "          [ 0.4728,  0.1059, -0.3288,  ...,  0.3304, -0.2696, -0.5827],\n",
       "          ...,\n",
       "          [-0.5340,  0.6412, -0.6139,  ...,  1.1827, -0.4460,  0.2024],\n",
       "          [-0.2796,  0.5326,  0.7645,  ..., -0.8313, -0.4122, -0.2219],\n",
       "          [-0.6654,  0.1355,  0.2340,  ...,  0.1725, -0.5067, -0.3692]],\n",
       "\n",
       "         [[ 0.6972, -0.1223, -0.3944,  ...,  0.3751, -0.0913, -0.5860],\n",
       "          [ 0.4044, -0.6513, -0.2424,  ..., -0.1887,  0.3719,  0.4337],\n",
       "          [ 0.0909,  0.0642,  0.4838,  ...,  0.2809,  0.2446, -0.0195],\n",
       "          ...,\n",
       "          [-0.3292,  0.5371, -0.5733,  ..., -0.5703, -0.2628,  0.8182],\n",
       "          [ 0.8781, -0.8051,  0.0275,  ..., -0.5212, -0.4367,  0.5186],\n",
       "          [ 0.7933,  1.3692, -0.3217,  ...,  0.7981,  0.4333, -0.1841]],\n",
       "\n",
       "         [[ 0.6887, -0.4814,  0.2791,  ..., -0.4658, -0.2964,  0.0224],\n",
       "          [-0.2777, -0.3634, -1.1683,  ...,  0.2316,  0.3945,  0.3472],\n",
       "          [-0.7319, -0.8691,  0.0779,  ...,  0.0455, -0.2012,  0.3930],\n",
       "          ...,\n",
       "          [ 0.5946,  0.2121,  1.0989,  ...,  1.3513,  0.0342, -0.3202],\n",
       "          [-0.5017, -0.4295,  0.2046,  ..., -0.8747,  0.5459,  0.1141],\n",
       "          [-0.6502,  0.3172,  0.1239,  ...,  0.1894, -0.6636,  0.1920]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.3040, -0.9277, -0.9297,  ...,  0.7559, -0.1582,  0.5563],\n",
       "          [-0.3165, -0.4119, -0.2120,  ...,  0.3857, -0.1699,  0.0215],\n",
       "          [-0.0326,  0.7762, -0.9375,  ...,  0.2292, -0.6286, -0.5745],\n",
       "          ...,\n",
       "          [-0.5190,  1.2813, -0.3527,  ...,  0.3672, -0.3629,  0.5044],\n",
       "          [ 0.2754, -0.2674,  0.2719,  ...,  0.6607, -0.0429, -0.0467],\n",
       "          [-0.1302, -0.5020,  0.2448,  ...,  0.2968, -0.4294,  0.1428]],\n",
       "\n",
       "         [[ 0.5898, -0.1826,  0.7944,  ..., -0.1789,  0.4580,  0.6430],\n",
       "          [ 1.6340,  0.1011, -0.4286,  ..., -0.0244,  0.3540, -0.3152],\n",
       "          [ 0.3916, -0.4115,  0.1442,  ...,  0.1580, -0.4045,  0.3085],\n",
       "          ...,\n",
       "          [ 0.7268, -0.3480, -1.1117,  ...,  0.4771,  0.4026, -0.4086],\n",
       "          [ 0.3023, -0.2529,  0.1364,  ..., -0.6703, -0.1836, -0.1612],\n",
       "          [ 0.4689,  0.7290, -0.7298,  ..., -0.1200,  0.3668, -0.3579]],\n",
       "\n",
       "         [[-0.1914, -0.1737, -0.3903,  ...,  0.2913,  0.0519,  0.2546],\n",
       "          [ 0.4008, -0.4539,  0.8732,  ..., -0.1407, -0.0654,  0.8978],\n",
       "          [ 0.1980,  0.3805, -0.0242,  ...,  0.2676, -0.3144,  0.0397],\n",
       "          ...,\n",
       "          [-0.0721, -0.7601, -0.3074,  ..., -0.4828,  0.0191,  0.0291],\n",
       "          [ 0.2440,  0.6511, -0.3800,  ...,  0.2674, -0.0684,  0.6406],\n",
       "          [ 0.4901,  0.5067, -1.0415,  ..., -0.2539, -0.1019, -0.1389]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.4149,  0.5716, -0.7252,  ...,  0.2019, -0.1359, -0.5193],\n",
       "          [-0.1034, -0.2168,  0.4235,  ..., -1.1499,  0.6216, -0.1754],\n",
       "          [ 0.7550,  0.0331,  0.2597,  ...,  0.2009,  0.3482,  0.0518],\n",
       "          ...,\n",
       "          [-0.8670, -0.1457,  0.1763,  ..., -0.7075, -0.0489, -0.3206],\n",
       "          [-0.0053, -0.8124, -0.1322,  ..., -0.4825, -0.8484, -0.1890],\n",
       "          [-0.2138, -1.1078, -0.1480,  ..., -0.4412, -0.9111,  0.7278]],\n",
       "\n",
       "         [[-0.6500, -0.1469,  0.0787,  ..., -0.8934,  0.0218, -0.2729],\n",
       "          [-1.3226,  0.2202,  0.3652,  ..., -0.0182,  0.0790,  0.5086],\n",
       "          [-0.2152, -0.8279,  0.4502,  ...,  0.1727,  0.1222, -0.9108],\n",
       "          ...,\n",
       "          [ 0.3475,  0.4798,  0.7176,  ..., -0.4078,  0.0158,  0.4951],\n",
       "          [-0.7661,  0.2230, -0.1039,  ..., -0.2604,  0.0907, -0.1655],\n",
       "          [ 0.1068,  0.1592, -0.7527,  ...,  0.2978,  0.6308,  0.7528]],\n",
       "\n",
       "         [[-0.9533, -0.2337,  0.2419,  ...,  0.4500, -0.3104,  0.0416],\n",
       "          [-0.5054,  0.0090,  0.3035,  ..., -0.0536,  1.3170,  0.4068],\n",
       "          [-0.3247, -0.1464,  0.7399,  ..., -1.1742, -0.3667, -0.4618],\n",
       "          ...,\n",
       "          [-0.0274, -0.5079,  0.1521,  ..., -0.3840, -0.7359,  0.4636],\n",
       "          [-1.2426, -0.3557, -0.8977,  ..., -0.3524,  0.0401, -0.1457],\n",
       "          [ 1.2322,  0.0487, -0.4387,  ...,  0.3803, -0.0712,  0.0038]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.5378,  0.5366, -0.5191,  ...,  0.0283,  0.0588,  0.8912],\n",
       "          [ 0.5937, -0.2251, -0.2117,  ..., -0.4451,  0.0182,  0.6675],\n",
       "          [ 0.0073, -0.1705, -0.0500,  ..., -1.6868,  0.0505,  0.3274],\n",
       "          ...,\n",
       "          [-0.3135,  0.3406, -0.0908,  ..., -0.9935,  0.1614,  1.0188],\n",
       "          [-0.4072,  0.4737, -0.3225,  ..., -0.6549, -0.3143,  0.9875],\n",
       "          [-0.6026,  0.5836, -0.2398,  ..., -0.5420, -0.3105,  0.2408]],\n",
       "\n",
       "         [[ 0.4980,  0.3444,  0.2044,  ..., -0.6700, -0.5405,  0.9746],\n",
       "          [-0.5405,  0.1625,  0.3597,  ..., -0.0407, -0.4984,  0.2476],\n",
       "          [-0.5272,  0.5866,  0.6455,  ...,  0.2754,  0.1398,  0.0990],\n",
       "          ...,\n",
       "          [-0.1218,  0.5831, -0.1336,  ...,  0.5867,  0.2165, -0.0909],\n",
       "          [-0.3137, -0.0445,  0.2463,  ...,  0.7438, -0.4173, -0.6974],\n",
       "          [-0.3084,  0.3242, -0.8664,  ...,  0.1077,  0.2387, -0.3655]],\n",
       "\n",
       "         [[ 0.1551, -0.9845,  1.1052,  ...,  0.5536, -0.1393, -0.3881],\n",
       "          [ 0.2302, -0.2483,  0.1222,  ..., -0.2659,  0.3351,  0.3897],\n",
       "          [ 0.9856, -0.0102, -0.0782,  ..., -0.1575,  0.2842, -0.6309],\n",
       "          ...,\n",
       "          [ 0.7213, -0.5604,  0.4536,  ..., -0.0970,  0.7084, -1.2209],\n",
       "          [ 1.0872, -0.1973, -0.3692,  ...,  0.7290,  0.2038, -0.0034],\n",
       "          [ 0.7362, -0.8221, -0.4314,  ...,  0.1499,  0.7419,  0.3742]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0692, -1.1208,  0.2535,  ...,  0.4531,  0.0217,  0.5285],\n",
       "          [-0.4080, -1.2192,  1.0643,  ...,  0.1765, -0.3339, -0.0726],\n",
       "          [-0.7685, -1.0381,  1.0672,  ..., -0.1514, -0.2492, -0.8260],\n",
       "          ...,\n",
       "          [-1.4787, -1.7437, -0.1625,  ...,  0.5742,  0.2969,  0.2574],\n",
       "          [-1.4494, -0.7948,  0.0391,  ...,  1.1565,  0.1025, -0.2711],\n",
       "          [-1.9229, -0.7992,  0.6588,  ...,  0.7920,  0.4433,  0.1784]],\n",
       "\n",
       "         [[ 0.3638,  0.2033,  0.6662,  ..., -0.2529,  0.3595, -0.3986],\n",
       "          [-0.9129, -0.6646,  0.3260,  ..., -0.1157,  0.3165, -0.4314],\n",
       "          [-1.0412, -0.0884, -0.1899,  ..., -0.1743,  0.3103, -0.0143],\n",
       "          ...,\n",
       "          [-0.0278, -0.4828, -0.6455,  ..., -0.6332, -0.1125,  0.0400],\n",
       "          [-0.0282, -0.2044, -0.1186,  ..., -0.7524, -0.5305, -0.1955],\n",
       "          [-0.3078,  0.2802, -0.6709,  ...,  0.0681,  0.2614,  0.1759]],\n",
       "\n",
       "         [[-0.8214,  0.6886,  0.6806,  ...,  0.2209,  0.5835,  0.7229],\n",
       "          [ 0.2328,  0.5302,  0.3513,  ..., -0.0307,  0.6126,  0.8075],\n",
       "          [ 0.3728, -0.1778, -0.1650,  ...,  0.1895,  0.4633, -0.0294],\n",
       "          ...,\n",
       "          [ 0.3198,  0.9305,  0.0662,  ...,  0.2733,  0.3894, -0.3536],\n",
       "          [ 0.5269,  0.5037, -0.0282,  ...,  0.2229,  0.8225,  0.3346],\n",
       "          [ 0.9058,  1.1883,  0.0560,  ..., -0.0285,  0.4367, -0.6989]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 4.2951e-02, -4.5703e-01,  8.8782e-01,  ...,  5.9018e-02,\n",
       "            1.8585e-01, -6.5610e-02],\n",
       "          [ 7.8402e-01, -1.5406e-01,  4.9512e-01,  ...,  5.2823e-01,\n",
       "            9.2538e-01,  8.2724e-01],\n",
       "          [ 3.7968e-01, -2.4508e-01, -6.8472e-02,  ..., -4.7853e-03,\n",
       "            7.8429e-04,  9.2333e-01],\n",
       "          ...,\n",
       "          [ 6.6100e-01, -7.4061e-01, -1.6336e-02,  ...,  5.1497e-01,\n",
       "           -1.0801e-01, -2.3655e-01],\n",
       "          [ 1.2740e-01, -5.1403e-01,  2.8269e-01,  ..., -1.5140e-01,\n",
       "           -5.0897e-01, -5.0750e-01],\n",
       "          [-4.0789e-01, -8.5708e-01,  1.1250e-01,  ..., -1.7316e-01,\n",
       "            1.6960e-01, -3.1456e-01]],\n",
       "\n",
       "         [[ 1.3165e-01,  7.4182e-01,  2.0266e-01,  ..., -1.3899e-01,\n",
       "            4.7766e-02,  1.0104e+00],\n",
       "          [ 9.4465e-02,  2.1624e-01, -8.1084e-01,  ..., -2.4806e-01,\n",
       "            6.3297e-01,  6.1647e-01],\n",
       "          [-1.4940e-01, -6.0725e-01, -2.9307e-01,  ...,  1.3573e-02,\n",
       "            5.1988e-01,  6.9265e-01],\n",
       "          ...,\n",
       "          [-6.7202e-01,  2.0542e-01, -2.0255e-01,  ..., -1.5942e-01,\n",
       "            4.0897e-01, -4.4926e-01],\n",
       "          [-2.8196e-01,  1.0771e-01, -3.5712e-01,  ...,  2.9540e-01,\n",
       "            2.8449e-01,  2.6365e-01],\n",
       "          [ 2.2874e-01, -2.8791e-01, -1.9084e-01,  ..., -1.7373e-01,\n",
       "           -2.6542e-01, -1.3571e-01]],\n",
       "\n",
       "         [[ 1.1851e+00, -1.1335e+00,  3.4162e-01,  ...,  4.7524e-01,\n",
       "           -8.0459e-01, -7.7177e-02],\n",
       "          [ 6.3044e-01, -3.8695e-01,  5.5501e-01,  ...,  4.7253e-01,\n",
       "           -5.2275e-01, -9.4893e-02],\n",
       "          [ 6.7241e-01, -7.5551e-01,  1.9124e-01,  ..., -2.0377e-03,\n",
       "            5.9921e-01,  9.2762e-01],\n",
       "          ...,\n",
       "          [ 1.9574e-01, -1.3955e-01,  4.5654e-01,  ..., -9.5449e-02,\n",
       "            2.1264e-01,  2.8083e-01],\n",
       "          [ 9.1369e-03, -2.2550e-01,  1.1884e+00,  ..., -7.0550e-01,\n",
       "            4.4988e-01,  2.7320e-01],\n",
       "          [ 4.0729e-01, -9.1970e-01,  4.4840e-01,  ..., -1.6689e-01,\n",
       "            4.9921e-01, -1.0384e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.6436e-02,  8.7440e-01, -2.2035e-01,  ..., -1.0111e+00,\n",
       "            1.8312e-02, -3.9826e-01],\n",
       "          [ 2.2594e-01,  5.3483e-01, -6.8165e-01,  ..., -1.7320e-01,\n",
       "           -3.6409e-01, -1.1427e-01],\n",
       "          [ 2.7974e-02,  1.0316e+00,  5.4461e-01,  ..., -1.2883e-01,\n",
       "           -8.6876e-01, -6.1960e-01],\n",
       "          ...,\n",
       "          [ 6.6441e-01,  1.2293e+00,  3.5005e-01,  ..., -6.8829e-02,\n",
       "           -6.0663e-01, -1.1141e+00],\n",
       "          [ 4.7659e-01,  1.1766e+00,  3.6304e-01,  ...,  7.8355e-02,\n",
       "           -4.7531e-02, -3.8149e-01],\n",
       "          [ 7.7432e-01,  2.9759e-01, -6.8069e-01,  ...,  3.5364e-02,\n",
       "            2.5436e-02, -4.9299e-01]],\n",
       "\n",
       "         [[ 5.8818e-01,  2.4869e-02, -7.8526e-01,  ...,  3.0312e-01,\n",
       "            4.2890e-01, -2.6140e-01],\n",
       "          [-2.6441e-01,  5.3801e-01, -8.8665e-01,  ..., -3.1525e-01,\n",
       "            1.5908e-02,  6.7267e-01],\n",
       "          [ 2.3680e-01,  2.0092e-01, -1.0387e-01,  ...,  2.3028e-01,\n",
       "           -1.5695e-01,  9.0902e-02],\n",
       "          ...,\n",
       "          [-1.5895e-01,  1.2500e-01, -2.5095e-02,  ..., -2.4127e-01,\n",
       "           -7.5709e-02,  5.0519e-01],\n",
       "          [ 5.0694e-01,  6.8176e-01,  3.1742e-01,  ..., -6.4337e-01,\n",
       "            3.3328e-01,  8.1725e-01],\n",
       "          [-9.2887e-02,  1.2762e+00,  5.3104e-01,  ...,  3.7389e-02,\n",
       "            4.0276e-02,  3.7257e-01]],\n",
       "\n",
       "         [[ 1.2227e+00, -1.2932e-01,  5.0433e-01,  ..., -2.7684e-01,\n",
       "            4.7534e-01, -2.6069e-01],\n",
       "          [-8.2350e-02,  3.6571e-03,  6.8632e-02,  ..., -2.1883e-01,\n",
       "            6.4430e-01,  2.1769e-01],\n",
       "          [-1.7448e-01,  7.2736e-01,  2.4414e-01,  ..., -2.4359e-01,\n",
       "            3.2641e-01, -4.4415e-01],\n",
       "          ...,\n",
       "          [-2.7251e-01,  7.3112e-01, -7.2422e-01,  ...,  3.8448e-01,\n",
       "            3.6508e-01,  1.7147e-02],\n",
       "          [ 3.2374e-01,  4.7104e-01,  1.5541e-01,  ...,  8.1645e-02,\n",
       "           -7.1591e-02, -6.1765e-01],\n",
       "          [ 2.2003e-01,  1.0910e+00, -3.8523e-01,  ..., -2.2883e-02,\n",
       "            6.4625e-01,  6.3042e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.8803, -0.2351, -0.1471,  ...,  0.3409,  0.5348,  0.0056],\n",
       "          [ 0.3045, -1.6775, -0.1505,  ..., -0.1160,  0.2990,  0.9230],\n",
       "          [ 0.5150, -0.8894,  0.0525,  ...,  0.3512,  0.1837,  0.5317],\n",
       "          ...,\n",
       "          [ 0.8614, -1.3132, -0.1388,  ...,  0.5442, -0.1078,  0.1049],\n",
       "          [ 1.2019, -1.2256, -0.3092,  ...,  0.8830,  0.7350,  0.0113],\n",
       "          [ 0.9866, -0.8783, -0.3514,  ...,  0.3745,  0.9391,  0.1246]],\n",
       "\n",
       "         [[-0.9213, -0.0184, -0.2145,  ...,  0.1176, -0.0320,  0.6479],\n",
       "          [-0.2140, -0.1581, -0.4094,  ...,  0.7882,  0.4070, -0.5982],\n",
       "          [ 0.6844,  0.4448,  0.3270,  ...,  0.2535,  0.2502, -0.2068],\n",
       "          ...,\n",
       "          [ 0.2508,  0.6931,  0.7069,  ...,  0.2529,  0.7007, -0.6341],\n",
       "          [ 0.5819,  0.2950,  0.2798,  ..., -0.2762,  0.4755, -0.6243],\n",
       "          [-0.5053,  0.2214,  0.2347,  ...,  0.3101,  0.4604, -1.4897]],\n",
       "\n",
       "         [[-0.5363,  0.3867, -0.4342,  ..., -0.4972,  0.1779, -0.2058],\n",
       "          [ 0.0946,  0.3099,  0.5135,  ..., -0.9833,  0.2204,  0.0414],\n",
       "          [-0.2106,  0.3731,  0.0908,  ..., -1.0948,  0.1150,  0.5948],\n",
       "          ...,\n",
       "          [ 0.1751,  0.8822, -0.7771,  ...,  0.6621,  0.3609,  0.4617],\n",
       "          [ 0.0028,  0.1861, -0.5287,  ..., -0.1551,  0.9865,  0.5006],\n",
       "          [ 0.4843,  0.3469, -0.8056,  ...,  0.0809,  0.6420,  0.0510]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3202,  0.5669, -0.4911,  ..., -0.3317,  0.8424, -0.0436],\n",
       "          [ 0.1051,  0.2751,  0.4262,  ...,  0.1783,  0.6937,  0.6415],\n",
       "          [ 0.2653, -0.3390,  0.1696,  ..., -0.1755,  0.6830,  0.5478],\n",
       "          ...,\n",
       "          [-0.2328,  0.0569,  0.7433,  ...,  0.0385,  1.1304,  0.8967],\n",
       "          [-0.2049,  0.2758,  0.5830,  ..., -0.1997,  0.3602,  0.3946],\n",
       "          [-0.5718,  0.1292,  0.7974,  ..., -0.0219,  0.9353,  0.4597]],\n",
       "\n",
       "         [[ 0.4885,  0.3969, -1.3256,  ...,  1.1409,  1.0301, -0.2810],\n",
       "          [ 0.1633,  0.2393, -1.2687,  ...,  0.5202, -0.0174, -0.1346],\n",
       "          [-0.8681,  0.5081, -0.7902,  ...,  0.4771,  0.4559, -0.8601],\n",
       "          ...,\n",
       "          [-0.3062,  0.1107, -0.7117,  ..., -0.2262,  0.1681, -0.1229],\n",
       "          [-0.4563,  0.5029, -0.8399,  ...,  0.0735,  0.9573,  0.3014],\n",
       "          [-0.5449, -0.0782, -1.3588,  ..., -1.1087,  0.2197,  0.3413]],\n",
       "\n",
       "         [[ 0.6104,  0.4527,  0.1558,  ...,  0.5838,  0.0568,  0.4495],\n",
       "          [ 0.4557, -0.1719, -0.0786,  ...,  0.3676,  0.4377,  0.5528],\n",
       "          [-0.6096,  0.1579, -0.4174,  ...,  0.6069, -0.0112, -0.5594],\n",
       "          ...,\n",
       "          [-0.3661, -0.0774, -0.5358,  ...,  0.3958, -0.3049, -0.2429],\n",
       "          [-0.5730,  0.2838, -0.3923,  ...,  0.9401,  0.2288, -0.2500],\n",
       "          [-0.1600,  0.3858, -0.4959,  ...,  0.5062,  0.4871,  0.6272]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.1514, -0.1589, -0.5173,  ...,  0.2352, -0.8429, -0.3970],\n",
       "          [ 0.3473, -0.2564, -0.5087,  ...,  0.1015, -1.0738, -0.0630],\n",
       "          [ 0.4319,  1.1249, -0.3173,  ...,  0.3988, -0.9005, -0.2969],\n",
       "          ...,\n",
       "          [-0.0435,  0.7319,  0.8260,  ..., -0.2538, -0.1073, -0.0732],\n",
       "          [-0.5131,  0.6311,  0.4372,  ...,  0.1743, -0.4327, -0.8703],\n",
       "          [-0.3528,  0.7685,  0.0296,  ..., -0.6255, -0.4161, -0.2330]],\n",
       "\n",
       "         [[-1.0202,  0.2634,  0.7330,  ..., -0.5298,  0.2409, -0.7883],\n",
       "          [-0.1993,  0.1407,  0.4209,  ..., -0.9315,  0.7121, -0.7809],\n",
       "          [-0.2095,  0.6655,  0.4270,  ..., -1.2486,  0.7605, -0.5208],\n",
       "          ...,\n",
       "          [ 0.0146,  0.6793,  0.5130,  ..., -0.6817,  0.6726, -0.7881],\n",
       "          [-0.3736, -0.1862,  0.1100,  ..., -0.6491, -0.1935, -0.8740],\n",
       "          [ 0.4382, -0.0271,  0.3494,  ..., -0.3724, -0.1764, -1.0156]],\n",
       "\n",
       "         [[ 0.8158,  0.2902, -0.8502,  ...,  0.6439, -0.6778,  1.0006],\n",
       "          [ 0.1850, -0.5114, -0.4009,  ...,  0.2066, -0.3870,  0.7591],\n",
       "          [ 0.0677, -0.0544, -0.8182,  ..., -0.0337,  0.5604,  0.8968],\n",
       "          ...,\n",
       "          [ 0.1405, -0.3287,  0.1391,  ..., -0.1604, -0.1857,  1.4879],\n",
       "          [ 0.4093,  0.0548, -0.2711,  ...,  0.0244, -0.3137,  1.0301],\n",
       "          [ 0.1477,  0.3973, -0.6344,  ...,  0.0748, -0.1638,  0.4592]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0363,  0.3119, -0.2737,  ..., -1.0123,  0.1997,  0.3195],\n",
       "          [-0.3714,  0.1604, -0.5701,  ..., -1.0444, -0.0022, -0.4726],\n",
       "          [-0.5029, -0.1662, -0.0047,  ..., -0.8456,  0.8155, -0.1641],\n",
       "          ...,\n",
       "          [-0.3361,  0.3126, -0.1578,  ..., -0.9259,  0.7442,  1.0478],\n",
       "          [-0.6512,  0.3469, -0.0409,  ..., -1.1889,  0.3027,  0.8683],\n",
       "          [-0.8081,  0.6034, -0.0875,  ..., -0.8910,  0.3787,  0.7981]],\n",
       "\n",
       "         [[-0.3390,  0.2645, -1.0187,  ..., -0.4076,  0.8429,  0.5068],\n",
       "          [ 0.1743,  1.0502, -1.6560,  ...,  0.4284,  0.4496, -0.4002],\n",
       "          [-0.0107,  0.1513, -0.7260,  ...,  0.0761,  0.0306,  0.2824],\n",
       "          ...,\n",
       "          [-0.0226,  0.5378, -0.2411,  ..., -0.4386, -0.4872,  0.4050],\n",
       "          [-0.1394,  0.6969, -0.5075,  ..., -0.0189,  0.4813,  0.1019],\n",
       "          [ 0.2414,  0.7983, -0.2065,  ..., -0.5111,  0.0564,  0.4758]],\n",
       "\n",
       "         [[ 0.3729,  0.0693,  0.4306,  ..., -0.5849,  0.7688, -0.0221],\n",
       "          [ 0.4053, -0.4202,  1.1641,  ..., -0.0912,  0.3030,  0.0914],\n",
       "          [ 0.1587, -0.5449,  0.2706,  ...,  0.0108,  0.6640,  0.3398],\n",
       "          ...,\n",
       "          [-0.1047, -0.6236, -0.6943,  ..., -0.6976,  0.5555,  0.2947],\n",
       "          [-0.1463, -0.5551,  0.5882,  ..., -0.3540,  0.5815,  0.4134],\n",
       "          [-0.5670, -0.4788, -0.3571,  ...,  0.2580,  0.5502,  0.5969]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.3628, -1.0659,  0.3403,  ..., -1.1523,  0.3530,  0.5265],\n",
       "          [ 0.6524,  0.2324,  0.3113,  ..., -0.1395,  0.2029,  0.0085],\n",
       "          [ 0.3039,  0.8666,  0.1177,  ...,  0.1622,  0.2344,  0.5806],\n",
       "          ...,\n",
       "          [ 0.1561,  0.8096, -0.5170,  ...,  0.3732,  0.1698, -0.4152],\n",
       "          [ 0.2557,  1.1133, -0.5912,  ..., -0.4085,  0.2466,  0.1818],\n",
       "          [ 0.1699,  0.7365, -0.8372,  ..., -0.3836,  0.3291, -0.6194]],\n",
       "\n",
       "         [[ 1.0700,  0.4979,  0.1616,  ...,  1.3037,  0.1864,  0.1129],\n",
       "          [ 0.0434,  0.0845, -0.2931,  ...,  0.2269,  0.0607,  0.6786],\n",
       "          [-0.1729,  0.6606, -0.2813,  ..., -0.1716,  0.4724, -0.4357],\n",
       "          ...,\n",
       "          [ 0.4484,  0.4849,  0.8806,  ..., -0.3738,  0.0916,  0.2920],\n",
       "          [ 0.0780,  0.2790,  0.7512,  ...,  0.1037, -0.2158, -0.3645],\n",
       "          [ 0.0751,  0.6627,  0.0226,  ..., -0.0424,  0.3198,  0.4874]],\n",
       "\n",
       "         [[ 0.1656,  0.4942, -0.4332,  ..., -0.6812,  0.9206, -0.5742],\n",
       "          [ 0.7637,  0.8635, -0.1340,  ..., -0.3263, -0.2799, -0.5270],\n",
       "          [-0.4885,  0.9908, -1.0760,  ..., -0.3802, -0.4176,  0.2390],\n",
       "          ...,\n",
       "          [-1.0698,  0.3807, -0.6016,  ..., -0.0626,  0.0887,  0.0928],\n",
       "          [-1.3480,  0.5230, -0.8198,  ..., -0.0162, -0.0420, -0.2602],\n",
       "          [-0.8049,  1.2733, -0.7722,  ...,  0.3734, -0.0788,  0.0981]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.5747,  0.7008, -0.4236,  ...,  0.0379,  0.6904,  0.1947],\n",
       "          [-0.4526, -0.4848, -0.2611,  ...,  0.8264,  0.6501, -0.6804],\n",
       "          [-0.6211, -0.0245, -0.3305,  ...,  0.6243,  1.1882,  0.0898],\n",
       "          ...,\n",
       "          [-0.9045,  0.0884,  0.1276,  ...,  0.2206,  0.0091, -0.1119],\n",
       "          [-1.1674,  0.3651, -0.3420,  ..., -0.0228,  0.1761, -0.2202],\n",
       "          [-0.3490,  0.5630,  0.4526,  ..., -0.3531,  0.1792, -0.1149]],\n",
       "\n",
       "         [[-0.3022,  0.3349,  0.2042,  ...,  0.0721, -0.2213, -1.1021],\n",
       "          [-0.1994,  0.2317,  0.4873,  ..., -0.8164,  0.2826, -0.9537],\n",
       "          [-0.3944,  0.4967,  0.3554,  ..., -0.5704,  0.2331, -0.3035],\n",
       "          ...,\n",
       "          [-0.2957, -0.4843, -0.0919,  ..., -0.3438, -0.0408, -0.6561],\n",
       "          [ 0.2489,  0.4941,  0.4000,  ..., -0.8472,  0.3936, -0.5094],\n",
       "          [-0.0073, -0.0339,  0.9218,  ..., -0.4716, -0.1329, -0.4632]],\n",
       "\n",
       "         [[ 0.2206, -0.5632, -1.0734,  ...,  0.4240, -0.2025,  0.1766],\n",
       "          [ 0.5215,  0.3766, -0.1185,  ...,  0.1281, -0.2674,  0.4466],\n",
       "          [-0.2540, -0.5193, -0.0318,  ...,  0.1450, -0.6159, -0.1644],\n",
       "          ...,\n",
       "          [ 0.3230, -1.0415, -0.6617,  ...,  0.0674, -0.2571,  0.0067],\n",
       "          [-0.0873, -0.3734, -0.5452,  ...,  0.1583,  0.0654, -0.0106],\n",
       "          [-0.6932, -0.8357,  0.0349,  ...,  0.4689,  0.3312, -0.7265]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.1711,  0.6784, -0.5994,  ..., -0.5245, -0.4483, -0.6314],\n",
       "          [ 0.6511,  0.2004, -0.3128,  ..., -0.4847, -0.9315, -0.6844],\n",
       "          [-0.1359, -0.3179, -0.7988,  ..., -0.2806, -0.6984,  0.0802],\n",
       "          ...,\n",
       "          [-0.3143, -0.8560, -0.7262,  ..., -0.2538,  0.1126, -0.2306],\n",
       "          [ 0.2254, -0.5586, -0.4098,  ...,  0.0218, -0.2897, -1.2498],\n",
       "          [ 0.4362, -0.2704, -0.4241,  ...,  0.8241, -0.4920, -0.6600]],\n",
       "\n",
       "         [[-0.8211,  0.5152,  0.5249,  ...,  0.1404, -0.6924,  0.0622],\n",
       "          [-0.4709,  0.5950,  0.8916,  ...,  0.2483, -0.3027, -0.5450],\n",
       "          [ 0.3659, -0.3047,  1.1170,  ...,  0.4708, -0.0918,  0.0253],\n",
       "          ...,\n",
       "          [-0.4851, -0.1877,  0.9452,  ...,  0.7105, -0.5432,  0.2375],\n",
       "          [-0.0375,  0.0249,  0.8054,  ...,  0.1503, -0.7390, -0.1958],\n",
       "          [-0.3090, -0.0963,  0.4130,  ...,  0.5098, -1.1242,  0.5185]],\n",
       "\n",
       "         [[ 0.4798, -0.1353,  0.5544,  ...,  0.2968, -0.3746,  0.6593],\n",
       "          [ 0.3322, -0.7437,  0.8708,  ...,  1.1492,  0.3167,  1.0756],\n",
       "          [ 0.7758, -0.6997,  0.6755,  ...,  0.7720,  0.3841,  1.1301],\n",
       "          ...,\n",
       "          [ 1.4682, -0.5135,  0.5221,  ...,  0.8014,  0.3860,  0.6253],\n",
       "          [ 0.6786, -0.2747,  1.6234,  ...,  1.4033, -0.1897,  0.2564],\n",
       "          [ 0.7695, -0.2357,  1.6390,  ...,  0.9419,  0.2803,  1.0651]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2201,  0.2775, -0.7502,  ..., -0.3713,  0.7416,  0.2526],\n",
       "          [-0.0134, -0.0861, -1.0108,  ...,  0.5126, -0.3521,  1.2248],\n",
       "          [ 0.2322, -0.0850, -0.4550,  ..., -0.1952,  0.3966,  0.9655],\n",
       "          ...,\n",
       "          [ 0.9134, -0.3890, -0.2733,  ...,  0.4061, -0.4800,  0.4292],\n",
       "          [ 1.0145, -0.0974, -0.9857,  ..., -0.3851, -0.0157,  1.2376],\n",
       "          [ 1.1123, -0.0631, -1.0341,  ...,  0.3478, -0.5503,  0.3121]],\n",
       "\n",
       "         [[-0.9213, -0.8789, -0.2438,  ...,  0.0509, -0.2119,  0.8124],\n",
       "          [ 0.0020,  0.2329,  0.9251,  ...,  0.1851, -0.8334,  0.3199],\n",
       "          [-0.1793,  0.8876,  1.0139,  ...,  0.1181, -0.4645,  0.2682],\n",
       "          ...,\n",
       "          [ 0.1009,  0.2239,  0.1858,  ..., -0.7763,  0.2410, -0.5005],\n",
       "          [ 0.5158, -0.1618, -0.1456,  ..., -0.4726,  0.0542,  0.1710],\n",
       "          [-0.2952, -0.3253, -0.0559,  ..., -0.5951,  0.6951, -0.0819]],\n",
       "\n",
       "         [[ 0.0487, -1.2858,  0.5760,  ..., -1.1660, -0.7861,  0.2553],\n",
       "          [ 0.2222, -0.1931,  0.6927,  ..., -0.6847, -0.3816, -0.7827],\n",
       "          [ 0.5158,  0.1613,  0.4666,  ...,  0.0550,  0.5470, -0.0683],\n",
       "          ...,\n",
       "          [ 0.7786, -0.6064,  0.4811,  ...,  0.6223, -0.4059, -0.6602],\n",
       "          [ 0.3511, -0.4521,  0.5190,  ...,  0.9033,  0.1892,  0.1271],\n",
       "          [ 1.1273, -0.5691, -0.0661,  ...,  0.4949,  0.1739,  0.2991]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.7686,  0.1217,  0.4731,  ...,  0.5222, -0.4841,  0.2394],\n",
       "          [-0.7978, -0.0967,  0.1324,  ...,  0.1796, -1.0674, -1.3030],\n",
       "          [ 0.1550, -0.6695, -0.6073,  ...,  0.5547, -0.9725, -0.4769],\n",
       "          ...,\n",
       "          [-0.0852, -0.8187, -0.2827,  ...,  0.7499,  0.0182, -0.3671],\n",
       "          [ 0.5340, -1.4741, -0.1136,  ...,  0.6616, -0.7146, -0.3363],\n",
       "          [-0.1645, -1.0091, -0.0494,  ...,  1.5152, -0.8667, -0.1064]],\n",
       "\n",
       "         [[ 0.3628, -0.3522,  0.4326,  ..., -0.1074,  0.1194, -0.0090],\n",
       "          [ 0.0988, -0.0040,  0.0564,  ...,  0.2577, -0.3113,  0.3833],\n",
       "          [ 0.9981,  0.2845,  0.4691,  ...,  0.4120,  0.2246, -0.1731],\n",
       "          ...,\n",
       "          [ 0.2857, -0.2591,  0.7867,  ..., -0.1612, -0.2356,  0.5231],\n",
       "          [ 0.1775,  0.4711,  0.5484,  ..., -0.2868,  0.2387,  0.8566],\n",
       "          [ 0.1321,  0.5497,  0.7563,  ..., -0.3021,  0.4652,  0.3788]],\n",
       "\n",
       "         [[ 0.1863,  0.1478,  0.9788,  ...,  0.1784,  1.0022, -0.0378],\n",
       "          [ 0.0834,  0.2402,  0.3958,  ..., -0.2472,  0.6682, -0.1721],\n",
       "          [ 0.8014,  0.2735,  1.6037,  ...,  0.8762,  0.3153,  0.2397],\n",
       "          ...,\n",
       "          [ 0.6896, -0.0993,  0.9472,  ...,  0.1856,  0.6218,  0.0863],\n",
       "          [ 0.2060, -0.0397,  0.7197,  ...,  0.3645,  0.1780, -0.2148],\n",
       "          [ 0.3498,  0.7246,  0.8503,  ...,  0.4037,  0.6225, -0.6409]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5629, -0.9196, -0.0308,  ..., -0.7325,  0.0517, -0.2040],\n",
       "          [ 0.2273, -1.0166,  0.2713,  ..., -0.2726, -0.1278, -0.0556],\n",
       "          [ 0.3674, -0.3585, -0.4544,  ...,  0.1205,  0.0177,  0.2796],\n",
       "          ...,\n",
       "          [-0.2784,  0.2871,  1.1224,  ...,  0.3956, -0.6926,  0.5506],\n",
       "          [ 0.1757, -0.3999,  0.7480,  ...,  0.7566, -0.0312,  0.3815],\n",
       "          [ 0.0671,  0.2127,  0.9590,  ...,  0.4947, -0.2261,  0.4785]],\n",
       "\n",
       "         [[ 0.6993,  1.0486, -0.3772,  ...,  0.0859,  0.7755,  0.0285],\n",
       "          [ 0.7572,  0.8169, -0.7108,  ..., -0.1811,  0.6225,  0.0970],\n",
       "          [ 0.2614,  0.9525, -0.7600,  ...,  1.2102,  0.2999, -0.6550],\n",
       "          ...,\n",
       "          [-0.2351,  0.1584, -1.2095,  ...,  0.8203, -0.1836, -0.2068],\n",
       "          [-0.0456,  0.8236, -0.8989,  ...,  0.2277,  0.4249, -0.5504],\n",
       "          [-0.2385,  0.7401, -1.4430,  ...,  0.9244, -0.1317, -1.2112]],\n",
       "\n",
       "         [[-0.0034,  0.9587, -0.4539,  ..., -0.3674, -1.2688,  0.1763],\n",
       "          [-0.4746, -0.2710, -1.0369,  ..., -0.6990, -0.2828,  0.7660],\n",
       "          [-0.3980,  0.3428, -0.6469,  ..., -0.0324, -0.4925, -0.0123],\n",
       "          ...,\n",
       "          [-0.5926,  0.3466, -0.4530,  ...,  0.1184, -0.0675,  0.2765],\n",
       "          [-0.3117,  0.2362,  0.0321,  ...,  0.1239,  0.0922,  0.2487],\n",
       "          [-0.4264,  0.1100,  0.3723,  ...,  0.4550,  0.2305,  0.4509]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.3117,  0.0282, -1.5342,  ...,  0.4728, -0.0710,  0.8411],\n",
       "          [ 0.0052, -0.1632, -1.0386,  ..., -0.2132, -0.3953,  0.4057],\n",
       "          [-0.0582, -0.7820,  0.0763,  ..., -0.2379,  0.0311,  1.4651],\n",
       "          ...,\n",
       "          [ 0.3678, -0.5739, -0.2988,  ...,  0.6128,  0.2980,  0.6507],\n",
       "          [ 0.6919, -0.4693,  0.1678,  ...,  0.0836,  0.8116,  1.0745],\n",
       "          [ 0.8796, -0.6631, -0.0950,  ...,  0.2617, -0.6466,  0.8857]],\n",
       "\n",
       "         [[ 0.4202, -0.0211,  0.5264,  ..., -0.0028, -0.1428, -0.0946],\n",
       "          [-0.1863,  0.9285,  0.7483,  ...,  0.6978, -0.8529, -0.0422],\n",
       "          [-0.1285,  1.1149,  0.9470,  ...,  0.4749, -0.9430,  0.7616],\n",
       "          ...,\n",
       "          [ 0.0386,  0.4135,  1.0302,  ...,  0.4756,  0.3549, -0.5333],\n",
       "          [-0.0777,  0.7981,  1.1191,  ...,  0.3929,  0.1226, -0.1392],\n",
       "          [ 0.0762, -0.1106,  0.8014,  ...,  0.1101,  0.1883, -0.7242]],\n",
       "\n",
       "         [[ 0.6103, -0.1374, -0.2611,  ..., -0.0139,  0.4851, -0.5347],\n",
       "          [-0.3592,  0.2046, -0.2179,  ..., -0.1501,  0.2470, -0.5958],\n",
       "          [-0.2806,  0.2584, -0.4197,  ..., -0.5606,  1.1999, -0.1708],\n",
       "          ...,\n",
       "          [-0.2461,  0.0871, -0.6504,  ..., -0.0584, -0.2522, -0.2794],\n",
       "          [ 0.1384,  0.3475, -0.4401,  ...,  0.2558,  0.1566, -0.1602],\n",
       "          [-0.6990,  0.1483, -0.8057,  ...,  0.1681,  0.6442, -0.6960]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.8493, -0.6544, -0.5068,  ...,  1.3339, -0.5868, -0.5166],\n",
       "          [ 0.0505, -0.6455,  0.1905,  ...,  0.8596, -0.5670, -0.6471],\n",
       "          [ 0.6162, -0.0366, -0.7357,  ...,  0.5899, -0.4783, -1.2180],\n",
       "          ...,\n",
       "          [ 0.6510, -0.0049,  0.1453,  ...,  0.0153, -0.7310, -0.2953],\n",
       "          [ 0.4596,  0.1032,  0.0367,  ...,  0.8636,  0.0333, -0.3679],\n",
       "          [-0.2109, -0.8577, -0.0326,  ...,  0.2005, -0.6948, -0.5632]],\n",
       "\n",
       "         [[-1.0474,  0.3399, -0.5227,  ..., -0.6938, -0.8600, -0.3958],\n",
       "          [-0.3121,  0.5677, -0.5658,  ..., -0.7132, -1.3746, -0.0962],\n",
       "          [ 0.6967,  0.3000, -0.2780,  ..., -0.5782, -1.4685, -0.0949],\n",
       "          ...,\n",
       "          [-0.2435,  0.4602,  0.3111,  ...,  0.0614, -1.7452,  0.3989],\n",
       "          [ 0.3901, -0.2085, -0.0431,  ..., -1.7399, -1.5024, -0.0454],\n",
       "          [-0.2756,  0.1981,  0.3712,  ..., -0.2844, -0.9077, -0.1074]],\n",
       "\n",
       "         [[-0.2297, -0.3830, -0.1573,  ..., -0.1082,  0.4763, -0.4378],\n",
       "          [ 0.8809, -0.1833,  0.5458,  ...,  0.6069,  0.4649,  0.1801],\n",
       "          [ 1.6612, -0.0605,  0.3162,  ...,  0.5624, -0.1004,  0.0094],\n",
       "          ...,\n",
       "          [ 0.0498,  0.2437,  0.6171,  ...,  0.6619, -0.3556,  0.1426],\n",
       "          [ 0.5282, -0.0434,  0.3695,  ...,  0.9081,  0.1247,  0.1015],\n",
       "          [ 0.5012, -0.0033, -0.0204,  ...,  0.2884,  0.0407,  0.6156]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.4372, -0.0745,  0.1800,  ...,  0.5806,  0.0887, -1.1231],\n",
       "          [ 0.2427,  0.3072, -0.3281,  ...,  0.7577,  0.0703,  0.1026],\n",
       "          [ 0.0857, -0.0199,  0.3463,  ...,  1.1378, -0.3093,  0.8454],\n",
       "          ...,\n",
       "          [ 0.1582, -0.4134,  0.2256,  ...,  0.4479, -0.3063,  0.6373],\n",
       "          [-0.9068, -0.6701,  0.1602,  ...,  0.7160,  0.2304,  0.9012],\n",
       "          [-0.6389, -0.6813,  0.4554,  ..., -0.1113, -0.1682,  0.9636]],\n",
       "\n",
       "         [[-0.0273, -0.5716, -0.6783,  ..., -0.1712,  0.2360, -0.3271],\n",
       "          [-0.0594, -0.3107,  0.7008,  ...,  0.0063, -0.0770, -0.9164],\n",
       "          [ 0.4675, -0.4753,  0.9270,  ..., -0.4112,  0.3148,  0.0935],\n",
       "          ...,\n",
       "          [ 0.4438, -0.3928,  0.3259,  ..., -0.0474,  0.2644, -0.8398],\n",
       "          [ 0.7473, -0.1776, -0.1805,  ..., -1.0866,  0.2885,  0.5567],\n",
       "          [ 0.6129,  0.1132, -0.4120,  ..., -0.1265,  0.2794, -0.5654]],\n",
       "\n",
       "         [[ 0.5373,  0.1213, -0.2160,  ...,  0.4200, -0.0731,  0.3527],\n",
       "          [ 0.7148, -0.0664, -0.5365,  ...,  0.2279, -0.5188, -0.2090],\n",
       "          [ 0.6212,  0.6777, -0.4513,  ..., -0.5869, -0.3737,  0.5374],\n",
       "          ...,\n",
       "          [ 0.2903, -0.3370,  0.5450,  ...,  0.3256, -0.1980,  0.3272],\n",
       "          [ 0.8044,  0.4488,  0.2908,  ..., -0.2556, -0.6095,  0.4796],\n",
       "          [ 0.7676, -0.2332, -0.0103,  ..., -0.3462, -0.2116,  0.0994]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3551,  0.3074, -0.4914,  ..., -0.0642,  0.2640, -0.4210],\n",
       "          [-0.3052, -0.1888, -0.1668,  ...,  0.6795,  0.2963, -0.3295],\n",
       "          [ 0.2963,  0.2047, -0.3342,  ...,  0.3414,  1.1548, -0.0199],\n",
       "          ...,\n",
       "          [ 0.4400,  0.2264, -0.6744,  ..., -0.1020,  0.4551,  0.5167],\n",
       "          [-0.1481,  0.2745, -0.3867,  ...,  0.0515,  0.9977,  0.1055],\n",
       "          [ 0.3068,  0.0353,  0.1047,  ..., -0.1266,  0.2709,  0.8328]],\n",
       "\n",
       "         [[ 1.1300, -0.9457,  0.1017,  ...,  0.5048, -0.3662,  1.2631],\n",
       "          [ 0.0578,  0.0644,  0.3416,  ..., -0.0188, -0.1576,  0.5856],\n",
       "          [ 0.2928, -0.4310,  0.8199,  ...,  0.4363, -0.3299, -0.4501],\n",
       "          ...,\n",
       "          [-0.1615,  0.6011,  0.1242,  ...,  0.7704, -1.0331, -0.3607],\n",
       "          [-0.7760,  0.2341,  0.0323,  ...,  0.4948, -0.9622, -0.6557],\n",
       "          [-1.4747,  0.3326,  0.0848,  ..., -0.0792, -0.0429, -0.1776]],\n",
       "\n",
       "         [[-0.1523, -0.4008, -0.2273,  ...,  0.3772, -0.3884, -0.5330],\n",
       "          [-0.3240, -0.0864, -0.9310,  ...,  0.5014, -0.1730, -1.4862],\n",
       "          [-0.5024,  0.0123,  0.4394,  ...,  0.5101,  0.2662, -0.9192],\n",
       "          ...,\n",
       "          [-0.3438,  0.1262,  0.9042,  ...,  0.2279, -0.0229, -0.3015],\n",
       "          [-0.4710,  0.0527,  0.2866,  ...,  0.2548, -0.5032, -0.7239],\n",
       "          [-0.2995, -0.0332,  0.3279,  ...,  0.7950, -0.8237, -0.2366]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.6912,  0.4963, -0.3709,  ...,  0.2819,  0.2837,  0.7905],\n",
       "          [ 1.1445,  0.3283, -0.6582,  ...,  0.1521,  0.1057,  0.8084],\n",
       "          [ 0.8641,  0.2221,  0.5190,  ...,  0.4812, -0.4082,  0.5426],\n",
       "          ...,\n",
       "          [ 0.1056,  1.0618, -0.0466,  ...,  0.8754, -0.1888,  0.8583],\n",
       "          [ 0.0603,  0.9795,  0.5437,  ...,  0.7307, -0.2803,  1.1554],\n",
       "          [ 0.5359,  0.6725, -0.1722,  ...,  0.4282, -0.3615,  0.5477]],\n",
       "\n",
       "         [[ 0.9446, -0.2230, -0.2273,  ..., -0.3644,  0.1536, -0.1141],\n",
       "          [ 0.2919, -0.3613, -0.6019,  ..., -0.2615,  0.3151,  0.4748],\n",
       "          [-0.1141, -0.0847,  0.1858,  ..., -0.5414, -0.2709, -0.1335],\n",
       "          ...,\n",
       "          [-0.4289,  0.1942, -0.1091,  ..., -0.1722, -0.9414, -0.5269],\n",
       "          [-0.2542,  0.2699, -0.2303,  ..., -0.5390, -0.3825, -0.3393],\n",
       "          [-0.0172,  0.3189, -0.4125,  ..., -0.3359, -1.0586,  0.3381]],\n",
       "\n",
       "         [[-0.5930,  0.3948,  0.0101,  ...,  0.0760,  0.2971, -0.9119],\n",
       "          [-1.6832,  0.5371,  0.0927,  ..., -0.5059, -0.4647,  0.1050],\n",
       "          [-0.7393, -0.2136,  1.1308,  ..., -0.9375, -0.3241,  0.1214],\n",
       "          ...,\n",
       "          [-1.1571, -0.4647,  0.1069,  ..., -0.3997,  0.0288,  0.1901],\n",
       "          [-0.5292, -0.0321,  0.4430,  ...,  0.0949,  0.2015,  0.2362],\n",
       "          [-1.0251, -0.7388,  0.4345,  ..., -0.9010,  0.3916, -0.1217]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5172, -0.2581, -0.5061,  ...,  0.4540, -0.1665,  0.8655],\n",
       "          [-0.6268, -0.8093, -0.1162,  ...,  0.0796,  0.0196,  0.4323],\n",
       "          [-0.7941,  0.7059, -0.3581,  ..., -0.1350, -0.4843,  1.0151],\n",
       "          ...,\n",
       "          [-0.6617,  0.5939,  0.1996,  ..., -0.3468, -0.6171,  0.5296],\n",
       "          [-0.8019,  0.4110, -0.5506,  ..., -0.2329, -0.5710,  1.0135],\n",
       "          [-0.8197,  0.0803, -0.4798,  ...,  0.1603, -1.1295,  0.4515]],\n",
       "\n",
       "         [[-0.2802, -0.6872, -0.8905,  ..., -0.0965,  0.8597,  0.8554],\n",
       "          [-0.3388, -1.2516, -0.1734,  ..., -0.0500, -0.8259,  0.4843],\n",
       "          [-0.4144, -0.4811,  0.0094,  ..., -0.6274, -0.1089,  0.3639],\n",
       "          ...,\n",
       "          [-0.1914, -0.4286, -0.9963,  ...,  0.0811,  0.8317,  0.5924],\n",
       "          [-0.2140, -1.4137, -0.4054,  ..., -0.1707,  0.2757,  1.0112],\n",
       "          [ 0.2785, -0.6407, -0.6307,  ...,  0.0584,  0.5188,  0.4406]],\n",
       "\n",
       "         [[ 0.6342,  0.4916,  0.3904,  ..., -0.4795, -0.0487, -0.7205],\n",
       "          [ 1.1930,  0.2918, -0.5501,  ..., -0.7895,  0.5985, -0.3507],\n",
       "          [ 0.4428, -0.3378, -0.1934,  ..., -0.7660, -0.1535,  0.1471],\n",
       "          ...,\n",
       "          [ 0.4000, -0.5678, -0.1695,  ..., -0.7901, -0.3793,  0.0091],\n",
       "          [ 0.5397, -0.3974,  0.1744,  ..., -0.9266, -0.6170, -0.3361],\n",
       "          [ 0.4444,  0.3144,  0.2892,  ..., -0.8346, -0.5968, -0.9377]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.2882,  0.1833,  0.6489,  ...,  0.7943, -0.5814, -0.3636],\n",
       "          [-0.0290,  0.3383,  0.3678,  ...,  0.6110, -0.9540, -0.5017],\n",
       "          [ 0.0637,  0.0325,  0.6820,  ..., -0.0647, -0.4902, -0.5461],\n",
       "          ...,\n",
       "          [ 0.5569, -0.2920, -0.0857,  ...,  0.6643, -0.5546, -0.8337],\n",
       "          [ 0.0913,  0.2696, -0.0576,  ...,  0.1410, -0.1775, -0.3854],\n",
       "          [ 0.4628, -0.0934, -0.7892,  ...,  0.0674, -1.1177, -0.1349]],\n",
       "\n",
       "         [[-0.8942, -0.3867, -0.6347,  ...,  0.5110,  0.6137, -0.2097],\n",
       "          [-0.2749,  0.4580, -0.3898,  ...,  0.5697,  0.0959,  0.2015],\n",
       "          [-0.3840,  0.2688, -0.6021,  ..., -0.1493,  0.4326,  0.1700],\n",
       "          ...,\n",
       "          [-0.1641,  0.4128, -0.3504,  ...,  0.7539,  0.6983,  0.6508],\n",
       "          [ 0.1155,  0.4623, -0.5586,  ...,  0.6464, -0.0382,  0.0253],\n",
       "          [ 0.7349, -0.0456, -1.5551,  ...,  0.2426,  0.9932,  0.7394]],\n",
       "\n",
       "         [[ 0.3765,  0.1582,  1.0998,  ..., -0.6014, -1.1831,  0.7899],\n",
       "          [ 0.5579,  0.5757,  0.0778,  ..., -0.7068, -0.6598, -0.1069],\n",
       "          [ 0.6393,  0.4932, -0.6525,  ..., -1.3481,  0.0274,  0.4047],\n",
       "          ...,\n",
       "          [ 1.7522, -0.2979,  0.2201,  ..., -1.1990, -0.5803, -0.0273],\n",
       "          [ 0.9928,  0.1567,  0.0074,  ..., -1.0930, -0.4945,  0.5445],\n",
       "          [ 1.1593, -0.6219,  0.9036,  ..., -0.7537, -0.2230,  0.2642]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2955, -0.1136, -1.3123,  ...,  0.3799, -0.0597,  0.5060],\n",
       "          [-0.4032, -0.8645, -0.5523,  ..., -0.6467, -0.8035, -0.7309],\n",
       "          [ 0.0365, -0.7440, -0.3586,  ..., -0.7440, -0.3402, -0.2038],\n",
       "          ...,\n",
       "          [-0.8126, -0.5544, -0.4425,  ..., -0.2872, -0.3158, -0.4817],\n",
       "          [-0.0554,  0.2788, -0.8466,  ..., -0.8527, -0.9519, -0.2466],\n",
       "          [-0.5331, -0.0596, -0.5475,  ..., -1.0015, -0.1509, -0.3182]],\n",
       "\n",
       "         [[ 0.5261, -0.0700, -0.5078,  ...,  0.6069,  0.5225,  1.1582],\n",
       "          [ 0.1442,  0.0684,  0.0052,  ...,  0.2057,  1.2781,  0.6626],\n",
       "          [-0.4052,  0.6322, -0.3965,  ...,  0.0580,  1.1940,  0.6666],\n",
       "          ...,\n",
       "          [-0.1589, -0.2050, -0.2939,  ..., -0.7124,  0.8033,  1.0282],\n",
       "          [-0.0072,  0.2693, -1.0032,  ..., -0.0723,  0.9467,  0.9947],\n",
       "          [-0.7500,  0.4423, -0.1219,  ..., -1.0140,  0.8905,  0.6415]],\n",
       "\n",
       "         [[ 0.5246, -0.3746,  1.0520,  ...,  0.2850, -0.2778,  0.5455],\n",
       "          [ 0.4859, -0.2978,  0.4059,  ..., -0.4381, -0.5246, -0.3634],\n",
       "          [ 0.1075, -0.5523, -0.1948,  ..., -0.0798, -1.0127, -0.5184],\n",
       "          ...,\n",
       "          [ 0.2521, -1.1045,  0.5081,  ..., -0.6401, -1.1647, -0.7775],\n",
       "          [ 0.4540, -0.3304,  0.0990,  ...,  0.6870, -1.3860, -0.8339],\n",
       "          [ 0.7225,  0.0068,  0.2647,  ...,  0.4413, -0.5349, -0.9514]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-7.2631e-01, -5.5715e-01,  1.3711e-01,  ..., -1.4661e+00,\n",
       "           -1.1020e+00,  1.2068e+00],\n",
       "          [-8.1904e-01, -1.3143e-01,  3.3772e-01,  ..., -4.3216e-01,\n",
       "           -1.9737e-01,  7.1794e-01],\n",
       "          [-2.4627e-01, -4.9449e-01,  3.4376e-01,  ..., -1.6070e-01,\n",
       "           -9.8103e-01,  9.2429e-01],\n",
       "          ...,\n",
       "          [-2.7012e-01, -5.7869e-01,  8.1173e-02,  ...,  1.1553e-01,\n",
       "           -8.4791e-01, -2.1853e-01],\n",
       "          [ 3.1315e-01, -5.0247e-01,  4.0504e-01,  ..., -6.2136e-01,\n",
       "           -1.6322e+00,  8.3092e-01],\n",
       "          [-1.5402e-01,  6.0704e-02,  1.3729e-01,  ..., -3.7824e-01,\n",
       "           -1.0232e+00,  7.1306e-01]],\n",
       "\n",
       "         [[ 2.5792e-01, -3.7500e-01, -4.8251e-01,  ..., -2.9575e-01,\n",
       "           -9.3541e-03, -2.6456e-01],\n",
       "          [-1.0463e-01, -4.3539e-01, -1.5671e-01,  ..., -4.3681e-01,\n",
       "           -4.8458e-01,  5.9085e-01],\n",
       "          [ 1.0155e-01,  6.7458e-01, -4.2596e-01,  ..., -5.8831e-01,\n",
       "            1.9768e-01,  4.3272e-01],\n",
       "          ...,\n",
       "          [ 4.9517e-01,  3.1197e-02, -5.6744e-01,  ...,  6.7005e-02,\n",
       "            8.5907e-02, -2.0902e-01],\n",
       "          [-2.0884e-01,  2.8007e-01,  1.1268e-02,  ..., -4.1291e-01,\n",
       "           -3.5563e-01, -7.4907e-01],\n",
       "          [-6.7061e-02,  5.0081e-01,  3.2925e-01,  ...,  2.3180e-01,\n",
       "            2.2807e-02, -2.7490e-02]],\n",
       "\n",
       "         [[-5.5494e-01,  1.6032e+00, -6.8883e-01,  ..., -1.3270e-01,\n",
       "            9.0770e-01, -5.7558e-01],\n",
       "          [-1.2280e-01,  4.7460e-01, -9.9623e-01,  ..., -1.0343e+00,\n",
       "            4.3387e-01, -7.5875e-01],\n",
       "          [-3.0975e-01,  1.6439e-01, -2.3649e-01,  ..., -8.6897e-01,\n",
       "           -6.7520e-01, -4.6590e-01],\n",
       "          ...,\n",
       "          [-3.8165e-01, -3.1876e-01,  9.0800e-03,  ..., -5.8027e-01,\n",
       "           -3.6163e-01, -3.4339e-01],\n",
       "          [ 5.1118e-01,  2.1430e-01, -6.9404e-01,  ..., -6.8432e-01,\n",
       "            7.9137e-02, -7.4457e-01],\n",
       "          [ 4.5452e-02, -4.4350e-02, -5.8325e-01,  ..., -1.0201e+00,\n",
       "            5.1243e-01, -2.5450e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3495e-01,  1.2518e+00, -3.0855e-01,  ...,  3.2225e-01,\n",
       "           -3.3102e-01,  3.6424e-01],\n",
       "          [-7.7812e-02, -3.3957e-01,  7.1004e-01,  ..., -1.1608e-01,\n",
       "           -4.3129e-01,  3.2026e-02],\n",
       "          [-4.7809e-01,  7.3141e-01,  7.1301e-01,  ..., -1.8268e-01,\n",
       "            2.9380e-01, -2.6735e-02],\n",
       "          ...,\n",
       "          [-5.7825e-01,  6.4915e-01,  9.4460e-01,  ..., -1.7771e-01,\n",
       "            5.3803e-01,  4.5122e-01],\n",
       "          [-5.3428e-01,  1.1441e+00,  6.7477e-01,  ..., -6.2507e-01,\n",
       "           -2.9388e-01,  5.2857e-01],\n",
       "          [-6.9468e-01,  1.2398e+00,  5.2427e-01,  ...,  3.0358e-01,\n",
       "            3.3047e-01,  3.5822e-01]],\n",
       "\n",
       "         [[ 3.2104e-02,  2.9617e-01, -5.2947e-01,  ..., -7.1874e-01,\n",
       "           -5.4521e-01,  5.6207e-01],\n",
       "          [-2.4477e-01,  1.8346e-01, -4.2460e-01,  ..., -6.8889e-01,\n",
       "           -1.3713e-01,  2.0573e-01],\n",
       "          [ 3.6962e-01,  6.5055e-01, -4.8944e-01,  ..., -3.9247e-02,\n",
       "           -1.7918e-01, -6.5258e-01],\n",
       "          ...,\n",
       "          [ 1.5211e+00,  2.6271e-01, -4.2805e-01,  ...,  1.8258e-03,\n",
       "            8.1199e-02, -4.3255e-01],\n",
       "          [ 8.0859e-01, -4.1416e-01,  1.2324e-03,  ...,  1.1279e-01,\n",
       "            1.6882e-01, -2.5118e-01],\n",
       "          [ 1.0088e+00,  8.7577e-02, -3.9229e-01,  ...,  9.5026e-02,\n",
       "            4.1520e-01, -1.3198e-02]],\n",
       "\n",
       "         [[-5.9253e-01, -3.6804e-01, -3.6954e-01,  ..., -5.0606e-01,\n",
       "           -1.3623e-01,  1.0658e+00],\n",
       "          [ 1.0262e-01, -6.2312e-01, -1.2511e+00,  ..., -1.2472e+00,\n",
       "           -1.7170e-01,  3.2026e-01],\n",
       "          [ 6.4567e-01, -6.2966e-01, -1.5560e+00,  ..., -7.1586e-01,\n",
       "            5.0711e-01,  1.2442e+00],\n",
       "          ...,\n",
       "          [ 6.8288e-01,  2.3625e-02, -1.4285e-01,  ..., -8.6532e-01,\n",
       "           -3.7066e-01,  2.1277e-01],\n",
       "          [ 1.1630e+00, -9.4550e-02, -7.5750e-01,  ..., -9.6829e-01,\n",
       "           -4.5217e-01,  4.2872e-01],\n",
       "          [ 7.5662e-02, -3.2748e-01, -1.2781e-01,  ..., -6.1721e-01,\n",
       "           -7.0719e-01,  6.1101e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.7932,  0.0360, -0.3467,  ..., -0.2641, -0.2611, -0.2574],\n",
       "          [-0.2097, -0.4002, -1.0713,  ..., -0.0490, -0.4054, -0.1124],\n",
       "          [-0.7695, -0.1957, -0.3731,  ..., -0.7902, -0.2655, -0.1422],\n",
       "          ...,\n",
       "          [ 0.4088,  0.0064, -0.4861,  ..., -0.3515,  0.3686, -0.0331],\n",
       "          [-0.2372,  0.0129, -0.8567,  ...,  0.1997,  0.2116, -0.3484],\n",
       "          [ 0.8424,  0.5238, -0.1656,  ..., -0.2772,  0.1349, -0.1644]],\n",
       "\n",
       "         [[ 0.4388,  0.0312, -0.2861,  ...,  0.4953, -0.2710, -0.7486],\n",
       "          [-0.1583,  0.3404,  0.0807,  ...,  0.3457,  0.2004, -0.2187],\n",
       "          [-0.8157,  0.6648, -0.4205,  ...,  0.1825,  0.7988, -0.6104],\n",
       "          ...,\n",
       "          [-0.4550, -0.7984, -0.0814,  ..., -0.0901,  0.6297, -0.4201],\n",
       "          [-0.3299,  0.0097, -0.1490,  ...,  0.0826,  0.4675, -0.1996],\n",
       "          [-0.4921,  0.0028,  0.8305,  ..., -0.7502,  0.6655,  0.1207]],\n",
       "\n",
       "         [[-0.7142,  0.1918, -1.1053,  ...,  0.0444, -0.3979, -0.4130],\n",
       "          [-0.4368,  0.0070, -0.7974,  ...,  0.2046, -0.4533, -0.3820],\n",
       "          [ 0.3356, -0.3053, -0.7620,  ...,  0.2481, -0.2443, -0.5146],\n",
       "          ...,\n",
       "          [ 0.1049, -0.3765, -1.0783,  ...,  0.7665, -0.9387, -0.2929],\n",
       "          [-0.4851,  0.1697, -0.6146,  ...,  0.6497, -0.2374,  0.3143],\n",
       "          [-0.3405,  0.1051, -0.2692,  ...,  0.1361, -0.0373, -0.5380]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3860,  0.2160,  0.7917,  ..., -0.0602,  0.1528, -0.1804],\n",
       "          [ 0.0663, -0.2252,  0.8498,  ..., -0.3967,  1.2230, -0.5873],\n",
       "          [-0.6719,  0.0088,  0.0097,  ...,  0.0281,  0.6404,  0.1442],\n",
       "          ...,\n",
       "          [-0.1857, -0.3425, -0.1422,  ...,  0.1083,  0.4302, -0.3050],\n",
       "          [-0.6240, -0.1338,  0.4155,  ...,  0.4832,  0.5814, -0.3955],\n",
       "          [-0.5136, -0.3932,  0.0267,  ...,  0.2235,  0.5164, -0.4653]],\n",
       "\n",
       "         [[-0.7424, -0.1744, -0.6729,  ...,  0.1315,  0.1305,  0.3485],\n",
       "          [ 0.1845,  0.7571,  0.0647,  ..., -0.1340,  0.9699,  0.6890],\n",
       "          [-0.0115,  1.1685, -0.4394,  ..., -0.2655,  0.0883, -0.1041],\n",
       "          ...,\n",
       "          [ 0.1465,  1.0015, -0.7996,  ..., -0.0258,  0.0657, -0.0972],\n",
       "          [ 0.7129,  0.9708, -0.4897,  ..., -0.1068,  0.0717,  0.4517],\n",
       "          [ 0.5035,  1.6849,  0.5841,  ...,  0.4504,  0.5198,  0.3538]],\n",
       "\n",
       "         [[ 0.6732, -0.1484, -0.6257,  ..., -0.3139, -0.0546,  0.3544],\n",
       "          [ 0.3984,  0.4238, -0.0917,  ..., -0.6112, -0.3500,  0.9790],\n",
       "          [ 0.0134,  0.2970, -0.4554,  ...,  0.0735, -0.3535,  0.4262],\n",
       "          ...,\n",
       "          [ 0.1388,  0.5276,  0.3025,  ..., -0.6272,  0.4363,  0.2742],\n",
       "          [-0.2944,  0.3051, -0.4941,  ..., -0.4019,  0.0199, -0.1066],\n",
       "          [ 0.5520,  0.4723, -0.1337,  ..., -1.1482,  0.1803,  0.4218]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-4.9847e-02,  1.2464e+00, -3.4818e-02,  ..., -1.1013e+00,\n",
       "           -3.4231e-01,  3.2035e-01],\n",
       "          [ 3.7899e-01,  3.6852e-01,  1.0754e+00,  ..., -6.0363e-01,\n",
       "           -3.2460e-01,  1.0873e+00],\n",
       "          [ 2.9808e-01,  8.3841e-01, -1.6193e-02,  ..., -9.1252e-01,\n",
       "           -3.4274e-01,  7.1842e-01],\n",
       "          ...,\n",
       "          [ 8.1546e-01,  2.1184e-01, -8.1633e-01,  ..., -3.2819e-01,\n",
       "           -4.1544e-01,  5.8761e-01],\n",
       "          [ 4.4729e-01,  9.6315e-01, -4.6151e-01,  ..., -1.8233e-01,\n",
       "           -6.2348e-01,  1.1789e+00],\n",
       "          [ 1.8664e-01,  6.6644e-01, -7.5230e-01,  ..., -6.6220e-01,\n",
       "           -2.9211e-01,  1.1021e+00]],\n",
       "\n",
       "         [[-5.5230e-01, -6.6491e-01,  4.6933e-01,  ...,  4.2235e-02,\n",
       "            9.8783e-02,  1.7643e-01],\n",
       "          [-8.1391e-01, -9.3083e-01,  1.1875e+00,  ..., -1.3591e-01,\n",
       "            6.3918e-01, -6.2235e-01],\n",
       "          [ 2.3747e-01, -5.9692e-01,  6.2696e-01,  ...,  3.4987e-02,\n",
       "            1.5648e-01, -3.9574e-01],\n",
       "          ...,\n",
       "          [ 1.1268e-01, -1.8116e-01,  4.1005e-01,  ...,  3.5574e-01,\n",
       "            4.0811e-01, -7.8342e-01],\n",
       "          [ 1.1277e-01, -2.8529e-01,  9.2962e-01,  ...,  8.0463e-01,\n",
       "            7.9287e-01,  6.0026e-03],\n",
       "          [ 2.4422e-01, -4.3142e-02,  8.2421e-01,  ...,  9.0636e-01,\n",
       "           -2.0755e-03, -2.3573e-01]],\n",
       "\n",
       "         [[-4.3920e-02,  1.5092e-01, -5.3570e-01,  ..., -8.0334e-01,\n",
       "            7.9843e-01,  3.9844e-01],\n",
       "          [-6.4536e-01, -6.4247e-01, -3.4346e-01,  ...,  1.6641e-01,\n",
       "           -2.6609e-01,  1.0413e-01],\n",
       "          [-9.1016e-02, -8.7490e-01, -2.3274e-01,  ...,  4.3608e-01,\n",
       "           -3.4615e-01, -1.7257e-01],\n",
       "          ...,\n",
       "          [ 1.0116e-01, -4.5330e-01, -1.7989e-01,  ...,  3.8555e-01,\n",
       "           -5.7792e-01, -3.5103e-01],\n",
       "          [ 2.7784e-01, -2.7438e-01, -1.4997e-01,  ..., -3.3992e-01,\n",
       "           -4.4191e-01, -2.4547e-01],\n",
       "          [ 6.8464e-01,  6.9907e-02,  5.0367e-01,  ..., -1.0043e-01,\n",
       "           -1.5418e-01, -1.0341e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.2493e-01, -1.2501e+00,  1.6926e-01,  ...,  2.9894e-01,\n",
       "            1.4625e-01,  7.2757e-01],\n",
       "          [-5.3058e-01, -7.0498e-01,  5.2856e-01,  ...,  1.0455e-01,\n",
       "           -4.3230e-01,  5.1950e-01],\n",
       "          [ 2.4027e-01, -4.7628e-01,  4.1635e-01,  ..., -5.4853e-01,\n",
       "           -7.0232e-03,  5.6517e-01],\n",
       "          ...,\n",
       "          [-2.9225e-01,  4.2791e-01,  3.7944e-01,  ..., -2.2465e-01,\n",
       "            7.0646e-01,  5.7440e-01],\n",
       "          [-5.3273e-01,  1.4562e-01,  9.6574e-02,  ..., -3.0928e-02,\n",
       "            2.1335e-01,  2.1210e-01],\n",
       "          [ 2.1935e-01,  5.6929e-01,  7.2752e-01,  ..., -2.4958e-01,\n",
       "           -1.7534e-01, -1.5906e-01]],\n",
       "\n",
       "         [[ 6.3744e-01, -1.3281e-01, -9.7926e-01,  ..., -6.7107e-02,\n",
       "            6.9743e-02,  6.7234e-01],\n",
       "          [ 3.8405e-01,  1.9466e-01, -1.4821e+00,  ...,  7.2969e-01,\n",
       "            4.8176e-01,  7.2684e-01],\n",
       "          [ 9.2627e-01, -6.8845e-01, -6.7500e-01,  ..., -6.3482e-02,\n",
       "           -4.7910e-01,  9.0071e-01],\n",
       "          ...,\n",
       "          [ 9.3556e-01,  2.6871e-01,  6.7682e-01,  ...,  2.1412e-01,\n",
       "            9.3841e-02,  6.2760e-01],\n",
       "          [ 4.5719e-01, -1.2449e-01,  4.0929e-01,  ...,  1.8414e-01,\n",
       "           -4.8604e-01,  6.6261e-01],\n",
       "          [-6.8214e-02, -3.9281e-01,  1.0581e-01,  ..., -5.7951e-02,\n",
       "            2.3674e-01,  2.8777e-02]],\n",
       "\n",
       "         [[-3.4245e-01, -5.2510e-01,  2.7902e-01,  ...,  5.1900e-01,\n",
       "            3.3016e-01, -9.2593e-01],\n",
       "          [-1.5974e-01, -3.5439e-01, -6.7787e-01,  ...,  5.6335e-01,\n",
       "           -2.7562e-01,  1.0679e-01],\n",
       "          [-9.9840e-02, -8.0961e-01,  1.7568e-01,  ...,  9.1744e-02,\n",
       "           -2.2126e-01, -3.9577e-01],\n",
       "          ...,\n",
       "          [ 2.5925e-01, -8.0543e-01, -1.7902e-02,  ...,  2.9359e-01,\n",
       "           -3.3864e-01,  1.6333e-01],\n",
       "          [-3.7365e-01, -1.4121e+00, -4.0916e-01,  ...,  3.5648e-01,\n",
       "           -3.6686e-01,  3.2582e-01],\n",
       "          [ 1.3449e-03, -1.1623e+00, -8.3797e-01,  ..., -1.8843e-02,\n",
       "           -9.8400e-02,  1.1526e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.1172, -0.5604,  1.4978,  ..., -0.1911, -0.1719,  0.2954],\n",
       "          [ 0.2492, -0.2182,  0.7008,  ..., -0.0060, -0.2122,  0.3849],\n",
       "          [-0.1115, -0.9736,  0.9995,  ...,  0.0815,  0.2990, -0.1026],\n",
       "          ...,\n",
       "          [ 0.0531, -0.8019,  0.2958,  ...,  0.2329, -0.3753,  0.1121],\n",
       "          [ 0.4325, -1.0172,  0.6710,  ...,  0.0397,  0.0770,  0.0490],\n",
       "          [-0.1722, -0.8018,  0.2027,  ...,  0.2672, -0.0290,  0.3480]],\n",
       "\n",
       "         [[-0.2399,  1.0859, -0.4031,  ..., -0.5598, -0.6841,  0.0275],\n",
       "          [-0.4372,  0.6850, -0.4110,  ...,  0.0348, -0.1430,  0.5967],\n",
       "          [-0.4597,  0.6681, -0.1123,  ...,  0.3072, -0.3976,  0.9711],\n",
       "          ...,\n",
       "          [ 0.5190,  0.2190, -0.0539,  ..., -0.1323,  0.0295,  0.8630],\n",
       "          [-0.2952, -0.0380, -0.2226,  ..., -0.3136, -0.1792,  0.6616],\n",
       "          [ 0.4301, -0.1569,  0.0201,  ...,  0.1733, -0.4949,  0.0216]],\n",
       "\n",
       "         [[ 1.2454,  0.2103, -0.1874,  ..., -0.3708,  0.0280,  0.5340],\n",
       "          [ 0.0658,  0.0781,  0.4622,  ...,  0.0394, -0.1249,  0.1308],\n",
       "          [-0.6801, -0.2052,  0.4123,  ...,  0.2193, -0.6831,  0.2252],\n",
       "          ...,\n",
       "          [-0.2483,  0.3262,  0.4418,  ...,  0.5746, -0.8290,  0.5389],\n",
       "          [ 0.0167,  0.5384,  1.2057,  ...,  0.3747, -0.4308,  0.3815],\n",
       "          [-0.2449,  0.0553,  0.6227,  ..., -0.4590, -0.9371,  0.4812]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2340,  0.4511,  0.3898,  ..., -0.1112, -0.8156,  0.9466],\n",
       "          [ 0.0798, -0.3251,  0.6139,  ..., -0.2845, -0.3852,  0.4199],\n",
       "          [ 0.9367,  0.8436,  1.3599,  ..., -0.6811, -1.2308,  0.4663],\n",
       "          ...,\n",
       "          [ 0.8936,  0.6154,  1.1031,  ..., -0.0485, -0.5858,  0.2594],\n",
       "          [ 0.8199,  0.7707,  1.2903,  ..., -0.0582, -0.6594,  0.0668],\n",
       "          [ 1.1439,  0.6361,  1.0971,  ..., -0.2299, -0.7374, -0.2461]],\n",
       "\n",
       "         [[-0.1942, -0.7198, -0.2591,  ..., -0.4890, -0.1678,  0.5310],\n",
       "          [ 0.8095, -0.6387, -0.6871,  ...,  0.1071,  0.4165, -0.2584],\n",
       "          [ 0.3143, -0.0122, -1.1676,  ..., -0.1435, -0.1272,  0.6998],\n",
       "          ...,\n",
       "          [ 0.1882, -0.1775, -0.3755,  ...,  0.3011,  0.6856,  1.0927],\n",
       "          [ 0.9178, -0.2456, -0.6454,  ..., -0.1937,  0.0414,  0.7866],\n",
       "          [ 0.2044, -0.2130, -1.2910,  ..., -0.0184,  0.5682,  0.1528]],\n",
       "\n",
       "         [[ 0.1926, -0.4569,  0.8795,  ..., -0.6960, -0.1518,  0.0570],\n",
       "          [ 0.2930, -0.2558,  0.9524,  ..., -0.2943, -1.0279, -0.4146],\n",
       "          [ 0.8527,  0.1369, -0.0872,  ..., -0.0781, -0.6316,  0.2052],\n",
       "          ...,\n",
       "          [ 0.2470,  0.1360,  0.5453,  ..., -0.1333,  0.0973, -0.6353],\n",
       "          [ 0.3260,  0.2258, -0.2218,  ...,  0.3450,  0.8868, -0.1819],\n",
       "          [ 0.0254,  0.1760,  0.9467,  ...,  0.5431, -0.0353, -0.4059]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-2.1037e-01, -7.2462e-01, -2.6270e-01,  ...,  4.6986e-01,\n",
       "            3.5475e-02,  1.5660e-01],\n",
       "          [-4.6224e-01,  1.6003e-01, -2.8054e-01,  ..., -1.1419e-01,\n",
       "            3.4371e-01,  2.9397e-01],\n",
       "          [-8.6506e-01, -2.6815e-02,  7.9124e-02,  ...,  2.8243e-01,\n",
       "           -1.4719e-01,  2.7903e-01],\n",
       "          ...,\n",
       "          [-4.0270e-01,  4.3975e-01, -1.6359e-01,  ..., -9.5447e-02,\n",
       "            1.3517e-01,  1.8592e-01],\n",
       "          [ 1.7937e-02,  4.6530e-01, -5.1148e-01,  ...,  1.1981e+00,\n",
       "           -1.4524e-01,  8.8534e-01],\n",
       "          [-1.3748e-01, -3.4959e-01, -1.5335e-01,  ...,  7.8550e-01,\n",
       "           -2.1907e-01,  4.9817e-01]],\n",
       "\n",
       "         [[-8.8247e-01,  2.2630e-01,  1.1966e+00,  ..., -1.4489e-02,\n",
       "           -3.3502e-01,  6.3697e-01],\n",
       "          [-1.1376e+00, -5.5342e-01,  5.1743e-01,  ..., -5.6566e-01,\n",
       "           -9.6486e-02, -3.9794e-01],\n",
       "          [-5.3376e-01,  2.0177e-04,  3.0971e-01,  ..., -1.9638e-01,\n",
       "           -1.1341e-01, -3.9896e-01],\n",
       "          ...,\n",
       "          [-2.5363e-01, -6.3449e-02,  3.9094e-01,  ..., -9.3729e-02,\n",
       "           -3.8417e-01,  2.5132e-01],\n",
       "          [-9.0940e-01, -4.5080e-01,  2.4546e-01,  ...,  4.3286e-01,\n",
       "           -5.4332e-01,  5.7672e-01],\n",
       "          [-6.3563e-01, -3.6452e-02,  2.5057e-01,  ...,  3.9452e-01,\n",
       "            4.5255e-01, -4.9673e-01]],\n",
       "\n",
       "         [[-8.8821e-01, -6.9825e-02,  3.6531e-01,  ...,  4.3073e-01,\n",
       "            3.8753e-01,  7.3199e-01],\n",
       "          [ 2.5911e-02,  9.2113e-01, -4.3127e-01,  ...,  7.3540e-01,\n",
       "           -8.6321e-01,  1.0646e-01],\n",
       "          [-9.7073e-01, -5.7967e-04,  5.8482e-01,  ..., -2.8925e-01,\n",
       "           -7.9839e-01, -5.4917e-01],\n",
       "          ...,\n",
       "          [-2.5391e-01,  8.3138e-01,  4.1946e-01,  ...,  6.7063e-01,\n",
       "           -5.8139e-01, -7.6199e-03],\n",
       "          [-5.2950e-01,  1.7261e-01,  6.0937e-01,  ...,  2.3198e-01,\n",
       "           -8.1986e-01, -3.4266e-01],\n",
       "          [-9.1577e-01,  9.9402e-02,  8.8548e-01,  ...,  1.1980e-02,\n",
       "           -7.3717e-01,  8.3616e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2910e-01, -2.8046e-01,  1.0178e-01,  ..., -5.2271e-01,\n",
       "           -7.5471e-01,  9.7330e-02],\n",
       "          [-7.1631e-01, -6.7002e-01,  1.7467e-01,  ...,  1.8416e-01,\n",
       "            2.8056e-01, -1.3037e-01],\n",
       "          [-1.1495e+00,  4.3356e-01,  1.7847e-01,  ...,  4.4654e-01,\n",
       "           -4.4679e-02,  1.0991e-01],\n",
       "          ...,\n",
       "          [-4.0415e-01, -8.1605e-02, -5.3500e-01,  ...,  6.0240e-01,\n",
       "            5.3247e-01,  3.1601e-01],\n",
       "          [-7.9693e-01, -1.8664e-01,  2.9593e-01,  ...,  7.6977e-01,\n",
       "            3.4653e-01, -3.5990e-02],\n",
       "          [ 5.6294e-01, -5.4900e-01, -4.8994e-01,  ..., -6.8490e-02,\n",
       "           -1.8762e-02, -3.0620e-01]],\n",
       "\n",
       "         [[-1.0828e-03,  7.1430e-01,  1.2529e-01,  ...,  3.5922e-01,\n",
       "            3.1549e-03, -4.6255e-01],\n",
       "          [ 1.3395e-01,  1.5115e+00, -1.2062e+00,  ...,  3.6151e-01,\n",
       "           -1.3508e-01, -5.9730e-01],\n",
       "          [-1.7587e-01,  1.1155e+00, -2.2169e-01,  ...,  5.7796e-01,\n",
       "           -2.8344e-02, -4.5293e-01],\n",
       "          ...,\n",
       "          [-3.8063e-01,  6.6730e-01, -3.1219e-01,  ...,  2.8803e-01,\n",
       "            3.2066e-01, -1.0369e-01],\n",
       "          [-3.7096e-01,  8.7839e-01,  2.8832e-02,  ...,  5.1984e-01,\n",
       "            7.5268e-01, -6.6156e-01],\n",
       "          [ 3.8182e-01,  8.8278e-01, -4.1816e-01,  ...,  3.2795e-01,\n",
       "            3.6548e-01, -1.2280e-01]],\n",
       "\n",
       "         [[ 1.9960e-01, -1.7090e-01, -5.7051e-01,  ...,  1.2391e-01,\n",
       "           -1.0762e+00,  5.3262e-01],\n",
       "          [-2.1449e-01, -6.9824e-01, -3.7419e-03,  ..., -3.9335e-01,\n",
       "           -9.8834e-02, -3.3149e-02],\n",
       "          [ 2.7997e-01,  6.9403e-02, -1.4521e-01,  ...,  6.5816e-01,\n",
       "           -6.7024e-01,  2.4950e-01],\n",
       "          ...,\n",
       "          [ 9.2212e-01, -2.7874e-01,  3.5286e-01,  ...,  9.2574e-01,\n",
       "           -8.5661e-01, -4.5267e-01],\n",
       "          [-8.2662e-02, -5.1584e-01,  6.0263e-02,  ...,  4.7792e-01,\n",
       "           -1.0722e+00, -3.7110e-01],\n",
       "          [ 3.3395e-01, -5.2049e-01,  8.3770e-02,  ...,  1.1427e-01,\n",
       "           -1.1408e+00,  4.1744e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-2.2935e-04, -6.6260e-01, -1.0795e+00,  ..., -2.7503e-01,\n",
       "           -1.6273e-01,  4.3102e-01],\n",
       "          [-3.1502e-01, -6.8791e-01, -3.4261e-01,  ..., -1.5927e-01,\n",
       "           -3.6623e-01, -7.2830e-01],\n",
       "          [-9.9564e-01, -9.4157e-01, -4.5844e-01,  ..., -5.9725e-01,\n",
       "           -8.1526e-01, -1.5343e-01],\n",
       "          ...,\n",
       "          [-1.1989e+00, -1.1588e+00,  8.8282e-02,  ..., -7.3245e-01,\n",
       "           -4.8100e-01, -3.1372e-01],\n",
       "          [-5.3316e-01, -7.0943e-01, -1.2276e+00,  ..., -1.0856e+00,\n",
       "           -1.8129e-01, -5.3508e-01],\n",
       "          [-5.9108e-01, -9.8833e-01, -2.1215e-01,  ..., -1.0823e+00,\n",
       "           -5.4801e-01, -4.8325e-01]],\n",
       "\n",
       "         [[ 1.3169e-01,  2.7740e-01, -2.8586e-01,  ..., -6.4639e-01,\n",
       "           -2.9818e-01, -4.3514e-01],\n",
       "          [-1.1530e-01, -7.2223e-01, -8.5245e-02,  ..., -5.4084e-01,\n",
       "           -3.9011e-01,  7.2305e-01],\n",
       "          [-2.2819e-02,  4.9191e-01,  1.0770e-02,  ..., -6.1183e-01,\n",
       "            4.0088e-01,  6.7424e-01],\n",
       "          ...,\n",
       "          [ 3.4687e-01,  7.7368e-01,  7.4497e-01,  ..., -6.6777e-01,\n",
       "           -2.1907e-01,  7.8808e-01],\n",
       "          [ 3.7813e-01,  2.5921e-01,  1.2164e-01,  ..., -9.2335e-01,\n",
       "           -1.0388e-01,  8.4296e-01],\n",
       "          [ 2.6576e-01,  4.0645e-02, -2.3579e-01,  ..., -1.0613e+00,\n",
       "           -3.9677e-01,  5.2545e-02]],\n",
       "\n",
       "         [[-6.0795e-03,  6.5313e-02, -1.3025e-01,  ..., -2.9571e-01,\n",
       "           -6.0072e-02,  5.6691e-01],\n",
       "          [ 4.8139e-01, -5.3826e-01, -8.6615e-01,  ..., -2.9226e-01,\n",
       "            1.9933e-01,  4.9298e-01],\n",
       "          [ 1.4243e+00, -8.5920e-01, -7.0914e-01,  ...,  1.9484e-02,\n",
       "           -4.7455e-01,  5.6949e-01],\n",
       "          ...,\n",
       "          [ 1.2202e+00, -5.8953e-01, -4.8053e-01,  ...,  7.0181e-01,\n",
       "           -4.8294e-01,  2.7952e-01],\n",
       "          [ 1.7874e+00, -4.9718e-01, -1.0092e+00,  ...,  6.8955e-01,\n",
       "           -3.8547e-01,  5.1569e-01],\n",
       "          [ 7.3449e-01, -4.6449e-01, -8.4414e-01,  ...,  1.4805e-01,\n",
       "           -8.4391e-01, -4.4884e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.0228e-01, -3.4184e-01, -4.5143e-01,  ...,  9.0283e-02,\n",
       "           -1.0297e-01, -1.0498e-01],\n",
       "          [-5.3772e-01, -7.1124e-01,  2.1869e-01,  ..., -1.4630e-01,\n",
       "            6.5893e-01, -6.9548e-01],\n",
       "          [-4.3844e-01, -1.4422e+00,  7.4875e-03,  ...,  3.8684e-01,\n",
       "            6.6434e-01, -1.1915e+00],\n",
       "          ...,\n",
       "          [-8.7893e-01, -9.5349e-01, -2.4293e-01,  ..., -2.3344e-02,\n",
       "            3.3364e-01,  2.0602e-01],\n",
       "          [-4.8685e-01, -4.5165e-01, -6.6738e-02,  ...,  4.8390e-01,\n",
       "            1.3430e+00, -1.9973e-01],\n",
       "          [-6.1446e-04,  2.7490e-01,  1.7657e-02,  ...,  4.4051e-01,\n",
       "            5.1446e-01, -3.9054e-01]],\n",
       "\n",
       "         [[-7.3626e-01,  6.2834e-01,  1.2736e+00,  ...,  4.5308e-01,\n",
       "           -1.8840e-01,  6.4309e-01],\n",
       "          [-8.0611e-02,  3.4822e-01,  8.6930e-01,  ..., -2.5516e-01,\n",
       "           -2.0236e-01,  1.3494e+00],\n",
       "          [ 1.0146e+00,  6.1379e-01,  6.3805e-01,  ...,  6.8693e-01,\n",
       "           -2.3757e-01,  1.1048e+00],\n",
       "          ...,\n",
       "          [ 8.9438e-01,  7.9429e-01,  5.3388e-01,  ..., -1.0036e-01,\n",
       "            5.2322e-01,  1.4812e+00],\n",
       "          [ 7.3222e-01,  2.0186e-02, -1.5368e-01,  ...,  5.3903e-01,\n",
       "           -3.8392e-01,  1.5780e+00],\n",
       "          [ 1.9809e-01, -1.0171e-01,  8.8122e-02,  ..., -2.4165e-01,\n",
       "            1.3556e-02,  1.1683e+00]],\n",
       "\n",
       "         [[-7.3599e-01,  3.5262e-01, -4.7936e-02,  ...,  2.0920e-02,\n",
       "           -4.6453e-01, -3.8281e-01],\n",
       "          [ 8.3766e-02,  2.0976e-01, -1.2374e-01,  ...,  4.2156e-01,\n",
       "           -1.6281e-01,  1.3760e-01],\n",
       "          [ 5.2754e-02, -1.4168e-01, -2.5731e-01,  ..., -4.7189e-01,\n",
       "            2.3777e-02,  9.1131e-01],\n",
       "          ...,\n",
       "          [ 6.7436e-01, -2.6522e-01, -2.6070e-01,  ..., -9.3306e-02,\n",
       "           -3.1798e-01,  1.5966e+00],\n",
       "          [ 3.6894e-02, -5.9242e-02, -8.0178e-01,  ..., -4.9496e-01,\n",
       "           -8.4876e-01,  7.2180e-01],\n",
       "          [ 6.2379e-03,  3.6635e-01, -6.5628e-01,  ..., -3.4718e-01,\n",
       "           -9.9175e-01,  8.8872e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.3830, -0.5434,  0.4378,  ..., -0.7527, -0.9462,  0.6379],\n",
       "          [-0.0467,  0.1394,  0.3414,  ..., -0.1129, -0.2782,  0.1366],\n",
       "          [-0.5743, -0.2623,  0.8894,  ..., -0.2249,  0.3989, -0.3769],\n",
       "          ...,\n",
       "          [-0.0946, -0.1324,  1.4980,  ..., -0.7582, -0.4502,  0.0736],\n",
       "          [ 0.0694,  0.1347,  0.9086,  ..., -0.8347, -0.8469,  0.0246],\n",
       "          [ 0.0852, -0.1596,  0.1623,  ..., -0.5401, -0.2174,  0.1441]],\n",
       "\n",
       "         [[ 0.1548,  0.7387,  0.1715,  ...,  0.4306,  0.1164, -0.6995],\n",
       "          [-0.9638,  0.8436,  0.1381,  ..., -0.3093, -0.0527,  0.4159],\n",
       "          [-0.2277,  0.9219, -0.4651,  ..., -0.4193,  0.4039,  0.4156],\n",
       "          ...,\n",
       "          [-0.7454,  0.5239, -0.3434,  ..., -0.4159,  0.9659,  0.9831],\n",
       "          [-0.2244,  0.4734,  0.0994,  ..., -0.0134,  0.7518,  0.0252],\n",
       "          [-0.6915,  0.3891,  0.0517,  ..., -0.5549,  0.7467,  1.2364]],\n",
       "\n",
       "         [[-0.0288, -0.1637, -0.0537,  ..., -0.0792, -0.4243, -0.6568],\n",
       "          [ 0.6235,  0.3505, -0.5301,  ..., -0.0756, -1.0150, -0.0698],\n",
       "          [ 0.5932,  0.3808, -0.4572,  ..., -0.0062,  0.1569,  0.7772],\n",
       "          ...,\n",
       "          [ 0.2081, -0.1127, -0.0738,  ..., -0.6880, -0.6118,  0.9168],\n",
       "          [ 0.0642,  0.0305, -0.8516,  ..., -0.6029,  0.0521,  1.0239],\n",
       "          [ 0.8008,  0.0498, -0.8239,  ...,  0.1610, -0.0701,  0.5455]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2389, -0.5515, -0.3555,  ..., -0.7465,  0.6404, -0.6111],\n",
       "          [-0.2836, -0.0715, -0.4054,  ..., -0.8860, -0.5346, -0.4710],\n",
       "          [-0.0668, -0.4311, -0.9390,  ..., -0.5491, -0.2382, -0.0193],\n",
       "          ...,\n",
       "          [-0.2860, -0.4703, -0.1304,  ..., -0.5358, -0.7100,  0.3892],\n",
       "          [-0.7915, -0.5788, -0.7658,  ..., -0.6329, -1.1498,  0.1220],\n",
       "          [-0.4280, -0.7645, -0.9949,  ..., -0.8261, -0.7347,  0.3785]],\n",
       "\n",
       "         [[-0.3249, -0.6425, -0.0892,  ...,  0.5391, -0.2431, -0.0578],\n",
       "          [-0.1340,  0.0844, -0.0238,  ...,  0.4263, -0.5915,  0.1457],\n",
       "          [-0.2733, -0.0366, -0.6703,  ...,  0.6304, -0.6686,  0.2065],\n",
       "          ...,\n",
       "          [ 0.1779,  0.2776,  0.7196,  ...,  0.5712, -0.3346,  0.1083],\n",
       "          [ 0.0711,  0.4850, -0.3878,  ...,  0.1601,  0.1068,  0.3595],\n",
       "          [-0.4666,  0.4071, -0.6235,  ...,  0.0785, -0.0595, -0.2133]],\n",
       "\n",
       "         [[ 0.3878, -0.4193,  0.5053,  ..., -0.1203,  0.7266, -0.1118],\n",
       "          [-0.3316, -0.7417, -0.0082,  ...,  0.6657,  0.4606, -0.4007],\n",
       "          [-0.0758,  0.4198,  0.6633,  ..., -0.7797,  0.2579, -0.3551],\n",
       "          ...,\n",
       "          [ 0.0982,  0.0922, -0.1344,  ..., -0.5467,  0.4734,  0.4495],\n",
       "          [-0.0114, -0.1716,  0.3128,  ..., -0.0853,  0.8245,  0.2062],\n",
       "          [ 0.2423, -0.4984, -0.5352,  ...,  0.8585,  0.7108,  0.0921]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-1.8542e-01,  9.5370e-01,  1.1885e+00,  ..., -3.4734e-01,\n",
       "           -9.5995e-01,  8.2684e-01],\n",
       "          [-2.6085e-01, -2.4385e-01,  1.2295e+00,  ..., -7.5641e-03,\n",
       "           -2.0435e-01, -3.1618e-03],\n",
       "          [-6.8160e-01, -3.2433e-01,  5.7917e-01,  ..., -2.4477e-01,\n",
       "           -6.8279e-01,  1.0034e+00],\n",
       "          ...,\n",
       "          [-3.7367e-01,  2.2833e-01,  4.5032e-01,  ...,  1.8522e-02,\n",
       "           -4.8102e-01,  9.4729e-01],\n",
       "          [-2.7448e-01, -1.6545e-01,  2.9436e-01,  ..., -3.3162e-01,\n",
       "           -4.8830e-01,  7.0498e-01],\n",
       "          [-5.5330e-01, -5.2423e-02,  6.5593e-01,  ...,  1.4750e-02,\n",
       "           -2.0756e-01,  7.3529e-01]],\n",
       "\n",
       "         [[ 3.3028e-01,  6.8621e-01, -1.7448e+00,  ..., -3.9986e-01,\n",
       "            2.3486e-01, -5.5402e-01],\n",
       "          [ 1.0867e-01,  1.1461e+00, -1.1663e+00,  ...,  1.1177e-01,\n",
       "            3.3694e-01,  6.8961e-01],\n",
       "          [ 8.1796e-01,  1.1691e+00, -7.1250e-01,  ..., -1.8376e-01,\n",
       "            1.4984e-01,  2.7245e-01],\n",
       "          ...,\n",
       "          [-1.7991e-01,  6.2934e-01, -4.4872e-01,  ..., -9.5664e-01,\n",
       "            2.6127e-01, -2.2718e-01],\n",
       "          [ 1.3808e-01,  6.8295e-01,  2.2493e-01,  ..., -5.5010e-01,\n",
       "            5.6822e-01, -6.0177e-02],\n",
       "          [ 1.2909e-01,  4.4472e-01,  1.5528e-01,  ..., -6.3793e-01,\n",
       "            7.6209e-01,  1.1318e-02]],\n",
       "\n",
       "         [[-1.0666e+00,  7.8197e-01,  2.1637e-01,  ..., -9.9665e-01,\n",
       "           -1.6282e-01,  1.3471e-01],\n",
       "          [ 4.6843e-01, -3.2609e-02,  1.2424e+00,  ..., -6.7444e-01,\n",
       "           -4.5050e-01, -1.0953e-01],\n",
       "          [ 7.9510e-01,  1.4356e-01,  1.2013e+00,  ..., -9.2323e-01,\n",
       "           -2.7945e-01, -2.7149e-01],\n",
       "          ...,\n",
       "          [-3.0790e-01,  3.0453e-01,  1.1050e+00,  ..., -5.5931e-01,\n",
       "           -5.1101e-01,  3.5318e-01],\n",
       "          [ 2.3475e-01,  3.5494e-01,  1.0954e+00,  ..., -1.0465e+00,\n",
       "           -4.3896e-01, -6.3834e-02],\n",
       "          [ 4.6434e-02,  2.8737e-01,  1.2563e+00,  ..., -5.5712e-01,\n",
       "            1.0708e-01,  1.0953e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.6989e-01, -4.3377e-01, -4.5475e-01,  ...,  4.6895e-02,\n",
       "            2.3296e-01,  4.8119e-01],\n",
       "          [-9.1657e-02, -1.2282e-02, -7.0984e-01,  ..., -1.5253e-01,\n",
       "            3.0819e-01, -3.5224e-02],\n",
       "          [ 4.8405e-01, -4.4346e-01, -3.3092e-01,  ..., -6.6773e-01,\n",
       "            3.1758e-02, -8.3562e-02],\n",
       "          ...,\n",
       "          [ 2.4141e-01, -1.6194e-01, -6.7621e-01,  ...,  6.6244e-02,\n",
       "           -4.1302e-01,  4.5721e-02],\n",
       "          [ 6.2957e-01, -1.1671e-01, -6.5417e-01,  ...,  2.2113e-01,\n",
       "           -4.8033e-01, -5.3466e-01],\n",
       "          [-7.6808e-02, -8.6376e-03, -7.9756e-01,  ...,  1.3600e-03,\n",
       "           -8.6477e-01, -3.6558e-01]],\n",
       "\n",
       "         [[-2.9389e-01, -3.2846e-01,  1.4878e-02,  ...,  3.4963e-01,\n",
       "            6.8643e-01,  4.4145e-01],\n",
       "          [-1.8811e-01,  6.8663e-01,  7.1402e-01,  ..., -2.5647e-01,\n",
       "           -5.8499e-02,  6.7349e-01],\n",
       "          [-8.0014e-01, -7.5506e-01,  5.1894e-01,  ..., -1.7356e-01,\n",
       "            2.2029e-01,  2.5209e-01],\n",
       "          ...,\n",
       "          [-2.0913e-01, -4.4940e-01,  1.4443e+00,  ..., -2.2637e-01,\n",
       "            4.1667e-01, -2.7812e-01],\n",
       "          [-9.7328e-01,  1.6966e-02, -4.7853e-02,  ..., -2.0878e-01,\n",
       "            4.3858e-01, -3.0358e-01],\n",
       "          [ 1.2845e-01, -4.0028e-02,  1.1160e+00,  ..., -1.3982e-01,\n",
       "            6.7431e-01, -3.7192e-01]],\n",
       "\n",
       "         [[-1.9613e-01,  6.1449e-01,  3.7267e-01,  ...,  1.7442e-03,\n",
       "           -7.7711e-01, -6.8301e-01],\n",
       "          [-3.2034e-01, -6.7122e-03,  1.4657e+00,  ...,  9.7095e-01,\n",
       "           -9.9542e-01,  6.9329e-01],\n",
       "          [-6.0404e-01, -7.0931e-01,  1.0514e+00,  ...,  9.8267e-02,\n",
       "           -8.2922e-01, -3.7526e-02],\n",
       "          ...,\n",
       "          [-3.6384e-01, -1.2219e-01,  1.3665e+00,  ...,  1.3747e-01,\n",
       "           -1.1470e-01,  4.9192e-01],\n",
       "          [-6.6464e-01, -9.5062e-01,  8.8249e-01,  ...,  5.0530e-01,\n",
       "           -2.0852e-01, -6.2100e-01],\n",
       "          [-4.9337e-01, -8.1272e-01,  9.4447e-01,  ...,  4.6492e-01,\n",
       "           -1.6845e-01, -4.4851e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 4.1115e-01, -2.9559e-01, -1.5786e-01,  ...,  1.4922e-01,\n",
       "            5.4239e-01,  1.7164e-01],\n",
       "          [ 9.5504e-01,  5.1053e-01,  1.5619e-01,  ..., -9.2112e-03,\n",
       "            1.0685e+00, -3.9192e-02],\n",
       "          [ 1.0570e+00,  1.7796e-01,  3.0727e-01,  ..., -8.8023e-02,\n",
       "            9.4252e-01, -2.8458e-01],\n",
       "          ...,\n",
       "          [-2.6600e-02,  1.9744e-01,  3.1974e-01,  ...,  2.6375e-01,\n",
       "            8.9829e-01, -1.6785e-01],\n",
       "          [ 2.9301e-01, -5.9868e-02, -4.4211e-01,  ...,  5.3974e-01,\n",
       "            5.9565e-01, -2.1768e-01],\n",
       "          [ 8.0402e-01,  4.6409e-01, -1.5700e-01,  ...,  3.9482e-01,\n",
       "            1.0708e+00, -1.2332e-02]],\n",
       "\n",
       "         [[ 8.2896e-02,  7.0582e-01, -4.8348e-02,  ...,  2.8128e-01,\n",
       "           -7.7412e-01, -6.4155e-01],\n",
       "          [-4.9661e-01, -9.8768e-02,  3.5612e-01,  ..., -7.7693e-02,\n",
       "           -1.5390e-01, -7.0397e-01],\n",
       "          [-4.0039e-02,  2.2114e-01, -2.3845e-01,  ...,  5.3560e-01,\n",
       "            2.2541e-03, -7.7120e-01],\n",
       "          ...,\n",
       "          [ 6.4636e-01,  3.0064e-01, -5.9914e-01,  ...,  6.0285e-01,\n",
       "           -7.0834e-01, -1.3927e+00],\n",
       "          [ 5.9891e-01,  7.1745e-02, -5.9585e-01,  ...,  2.3493e-01,\n",
       "           -3.9050e-01, -2.8897e-01],\n",
       "          [ 3.3434e-01,  4.3126e-01, -4.6270e-01,  ...,  4.2554e-01,\n",
       "           -1.1398e-01, -5.5780e-01]],\n",
       "\n",
       "         [[ 4.1581e-01,  8.9456e-01,  5.2925e-01,  ..., -5.5777e-01,\n",
       "           -3.7695e-01, -1.6061e+00],\n",
       "          [-2.5093e-01,  8.8587e-01, -5.3257e-01,  ..., -1.4746e-01,\n",
       "           -5.5925e-01,  1.8389e-01],\n",
       "          [-2.1357e-01,  4.3648e-01, -7.2299e-01,  ...,  6.0845e-01,\n",
       "           -3.2165e-01, -6.7852e-01],\n",
       "          ...,\n",
       "          [-8.4572e-01,  3.1832e-01, -7.4110e-02,  ..., -1.0775e+00,\n",
       "           -6.4426e-01, -1.0300e+00],\n",
       "          [-1.2627e-01,  3.5254e-01, -7.8524e-02,  ..., -7.2352e-01,\n",
       "           -6.6590e-01, -1.8033e+00],\n",
       "          [-1.3748e-01,  8.4266e-01,  4.0800e-01,  ..., -1.4315e+00,\n",
       "           -7.6787e-01, -9.3081e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0645e+00, -3.9299e-01, -5.1932e-01,  ..., -5.7843e-01,\n",
       "           -3.4611e-01,  1.6640e-01],\n",
       "          [ 1.9365e+00,  1.2478e-01, -6.2466e-01,  ..., -2.7296e-01,\n",
       "            2.4882e-01, -5.5194e-01],\n",
       "          [ 6.4936e-01, -1.0641e-01, -5.1160e-01,  ...,  9.3434e-01,\n",
       "            2.0319e-01, -5.7400e-01],\n",
       "          ...,\n",
       "          [ 1.4613e-01, -7.3237e-01, -6.9223e-01,  ...,  8.0745e-01,\n",
       "            6.4611e-01, -3.4793e-01],\n",
       "          [ 1.0562e+00, -6.1146e-01, -9.3434e-01,  ...,  2.5871e-01,\n",
       "           -3.1172e-02, -2.9747e-01],\n",
       "          [ 4.7440e-01, -1.0198e+00, -4.6459e-01,  ...,  3.7215e-01,\n",
       "            2.0419e-01, -2.5750e-01]],\n",
       "\n",
       "         [[ 4.5966e-01,  5.3554e-02,  6.3607e-01,  ..., -3.6742e-01,\n",
       "            2.8033e-01,  4.2340e-01],\n",
       "          [-5.9987e-01,  4.0769e-01,  1.0899e-02,  ...,  5.0829e-01,\n",
       "            1.9133e-01,  2.4087e-02],\n",
       "          [-3.6381e-01,  2.3712e-01,  8.8602e-01,  ...,  1.0910e-02,\n",
       "            1.0866e+00,  1.8011e-01],\n",
       "          ...,\n",
       "          [-1.1735e+00, -1.2837e-01,  8.5841e-01,  ...,  4.9611e-01,\n",
       "            4.9158e-01,  6.5008e-02],\n",
       "          [ 1.2558e-01, -7.0077e-01,  7.2449e-01,  ...,  3.8013e-01,\n",
       "            3.6086e-01,  4.9123e-01],\n",
       "          [-5.4083e-01, -1.2788e-01,  2.8867e-01,  ...,  9.3721e-01,\n",
       "            7.1847e-01,  7.2155e-01]],\n",
       "\n",
       "         [[ 2.7782e-01,  6.0219e-01, -4.6577e-03,  ..., -4.7908e-01,\n",
       "           -3.8645e-01, -4.2461e-01],\n",
       "          [ 9.4909e-01,  3.0831e-01, -9.0516e-02,  ...,  3.5962e-01,\n",
       "           -5.6158e-02, -3.1386e-01],\n",
       "          [ 9.2723e-01, -1.5859e-01, -1.0698e+00,  ...,  2.3048e-01,\n",
       "            3.1805e-01, -7.2443e-01],\n",
       "          ...,\n",
       "          [ 1.1926e+00,  1.5996e-01, -5.8365e-01,  ...,  1.2017e+00,\n",
       "            9.8366e-01, -9.8190e-02],\n",
       "          [ 4.4715e-01,  6.0990e-01,  2.5586e-01,  ...,  6.9017e-01,\n",
       "            2.7199e-01,  9.9115e-02],\n",
       "          [ 2.2247e-01,  6.6443e-01, -2.9436e-01,  ...,  9.1434e-01,\n",
       "            4.8776e-01,  1.0807e-03]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5869, -0.2979, -0.7073,  ..., -0.1757,  0.0062,  0.8085],\n",
       "          [-0.0415,  0.2391,  0.1456,  ..., -0.3828,  0.3028,  0.1511],\n",
       "          [-0.2399,  0.6754,  0.7048,  ...,  0.4539, -1.1288,  0.0352],\n",
       "          ...,\n",
       "          [-0.3919,  0.8079,  0.6150,  ...,  0.1818, -0.4229,  0.1984],\n",
       "          [ 0.5422,  0.1348,  1.0959,  ..., -0.2378,  0.1065,  0.3970],\n",
       "          [ 0.8995, -0.0466,  0.6876,  ..., -0.0534, -0.1616,  0.5344]],\n",
       "\n",
       "         [[-0.2577,  0.1588, -0.2564,  ..., -0.2242, -0.4041, -0.4663],\n",
       "          [-0.5364, -0.7789,  0.0529,  ..., -0.5762, -0.1746, -0.5235],\n",
       "          [ 0.0914, -0.6046, -0.3691,  ...,  0.0795, -0.5974, -0.2406],\n",
       "          ...,\n",
       "          [-0.4984,  0.2091,  0.2562,  ..., -0.1222, -0.1912,  0.1286],\n",
       "          [-0.1939, -0.3808,  0.5480,  ...,  0.6422, -0.7113,  0.5568],\n",
       "          [-0.3738, -0.2078,  0.5018,  ..., -0.1078, -0.4556, -0.3099]],\n",
       "\n",
       "         [[ 0.3219,  0.1112,  0.5878,  ...,  0.2485,  0.6814, -0.4650],\n",
       "          [ 0.7237, -0.2474, -0.1988,  ...,  0.0511,  0.1550, -0.7081],\n",
       "          [ 0.5791, -1.0777,  1.1869,  ...,  0.1829,  0.6814, -0.4841],\n",
       "          ...,\n",
       "          [-0.1967, -0.4947,  0.4172,  ...,  0.5440,  0.4648, -1.1878],\n",
       "          [ 0.2347, -0.4332,  0.0523,  ...,  0.5052,  0.0930, -1.2543],\n",
       "          [ 0.2479, -0.3641,  0.2053,  ...,  0.7024, -0.0415, -0.5644]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8020, -0.0620, -0.1076,  ...,  0.3120, -0.1901, -0.4310],\n",
       "          [-1.3788,  0.4966,  0.4849,  ..., -0.7248, -0.5682,  0.5694],\n",
       "          [-1.4134,  0.1859,  0.5030,  ..., -0.9489, -0.3019, -0.4433],\n",
       "          ...,\n",
       "          [-1.7534,  0.0246, -0.2475,  ..., -0.2060, -0.3514, -0.5572],\n",
       "          [-1.3220,  0.1520, -0.2125,  ..., -0.2271, -0.6984, -0.4356],\n",
       "          [-1.4302,  0.4515, -0.3195,  ..., -0.1737, -0.1991, -0.5850]],\n",
       "\n",
       "         [[-0.3678, -0.0765, -0.3519,  ...,  0.4703, -0.5908,  0.1696],\n",
       "          [-0.4376, -0.3531, -0.4719,  ...,  0.3753, -0.6219,  0.0309],\n",
       "          [ 0.2982, -0.0734, -0.5903,  ...,  0.9047, -0.9301, -0.0985],\n",
       "          ...,\n",
       "          [-0.1886,  0.3027, -0.6903,  ...,  0.5156, -0.7156, -0.3335],\n",
       "          [-1.0372,  0.1655, -0.9124,  ...,  0.7826, -0.9783, -0.4720],\n",
       "          [-1.1662, -0.0422, -0.5163,  ..., -0.0740, -0.8873, -0.4128]],\n",
       "\n",
       "         [[ 0.4592,  0.0740, -0.0707,  ...,  0.2285, -0.1890, -0.3735],\n",
       "          [ 0.1101, -0.0452, -0.7477,  ..., -0.4696, -0.1338, -0.6247],\n",
       "          [ 0.0187,  0.2735, -0.1208,  ..., -0.9506,  0.0244, -0.7389],\n",
       "          ...,\n",
       "          [ 0.6598, -0.2226, -0.0441,  ..., -0.4616,  0.7100, -0.7751],\n",
       "          [ 0.3481,  0.0331, -0.1069,  ..., -1.0691,  0.1069, -0.5386],\n",
       "          [ 0.3130, -0.1125, -0.1153,  ..., -0.3652, -0.3130, -0.3593]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 0.4210,  0.2624,  0.3794,  ...,  0.2867,  0.1583,  0.0586],\n",
       "          [-0.2466, -0.3214,  0.4727,  ...,  0.2828,  0.6135,  0.7563],\n",
       "          [-0.2393, -0.4717,  0.2045,  ...,  0.4660,  1.0116,  0.1079],\n",
       "          ...,\n",
       "          [ 0.4736, -0.3702,  0.4495,  ...,  1.1570,  0.0498,  0.3713],\n",
       "          [ 0.3779, -0.3173,  0.7992,  ...,  1.4655,  0.6056,  1.0472],\n",
       "          [ 0.2479,  0.1391,  0.2455,  ...,  1.8622, -0.8091,  1.3003]],\n",
       "\n",
       "         [[ 0.8740, -0.6840, -0.7077,  ...,  0.0101,  0.0529,  0.0192],\n",
       "          [ 0.0181, -1.3201, -0.7511,  ..., -0.3231,  0.2478,  0.1720],\n",
       "          [ 0.0773, -0.9829, -0.1004,  ..., -0.1548,  0.8382,  0.1720],\n",
       "          ...,\n",
       "          [ 0.5767, -0.5116, -0.8520,  ...,  0.3357,  0.7358,  0.2655],\n",
       "          [ 0.5822, -0.2599, -0.4377,  ...,  0.5150,  1.0235, -0.0280],\n",
       "          [ 0.2965, -0.3243, -0.4689,  ...,  0.6161,  0.6723, -0.1641]],\n",
       "\n",
       "         [[-0.0331, -0.8626,  0.2310,  ...,  0.5652, -0.3845,  0.8126],\n",
       "          [ 0.4509, -1.0605,  1.0820,  ...,  0.2763,  0.1053,  0.1833],\n",
       "          [ 0.0534, -0.7241,  1.1084,  ..., -0.0174, -0.1275,  0.1865],\n",
       "          ...,\n",
       "          [ 0.1322, -1.1955,  0.3284,  ...,  0.2506, -0.5586,  0.2386],\n",
       "          [ 0.0692, -0.0029,  0.2744,  ...,  0.1186, -0.6655,  0.7402],\n",
       "          [ 0.1071, -0.1429,  0.1031,  ...,  0.3333, -0.6424,  1.2493]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1233, -0.5706,  0.3247,  ...,  0.4965, -0.5408,  0.2906],\n",
       "          [-0.1843, -0.5522,  0.4246,  ...,  0.4458, -1.0116,  1.1918],\n",
       "          [-0.4731, -0.1072, -0.0905,  ...,  0.6557, -0.6229,  0.5535],\n",
       "          ...,\n",
       "          [-0.3292, -0.0383,  0.5243,  ...,  0.3222, -0.9164,  0.0265],\n",
       "          [-0.2585, -0.3829,  0.2075,  ...,  0.3776, -0.8855,  0.4921],\n",
       "          [ 0.2265, -0.0424,  0.6064,  ...,  0.0969, -1.1870,  0.8055]],\n",
       "\n",
       "         [[ 0.5423,  0.2640,  0.3556,  ...,  0.6011,  0.5358, -0.1502],\n",
       "          [ 0.2084,  0.2237,  0.2548,  ...,  0.1404, -0.2013,  0.3209],\n",
       "          [-0.7290, -0.2339,  1.0870,  ...,  0.4089, -0.0650, -0.0216],\n",
       "          ...,\n",
       "          [ 0.0582, -0.1967,  0.5271,  ...,  0.5200,  0.0681, -0.4088],\n",
       "          [-0.3768, -0.3479,  0.3191,  ..., -0.0773, -0.4687, -0.2595],\n",
       "          [ 0.1329, -0.3793, -0.3731,  ...,  0.4975, -0.1518,  0.1317]],\n",
       "\n",
       "         [[ 0.3015,  0.5201,  0.3440,  ..., -0.8288, -0.3631,  0.4704],\n",
       "          [ 0.1036,  0.2624, -0.0079,  ..., -0.6830, -0.1734, -0.2837],\n",
       "          [ 0.2741, -0.0326,  0.2348,  ..., -0.7275, -0.2874,  0.7310],\n",
       "          ...,\n",
       "          [ 0.2152,  0.3692,  0.9478,  ..., -0.8692, -0.1597,  0.2628],\n",
       "          [ 0.2413,  0.3715,  0.5457,  ..., -0.4218, -0.0155, -0.3480],\n",
       "          [ 0.0781, -0.2116, -0.1074,  ..., -0.4707, -0.6335, -0.1509]]]],\n",
       "       grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ln_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 8, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x331b93190>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our king, being ready to leap out of\\nhimself for joy of his found daughter, as if that\\nOur king, being ready to leap out of\\njoy were now become a loss, cries 'O, thy mother,\\nthy mother!' then asks Bohemia forgiveness; then\\nembraces his son-in-law; then again worries he his\\ndaughter with clipping her; now he thanks the old\\nshepherd, which stands by like a weather-bitten\\nconduit of many kings' reigns. I never heard of such\\nanother encounter, which lames report to follow it\\nand undoes description to do it.\\n\\nSecond Gentleman:\\nWhat, pray you, became of Antigonus, that carried\\nhence the child?\\n\\nThird Gentleman:\\nLike an old tale still, which will have matter to\\nrehearse, though credit be asleep and not an ear\\nopen. He was torn to pieces with a bear: this\\navouches the shepherd's son; who has not only his\\ninnocence, which seems much, to justify him, but a\\nhandkerchief and rings of his that Paulina knows.\\n\\nFirst Gentleman:\\nWhat became of his bark and his followers?\\n\\nThird Gentleman:\\nWrecked the same instant of their master's death and\\nin the view of the shepherd: so that all the\\ninstruments which aided to expose the child were\\neven then lost when it was found. But O, the noble\\ncombat that 'twixt joy and sorrow was fought in\\nPaulina! She had one eye declined for the loss of\\nher husband, another elevated that the oracle was\\nfulfilled: she lifted the princess from the earth,\\nand so locks her in embracing, as if she would pin\\nher to her heart that she might no more be in danger\\nof losing.\\n\\nFirst Gentleman:\\nThe dignity of this act was worth the audience of\\nkings and princes; for by such was it acted.\\n\\nThird Gentleman:\\nOne of the prettiest touches of all and that which\\nangled for mine eyes, caught the water though not\\nthe fish, was when, at the relation of the queen's\\ndeath, with the manner how she came to't bravely\\nconfessed and lamented by the king, how\\nattentiveness wounded his daughter; till, from one\\nsign of dolour to another, she did, with an 'Alas,'\\nI would fain say, bleed tears, for I am sure my\\nheart wept blood. Who was most marble there changed\\ncolour; some swooned, all sorrowed: if all the world\\ncould have seen 't, the woe had been universal.\\n\\nFirst Gentleman:\\nAre they returned to the court?\\n\\nThird Gentleman:\\nNo: the princess hearing of her mother's statue,\\nwhich is in the keeping of Paulina,--a piece many\\nyears in doing and now newly performed by that rare\\nItalian master, Julio Romano, who, had he himself\\neternity and could put breath into his work, would\\nbeguile Nature of her custom, so perfectly he is her\\nape: he so near to Hermione hath done Hermione that\\nthey say one would speak to her and stand in hope of\\nanswer: thither with all greediness of affection\\nare they gone, and there they intend to sup.\\n\\nSecond Gentleman:\\nI thought she had some great matter there in hand;\\nfor she hath privately twice or thrice a day, ever\\nsince the death of Hermione, visited that removed\\nhouse. Shall we thither and with our company piece\\nthe rejoicing?\\n\\nFirst Gentleman:\\nWho would be thence that has the benefit of access?\\nevery wink of an eye some new grace will be born:\\nour absence makes us unthrifty to our knowledge.\\nLet's along.\\n\\nAUTOLYCUS:\\nNow, had I not the dash of my former life in me,\\nwould preferment drop on my head. I brought the old\\nman and his son aboard the prince: told him I heard\\nthem talk of a fardel and I know not what: but he\\nat that time, overfond of the shepherd's daughter,\\nso he then took her to be, who began to be much\\nsea-sick, and himself little better, extremity of\\nweather continuing, this mystery remained\\nundiscovered.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.datasets import get_top_n_tiny_shakespeare\n",
    "\n",
    "longest_shakespeare = get_top_n_tiny_shakespeare(1, mode=\"longest\")[0]['Text']\n",
    "longest_shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x33aa5ffd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAAGiCAYAAAB51+FxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQ8klEQVR4nO29e9BvV30X/Pmutfbev9tzOeckJ8lpE0j71oEilTaUENCO02aKlTqtosJMqox2QNtEpTjVxreAlpYU7IUXRLAdW+oIxnbGemGmKBMUR7mVaJ0KNIWW16SFk5zknOd5fte991rf7/vHZ+39nCMX8/x6esLv/WXNHEKe29nPN2uv9b18LmJmhqfXVVvuqX6A/7+tpwN6ldfTAb3K6+mAXuX1dECv8no6oFd5PR3Qq7yeDuhVXk8H9CqvpwN6lddTGtB3vOMdeOYzn4nBYIDbb78dH//4x5/Kx7k6y56idf/991tZlvYLv/AL9slPftJe9apX2f7+vj366KNP1SNdlfWUBfQFL3iB3X333f2/p5Ts3Llzdt999z1Vj3RVVngq3oqmafDggw/i3nvv7T/mnMOdd96Jj3zkI1/09XVdo67r/t9VFRcvXsSZM2cgIn8oz2hmmE6nOHfuHJx78ifjUxLQxx9/HCkl3HDDDVd8/IYbbsBv/dZvfdHX33ffffgH/+AfXKvHu2I98sgj+Nqv/don/fVPSUBPuu6991689rWv7f/98PAQt9xyC55x7+tQxCGa04qw5E5tTidMfiegPm3wC0F1ACyvB+KuQhLgFwJJguZMgpUKt/DQvQiZewwe9WhOK9IkwZ2PePhNb8TOzs6JnvUpCeh1110H7z0effTRKz7+6KOP4sYbb/yir6+qClVVfdHHZTRAOR0gVQodADpU+JGgrSv4FvADQX2rAR4IDhABghNYAbhxgo0ixAWEdgBxgBs4uNIAr3BVw7/jhEfKU5I2lWWJ2267DQ888ED/MVXFAw88gDvuuONJ/5xUKTQAaaywyiBJIM5gAHwtCAtACyCNFFDA1QLzQH1dgp87iABWGnQ3wq/4OfMGaQW25tH8lL3yr33ta/HKV74Sz3/+8/GCF7wAb33rWzGfz/FX/spfedI/w0pDHBnCoWdQRwlQgQBIXYATUF7y0MqghaGYCYoDBysAqx2g4H+EAEjNXV49FmCz9SL6lAX05S9/OS5cuIDXv/71OH/+PJ73vOfh/e9//xddVF9xlQotAC0MbiVQ8bAhUBwK2l2OytKA/zQHiAIu5l07UcggwVoHiwJXCzRHw4LBr9b7vZ7SS+mee+7BPffcs/b3u5AgxkDp9Q2s8UDtEMfcjb4WuIaB0tJQrgSpBNKOwk8d4tBBVh42jtDK4BoAhUILD7/mK7/RtbwmB9cIJAqsdYAYYNyBMJ6JouBuE0MaAHFscEthsA8DzBvQMgwmgNQergXSYL1n2uiAIjo0+wodMCiuShAVlIcCX+egGCAqCAuH8pKgPZUAAPUNkb+9AW4Y4Row9UqARIHE9R5pswMaFGHOi0caB50XzC1bfloSz1CtDBIBLQE/cwzwoYc5g0SBNh5xmHeoCr9e13ukzQ5o42CeaROCwe+0kMYhDQBJAr8CdD9CEgABfA24KEiVwYIBJnCtAHU+LgTQgfZfv87a7ICaQBSQ2sFKRVp6SJMjIQYtAeQAQoGwAF9lZ0yzSoU5wO20CHMHGIAqAYz1WmsjSs8vt9zKIY4M8AYEhQgvoVQZ0oCvMwqFpACXBHEI7jwVSAvYQKHjBNQecazwToDomJNu5Q4FUEwFbiWACjALgAm0AFwrzE8LhQVjoq+AXwnCQmDBILUDEiNXHDmEhUBK7tB112YHVBggLQBXJUABK4yXVOIFY4nNkLjD1zsODXGscCvH3WpAGEWkEaslgEVAKtd7pI0OaHeLW6HQ2gO++wTQ7imPg2nBc7YVNPuGODb4lYPuRkgtEBPE2sMcz01LDuYA36z3TBsdUPXgLhsoXJnyB4GwEO7eBsAkMlCFwaQrPQ0yC9ChshnijemXAUgCrVjSrrM2OqBWKlwtQCvQJaseSYJ2zNoeBphxJ1up8LWwq1QYGymlAs4gYtDSYN0Od/wPsM7a6IACXSnpeSkZkPZZl6cJz0wsPeAAN/PcnR6snhaMnrQ8S83z+JCCXStdM//Z7IAay0VXS5+Yh0sBfimQ2iHMBRgm5p7C4JsH/NIBzvhHmSL5pSDMBLbyQJK+2jrp2uiAutbBNflMrBmkNFT4JneZSgBJuFNzRuAim8jm8ystQJoFXlwJ8Dtt//G1nulq/GJP5dKSjWYbKBsdteTmMqCVAdHxnwBg7OQD4G9ee0gr8Dst2gl3r0Z+ft1KaaMDagDascEvHPyRh1TsPMUdhZYc0rFSyg2UfMO7mrmpNPlGbx3SiDe7LQP8oef3rrE2uvQE0L+qVqFP4l0E0lABOIg3uBYwZRff5bacVTnNStKXo1qwXE27CZiud8tvdECLuUCGyFUOA1Ac5ebxBdb5tsq3eQR8FKTSmG/m291PPbR18Ms8pKsUaOW4yXLCtdGvvLS5/WbI77/jmRqA5Y0KvxT43YZnYwE2RAJTJAjgJi1r+v0GOjBeXrm29/VX+Iu/wtrogPY7U3hzI7ARIonBtgDoExWgPDstMD2Csl9qByWrpejyuQrIKAKDNbvL2PCAinK3+ZUAhcFVibsxABCWmpLYXRIFNBjMGSshz/mTRDZQ4m6CSzyHYUCcbGGlJBGsjgbcndp6Du2MU05JgI4T0tCgQ05B21MKDQQzcHYkfbtOC/CVbx3WhdFtdEDdigm8XwqQBLL0vHxWbN2JAvCGdleBlM9Px3xTkhDTFMHvU/4cCWxYb+WQLo0NaWhoz0T4YQLGEXFHUZ8y9kkrBqu64GEDhUQgzPzxRdYKRAXYiRyVDAyYh3wUrPdMG502deeim3mkPF+vLuWb3gHNnsJGEa71QN6ZGpg26VABz1JVgiLM8uxp0jIt/WJs2pNaG71D24ny9TVAVh4yjEyJFIi7ic0R5AvGcjuvYmDdykHmAX7pYMp639cZbFb7vgA46drogKIDizjAxGDJIQ4tV0+5ObLyBD4cBPgGfVNEWoFNIuJOgkV26WGAHZYc4m1jYu+XDn7BehwlX124nDZFYfAGCe3EoENFqgCIobzkYIXBD/M2TAQ3wAgUc0u3nUAHVyOjQoQNY2VH3rUAHC8mOSp4DOQ2nj8KaHcUOlCk2kOGCRCDawQusZmiw9ycXueZruYveM2XY0DDwsF2IsQrId/KJolfCqxQuEZgpUILg1aKNFF2obzBGgepPef5pUFWHli/UNrsgLZjQ5jxV5BpgM4LtDssR3WYEMc8L+OQu7i6xHPRLTiPt9rnncuWX1gKLChLz23sh3Imb9xZSYBS4ZvjaWjXTA7LY5woMgwcUYg4KfNuzV9b7NUQrz1Q96RrowPKmpwXCa6v4YrEXFI4fEtlBoAVhrSbmA10DWbLc6jWQQruyFQa2mUBazyrr/UeaXOXBp6JfuHYXHZE4qWhMo3q2nQ5BYpjDuY05LK1YHaAw4KDPgdg5REuhe185XUnctK5Eog3pNojHHm42vH1X3VIPOJHJQGycjwKDNB54AW030JLY0kajNXUmhfTRpee/jDAB4/VuZY1ec0paHuK+WiqeEbGEaecLhIkpiVbe8gQcGscfGSfFMYzditr+R5uczFALKA9lWABCDOH1nF+JEtPMoJj4g5hx8m1gqRMq6C5H9oyl5Uo25nYA0Ac8fVsr4uQYYQGII4Uria62QqFS4JwwL1T7NcIc8c+qDf2P30GPESBm7Rs6aX1nmejA6qBO02r3MP0zEFdI9ARIyKDBBMg7iRoaYgNkXbSzaE8QRJu5RAnBO2S+7TeM210QF1kHqoBvGQWAWHOqSeUF1I5aHugQ3lE+k0a2DGoVjP4No9D0iLArwRpvIV5qLScqds4MoA+o5QXvK3NA82FEfxSeAQI2MZzhIxD80XkrA+6G0SksSJMtzAPTcNcuy/YQHYleUppxDOR4w7LM6XjQ9EKNk5QJSJIWscOUwJ0VsDP3ZYSv5ARdZOEaqeGLgMvqcSzU5TVUAerSRWOUXcCIDoi8XKs446iOLVCmuh2zuVdK9ChQpYe9cVhjxGVJLAlZ0MoOat3c8/KyeX/L8SMdmSvdKpFmDqklkiTdmcLz9A06V7ZDh/KnddODMPrFsw/V56dewcU847KnedKk0Sw2G6EzAPS0GAZdue2sWNfXAqkdZfKyybTZ1wE2t/Z4TlYZdi4sgslLgsMgChmGycy8krSHKGSx9PrPdNGBzRmCKIYW3cQMEkHwQ/FXDJ9Me9KBcwIhEhDY3YQeTkB+TyuCW1cd2126Qm+5eoNvkpIU3aNJAI2iagReJ4GAA5od9mhL6YO5gx1xdvdGib2LgJ2SuEulNBivWJ+o3cofM5FAaQ6z967o08z/HuYEDOvk1h8oNlVErsyvskKgw4y77NM0IGujbHf6B1qpULN4MYtzARW89cRzdCcJBCvCNMCMV/abspeZ9xPQCuwipdZmHr4WlBfrCCtIA238JaXhkQFu8RZuqsdLFDYBYUiDQhV7BrNWhi0JErErRy/fpB6TKgGgw0SNUe2sWPfVUIwgalAh4k0bQCSSWCypOQFjAQHaamIAxCZh8TgdvMnN0gkQGxjx75bVjC5R24ep3HiWKMR2DjyYioM5aHAdlu0+6lHkISDACsMcWjwjZDWbVuKYPYLh+LAI0w9TIzKDCoon/CwYeI8qVC25RoKDLigCIceYeoIFjvbEBJZU/xKkwAm28lG1kFG1FUGGUeWmQXVb6RmALX2KI74a7rIowDScZgEtvScig4N7Y7BB4Vbuu3shwK8aPwyU2OM83dkcZaU1XK617fZM6QVMwHzgFTUKoED0kBRzARxWvQ4/XXWRgfUMjjMBCQeTC+D0Qhp3n63IUtumAgXd8bu/Sjltj2Oz9luJJKpjOusjQ6otO5YwabLKTMsUTOTI61CnzalgcFyRwoxE2VbAZbE5rvcsEaU7cSHSkeMNZJerWAQdWA9+NaVJC0gExm6HeyXDtI4uMYxlfLs4rtJ21dQ66yNDmjPKHbciYjkGtkgQS7xmrZImrc0Di4KEB3EhBfaMEFLZUYAtux8UHajthHO6Jd8fC0NaBwvGAHcUYAF1vW8xVnzhzk7S24p8FPH7ykV0nWqBGhXISvnrPdMGx3QVBqKI8d+aKkodmsCwrJymF8h3/iAnWr5GjeUagNy288BmAYGX/j5uKtYnd3CWt5KRXNdghgHdfHCEKkyhCPW+HHHgCL3QVckInRzeqq2AqgJenC5ayXDlCeh6z3TRgdUGodw6LJeUz4L3WViAyvhbd7d/FkGU0dEM1uZ0yaX//gujUrw8y1sjgi6qWfE8MwSxaSBRGG3qdT+opYMAkuVwZes412b5/iRhYBvmDXIxRIy89uJbZI2V0a1x/JgAE2cWBKuSPJCV8NDyA6JuVLSwiDLrIgTmDL5GtARO1a95NAJ10YHVEtFOt3yBh9FpHkWY4lgoh8A7LZkxXlDMRWM9pY9wKGriGTMLD5VgBtFpKFtKSTcOsJsnmK2jtIYwsvFPCglbGyWiALLeQU0jjBIBdOngwJhnqujHMdwLc7Q++67D9/6rd+KnZ0dnD17Ft/7vd+Lhx566IqvWa1WuPvuu3HmzBlMJhO87GUv+yIDgIcffhgvfelLMRqNcPbsWfzwD/8wYjx5redaIRw8sOlhVWJuaoB7rESYSS8vJFF43s6p4BhHeTo6JqQ8DnNHf1Yw0NeiH/qhD30Id999Nz760Y/iAx/4ANq2xXd+53diPp/3X/NDP/RD+Hf/7t/hV37lV/ChD30In//85/Hn/tyf6z+fUsJLX/pSNE2DD3/4w/ilX/olvPvd78brX//6Ez+8VuS+93TDBXerGJBORTSneLO0O1mCKDLAxYFn4j/M7b6RwsW8KwsCH5pT673yYra+N/KFCxdw9uxZfOhDH8K3fdu34fDwENdffz3e+9734s//+T8PAPit3/otPPvZz8ZHPvIRvPCFL8Sv/dqv4bu/+7vx+c9/vtesf9e73oW/+3f/Li5cuICy/D93do+OjrC3t4dbfvLHIZOKEpejCFsEuAVLzLgf4ace6VSEvxSgZ1qE8yXa61pUv18yyI5jFEmC4sjBr4DFLRHSCoqHW3z2H/49HB4eYnd390nH5A90hh4eHgIATp8+DQB48MEH0bYt7rzzzv5rnvWsZ+GWW27pbX0+8pGP4LnPfe4VBgAveclLcHR0hE9+8pNf8u+p6xpHR0dX/Ol/gZVjk6RlB14MVPw+CMwCigTX8vPFoUAKcpAkg02sMOhOhDljnhoM8GywrLPWDqiq4jWveQ1e/OIX44/+0T8KADh//jzKssT+/v4VX3vDDTfg/Pnz/dd8Kduf7nNfat13333Y29vr/9x8880ACJDVgdLXo7gscTROMMOSzQ5Xc6ysJYWuiKcXKjQWCql9PxVFJJtOr/UI5O6778b//J//E/fff/+6P+JJr3vvvReHh4f9n0ceeQQAkHZi1g7JX5i6jj0nmh30BqDcOgyQRuBn1FwGADcL8PPciQJ6KM81rZTuuecevO9978N//I//8QrzphtvvBFN0+Dg4OCKr7/c1ufGG2/8krY/3ee+1KqqCru7u1f8AYDigM3j9lRCKLjbgFxielIW40GJdk+zEw2FrrTkaw1F35Vqdvm9UmrPaVpnnSigZoZ77rkHv/qrv4oPfvCDuPXWW6/4/G233YaiKK6w9XnooYfw8MMP97Y+d9xxB37zN38Tjz32WP81H/jAB7C7u4tv/MZvPPEvEOYsHbudCCHtW5Yeccfgdsnq0GHWp89gMWml1w7V0jItXMhZmjuEa2Gucvfdd+O9730v/s2/+TfY2dnpz7y9vT0Mh0Ps7e3h+7//+/Ha174Wp0+fxu7uLv7G3/gbuOOOO/DCF74QAPCd3/md+MZv/Eb8pb/0l/CWt7wF58+fx4/+6I/i7rvv/pImVF9pxbFCdhJ3Y/bzUA845U50Rx6aBD4KNB3z6M1lcMQoQebU2NAySw0rYThxeKJH6deJdug73/lOHB4e4k/+yT+Jm266qf/zL//lv+y/5md/9mfx3d/93XjZy16Gb/u2b8ONN96If/Wv/lX/ee893ve+98F7jzvuuAPf933fh7/8l/8yfuzHfuzkTy+AnzvAAePdFaRQ5qADopSBDLDNnSTXgmSGIv//rI5jWXDAfIaSG9au5f9AeehTtbo89Oa3vBG+GkJagd5Qw2oSXyUC7emEwecDVrfWKM6XaE8lVI8G4NlTNOdHGeaYu01gwKuLHquzEX7uUf5ejYfeeo3z0Kd8CW9zCxSmhmTWx8jgZ/RVEslqN5HeIPWlAfmelQI7ETomfDEsqObo9xuYN3Lu11gbDWf0CwcbONjphmP0RDgNYeI5wLVDzFr35j3c0vc3eBomyIojEWmBUAuWByXEA24bgQ49F35WEOo9yNPLCMBlJUZjcP0k0gpoNxLybYA7KOCXDm6VnRRbfh/c+jt0owMq2o0vDOWgpaBVsF5Z8XIKoimIJhlEuJrYJR3T9yMD7kil6fBRoy3sh2ppGYEMxDZQs15Ay8nsESKjSBx+Hg+n6I7hOiqI+4SMC/gfqBg1BJpt4wjEL4STS2fcgSWjYBXRdHCGUEVimmr+qpaEvKSEHHgHvyAYwq8Eqh1Veb210QFtTylLSG8I2QtEWoFbZN7R1CGuij6nTCXtgNA4GqtG6bXt2zHzVW38FYo6J10bHVBpOSaWhUczLfm650vF1Y4J+sqzCd3V+cvO44f/UM+LzQLxoda4DBxb75k2OqDmcqAMmJxZoBg3LCsB2H5LIVaXVcG7Wr8VuF2KWduAPND6TIJf0WnRDekl0m6lZHCmZts4wYznn1/ksfE8IEwd/IRbTUaRlha7LfSo4Nd00EbPiWe7S4VHKGDbCAnvCa5RMCxb6KWqt+2RmmLW6ahEMXWQJ0rEoUGnBVzjMjjMoPuRcsJF5oDmUnRdp4WNDmjcS0D2lnv8wg6QLxlzgO5G1Kc1I5gVOkooj8gJtVMNd/fKU7zAKDwYFoQ7+pXbztIzHHlgjxdKqBLiRBBTgG+ITg5LQbMKcAFUXTTAVQk6K3rDP4BQcC2PiwBXA9jGS0kMTMBzEu7KLBNs6Nlwvko8BsSQqqwmJny9425CPE2kc3HoUB0Q76QVthN9117fZpUG8ou0odhAB6Np9gxpFuCW3MXE33M231VY0mQ1W8kSGknoZLuNlRIyV9MqhQ956NbiWE+5MMggsUkSjDTwnRZIgupRT2p3/r76tFI2cxhhYtd+jPzVsIgPDZCFR0q0+mn3FBAm9mEuCFXMGncOLgn0UsnXfWywMw1HIS67gaVOHsMRxrPG2uyA5vPTCsVg2MBS9qczCl3HscE5spPDPOeXgwR4duhtFuBWrofyaDDs7C0JfNjG9p1f8WIREyynA/jhlYGQCLR1gGukF2qFSQ+4lazW6NpjA5ZVXVBQcBsbzGnEBkdx3RJwROCFBYPh57kT7wir0f2W9mlzD7dwWYHMEQABnrupAppLA8jSISzXe6aNDigUgCcVBgD8gDs0LCgw6FrADAgzgX+8ZNJ/uoGOiG9KO1QM1wHzUBcBFIowc9vJRu6w87YMFAk8KgFjQDstkr3dBcUGHSHfOCyAUq9g1UkeJ7tWaP3jtlSdURJBYH7qIYVSeySr3wAMUJvY30xjShK5MzUQBWHq2DxZUCHHr6RvSKWdLfVGBjiI05GiqCJ0Fahs0wC6k/oE368E5eO8rULI1JtMpaEtJUkKrgWq00uYrK/BvNEBtWC8uKuEqmr7klOiAG030hDEicICL536YJAVG4zNFRAJTRlhoFnw8NxKb2SCGRRuGjA7GEEqTjF7IUAFVssSJuQomct8+Myzl2x97lpKZNZnjLls47ZzLm8lXWd71FzGNrmW1U67p9jbXfTywMVMUJ5eQYQ5qpXav/rdOBqO2Ce/jWmT+Wzmp4AfR4hY3w/tOvKzRdULC4gBzaKA5XGzm/l+GhoWme8ZXW6mrPdMGx1QDeRrUqkBUBO0O4o04qzIN0CsA5UWDVicU1pVNEyN7LqG003hsK6r3/1qS2XXQ1b/crVgvLMivskExZTigH4pcMEIGtMcsEDhgfLQwZaeZIVhhjAWAIKiOZ22U4CA1rt87RfzAeK0QPUEXcA6b0+AN3bazSIuQSErx9o+6+MBbJNqMPhS4Rq3nXboGsgEoeqiUZB1RJ483bvBVl4L6oQ69DLrqbJeicwfBfhacndKM3pvvWfa7ICeraGThLSj2J0sGSzHI8AGbN+lBW+X8pLPckQ5sGdrjpE7rlJFdYh2VrK5vJUiLrNAfGctuHRhB/AG12R2XJZtcxU79iZAWABuQCay+z3qqrtGKCccsnG1sVfqt1EyGMjJ+VCpYT/zGask9InPq7ku9bQanQcgAWmi/N5Az+TqiRyKlFOsbUzsw9xnqx72PeFYJbWTLC4AUDZIBXEnIQ4NxV4N1/LcNW89Z77Zy3r33iCNbGfHHkYcKISA2k64Gq7znaclWnGRQi3FXBBXBdKYu7MjfnXcT2JLs3X6NgId0kjhSoOVdJmh4k12o0l89ZOiV7xp9gyYh0ybybsz5uaIF8BlAq5uaWJPFUWStUKRmKh7BlAygQEC1Gc5mOt8Pq2gehicUcc+B7czVEnZP3mdtdEBheUbvaVkcEfv7tAjfiXYPTPvb++wZP0OZPq2M8jSIxwEFAdZSjj3QrdS4VYSXWatMLRTClunism67rcQA46eGHNUXCiaXYPuJOrg5bO3S5k6j1BkJMlWDum6fqbUjr3QQlF0PkgNjahcleBqgVt4FHNBMWl6L5BOts01FG81D8goASpYXr+FyBFXC9ySJANfsJPU7maB/znRdpKr0u5iio3vGycdWaE7L10tMOMl1fPnT/pMV+uXe0pWLhslctTRW/94QPeov6S5+dF18iV7KbWZH6+B4q1xyI6+CzQI7BorJ10bHdA0UNgkQncoQLBz05Tay0OFrDzPyewPksbKWj458kM9js3+gMxixhVGgeusjQ5oZ1HhBhFV1WIxH8DXoKQ6yNeUXRIZrNK+fyq5vpcoTOCVqZdrgbQMmfK43jNtdEB1lCC1hw+K1bJEOiygRdZazoxiuQy0kEr61uGw6JEi/Q5NgG8IdJC0rUCHBW+T9IUhnGdjWAvr6dzmgTQP1GReOQasE2EFPei6r+93ZZ6crrs2O6AGNo4FiNHDTvM9FWVpWcyY7JtksRYP6EFJQG3FkrXD15sDmyfDluftNo5AKLzC1zXNC4jLuzORlxSHBj+gXn1nM9lp37sV+6XFEZlz5REnnSn6noW3ztro5kh7KsINCrhxi+ANzitSyK7d06IHhPmVQxooJAJuFKHRQQ48/MxR2nKokOiZkzpDHK7ZDMWGBzQcBmhFC18tFWgLlDUACJq9iHBUoG08vLIzrxWgc7LotCKgAbl81YL/IWLts779ekHd6Fc+7rIul0pRjRsgw7+b0wyGhZxXOkBz4g5nwIDlqJ+5XjO0v+0XAT4DctdZGx3Q3tJ8FhDbACmoIFZe9CR6AXAFBbBcB1d0BrQMpGZ/ZLciSUEM9LDbo3XQOmujX3kotex0kqCRbrMhd5H83ME1QNM4oCRNxvIYuVvmDAis4eMoG04nplxbaQwAl5seLgsLBFK9U0ncZxqwNmdOynJSSgId4oT/Xh66zrCWXM88pKv3136kzV1uxRZd8XiBNGVpk4bUZOqoMc7nnZexo5bp3GHOy6g+naAh0+kNQJUIuN3GqWc63QLB+rq7eJxQHFc7oCHM0XnSDMMhR8zi6IyYcpDD4vIOCYCsJbqdI5Ak8HOH5oyi2KsRJ0SLdChmc4ZmXsI8fTwt85Qk79puNOLrrBCu4CUn2M5KCYkjD2kFVRVh48gUaUBUsmtplFoeSu/3YSsPi8e1vhaGdv+41JQqobzg166UNjqg4dBT9hfA7NIIMO40v3BIOwlaGKpRS5Yx8ptdaH/TF4fZz7ObyXeyJPu6Nvpuo9OmNFG4QmGSveiyMgMA4JC/WrMKEG80oc4XTWdnkXK1FOaOEHEHiLAzVR6u90wbvUMlCnBYwKpsMqV8hc3xRgdYKQGgitgon48d8vky/2MtWFlp6zlFnaz3TBsdUAvsxLuVgzhy4+MO6TJ2pkExFzgxWMjsY0fVB4mSBQQ1K+sQnKvBIJ4Um610TTRhEHWc4EMCZkWPk7dFQDvJ0m0C6Km2H7xJQ/ZxmPGo6Em4UwdbBPjVlpJnRQU4CpAqoZ1WtEcPQPnEcRLZzgs2mldZgi0k2OmWxC6lso6W5IH2vPniGmkwf7Utq5T6yUuPam/FRnJkUu6yV5Ib0ATAZZ1QAIAY4kTJWF6R4HB53umaba3lMxtZhgltEziwW5J2qAPu1qKMvKhOt3At0C6okeeXjjJDdjwuafcUKBRxJ2H1VCBHfvInfxIigte85jX9x66l/Y+02eR0FuC9wqqEOGZw3NIhLATNsuDrfRRQzAXD3RW7+ZWR9KBk5GnIiuNKauI18VO6fP36r/86/sk/+Sf4pm/6pis+fi3tf/yCeiHSOsTGww8TtACa0wodZXeFmm6JECCODKtFCWm4M3WQzayioNlX8jsDbSuvKfpuNpvhrrvuws///M/j1KlT/ccPDw/xT//pP8XP/MzP4Nu//dtx22234Rd/8Rfx4Q9/GB/96EcBAP/hP/wHfOpTn8I//+f/HM973vPwXd/1XXjjG9+Id7zjHWiak6ELaDnpKTWUHNLS90FCoYgTSrmZgMl7Elh0BJjlaam5LOVkGbWsQNpL17Y5cvfdd+OlL33pFTY/wLW3/xHLDjOlErMUM/T7kuut1UDVIcRR1nUqUq+EG45YepYHme+5ErJEGnds2HLCdeKA3n///fhv/+2/4b777vuiz11r+5/OoVsWgfpMheaZPOWE/Mph97o51HdzecNownfZ18w/TXDsUa+Azgoq5F4L8uwjjzyCv/W3/hbe8573YDAYrPc3rrG+nP1P7zbjDbH1cEfhWKYysl4/enxMAO3Uw9WC+XQANFQJl0jYTk9acGCD2du1OUMffPBBPPbYY/iWb/kWhBAQQsCHPvQhvO1tb0MIATfccMM1tf8RE2An9qMPrajc4GuORQDmoVoa4i5t0UORyJEPhjTJeHtDH2AoTa/q/WuQNn3Hd3wHfvM3fxO/8Ru/0f95/vOfj7vuuqv//9fU/idlqmHr4AqF222RBob6+gQ3IxvEe972bIoA7WHFYCvFX4ojT4y+ZCUIYTpWXVovbTrRSbGzs9PbpXVrPB7jzJkz/cevpf1PxxoWFXpHLwOKWpCM6ovhkqA5KhFagZXZ1mLZ4UMz0KHOOH2XtUoMfZq1zrrq/dCf/dmfhXMOL3vZy1DXNV7ykpfgH//jf9x/vrP/+YEf+AHccccdGI/HeOUrX7mW/U+aJLimgA0SER9VAowGAO1eRLtL7pK7EKAhE7p2eX13SGfJPktxBBQrQWz40raj9X7/zbb/+YdvhBsNWILuRlgUDH6PFpNpn/aTdmMN+QIv0OqiYPFMSte6pScvydH3k3kq0NwQUTweEL5Q46H/Z9vsf4Asu85ZkZSKZk/hGkHxWMFXOQmKabZAL4HhGfblJDFtShXn8xryYC7riaaTnT792uyAZqEWMVBIdeURZuzMxwkhOFb7fi4PAerlMTRZYj4/PQPcOzIMt5XevUvnGYlgpVQc+3Falev0kjvWXCf2QjxUGtO+VzQbABqT/WrMSPbaTydcmx3Qw4IiVxNloJYe5SEHdW7hoR6Y7C8Qh+SEulqAWZGlhOgIBlBQy9ccM9fTChYM1cUtZIFYpXBtNkEVQMYRzR7l27RkGTp9bEKCQk0JTLKQM8RxEqFZ+ynldh4AdvLXLAQ3OqDhiOzjtJs4pHMsITUYUOTgCVOjlJsjUjtOSMUQLhYEkdX8D5KGmSUCbCc+NO7SoK+bs3dghTRS+FFkGTqKVMXJTRDstbDGoZiSMWJVBuEa2BzJTgvrQnE2GugAAKiUag6RADEPwM890jDBdUpDibtZUtYcqQgWkwzJAdAzRQAy9NZsh272Dg1TD5l59j4dO/dpYPArwJdK56/GcVa/T9yTDRNkSSReMSeapJhLtlEnkAyCta18NzqgqVIUU4cwdygGETZMfG0rQDOYgVAdfn0xO8YxxbGhnWQvupT17gw0C8xd/nXWRgcU2a2r3U+IracU8ELgasAaduy19mzRrTziyDDYq4Edirdqweqo2c9ngwH1xSGKQ4/m9DbmoYk0bUmC4aihVNvAyCrOCox+kDh3r5hGrWYlrM3zpUL7UXR9JhH7ZECcKM0D11ibHdDdmLtGhuW8zKw5cuitVKRJQqo92XW1Y55Ze6B11Bu5rONUPe5RPeEgo4gw4wh6nbXRAbXWIY2ocCMOSEes010tvXxwOW54yy/YM4WjjJAJx9CSqIirGY0nLnNDtxESLgtP6fS5Qyg4pKNmE5siYpJNqPn1Ggguo9RQptkI0GTigijhjJ1q7jprowPa8WHS0NDWAbL0KI5cv7ssUCCrk1f3Ndt8yHpPJNMa/NIhLKn1FAYxg3HXe6SNDqhV5LvbONIXecB/jztMn/zUo56X1BW5rs0Crt12zYm8dk7fmes5LzjE28q0yfKlMg+ZucVXvjjykIr2FOLp7Rm+wL6e320hPmvf2XFC3+xSQsOPIsVcrhXQ4atpFQe+x3TaysNXHBmn0tjBj4JqyJyzn70DOW3KF9FA4VeCsOwg41n7bhvTJvPHBNpyr+aOU048AVY/9aroMUySqOIIR+JscSTwSzrTdDp56aCkRt42yq7/7/7FaREyC8761p62jsDamG2BFgGIBDOkjjALjo67RrSWW0pNbHcUOlRYpfRUyq90+YRHPBV7c5XyIoOaBgAKg1sQW2/CXSvGne5roXRmlP57T7o2OqASCayVKmGytwSKXDqOLKuCG6pxw1faG7QwyJzi1p2So8QsG9wN5XKQrzng9qthUalBYbOAui4oitVKL3cZ5g5tHfhaV4SId6apXVOa4qz0lwfQy7NvpW4THM/KwWMBzayELDy1mEoAJfXsqmFLfbyQp6DDiOLQZQ0SVkthKbAOLDZMaHe31GmhE2Rt9sjf9NevjmHiJefyMToUlxxs5TkaKRTtniLMKY0B61B3BD6EKiFM1+3Xb3pAgV76otipEefFsZDAQUELtcOK6dWSdX9qPKzS/lxNQ3p/iALFkSBOi2Nx7DXWRgfUL6lca5OIqoqUykhE41mZU6Ksr2xVBjysPFAY4nUtdC9SX2Tm4Voh2etyAu4aa6MDqpX1fkjLRQW0joKBLQgPb4+JCV3tLo1Alh5uRgtgXxNV4hrwpi+UUJyt1GDueO5JMB6vqITT5h2WYYnIibpbOao1nm4ApexQp1kvKmj2CIbokM9b6eup2eJcVg7zRYW0S9GBOKHKg2tBAG0jsOsapkxg6kR+JxXFuoZyGhqKQczyGes900YHFMKxMCSXnVkWAwrYOGXcfBYXWAQyPpaMVMcg0WA9WEwUaA8qfr/bwkoJALBysFIx3F9BRtyhaWiQlUfajbCgaPdZXxZT1/dD3cqRGAYgzHL6lC+0MHPbKWptpQKlwk89YnSwxvWvNRKVwqVS+KUDCgLCJLt8iXVwcEFxRK2SdmLwp2pedtvYD4UJ/EGAFkAIZBJ3O40mf5RRhwpkepkHSJTsl8zSc3Ud5YnCUhCXgQYDwy185YtDVj9whqYJHBE77jzkOZIIESMusiJykxY2iVS3HfKIoFywIJWAqwhA20qV8PZUhJ9TdCWE1O9QC9Y7cgMgSiRkkZZZkd0XHNxB6D3qXEMEs8tGqVupYw/Nbl8rhyZj58uLvh+6AZS71JKDPC1YOemsYMNEAb8CpBWkCtASNBIwbOcr71ZkH+uYqZMrSehyK/5aogLnc/qUR8wQgwwjZTIH1p+jZNcBlhz8zB9bqp30ma7i73fNl0QiQGTlEMoIOyyzMRUrobiT6DzbEiUCALLysNZB94myi1kRp6uabOV7nOg6a6MBt1oqnNBwOjaBXfnKcrKeITeRmCatDBpzIaACmXniQbs5vRiIisiskG1Mm7rZetxLmOwuWTmBl49VCW4lGJ9aHlubGyBFtgtK0ltQdiNlDQAGicfBNnbsLWR8fEHrimKPSXl5icQEiYK2JS60OGIAQ5EoT9TR6vco/CIxp1bOEBZuO51nocw3EQVlFdEu2FSuzyQUj1RIE0VKHHfUZ4n/bGcl0AodvkrOlqw69qW36DL+fr1H2uiASh6BSOPgHRP8OEn08RwZO/SHJWv0luUmksDvN0jjPAZp8+daHh9oHbZW4bbnuAOYLyq+rtnET25YIQ4N5ekVwoLuXp3GsjYebumyICsDWh7lWXyHal5zbXZAR4ll5DgRONbSqcu8IR3xENTk8jCPf3yVb6jM/SxmNAlc3pR6JEpnXrXO2ui0SeYBIoGXjCIbA1gPE0cwxGWAjBnkYuqwmhaQHHjX8KIqDrhbi5mgaR3z1m0cgcDzlfeHHhoJ6gxz19fwbsmRhhaA7iQ0p5SUxQF3qV9RwznuEEXSTvjKuyhrA243e4emjK0fKRAdLNFfXpRsOknoPeYAquWocme7Nhv8ZWCYeY6Rm9ypL+brPdNG79DLRQPgDQhUVtTKkPYix8cNsaB+RPCYLKmFZy6TvRKyooORI197xJFuJ1/esmMCPBsbaKkFKg0/FncTJBhf4WnB2dIk9uenVpYbJkCRxyAICnS40zXWRgdUC3ba3cIzqCXxSzrS3jRAvNLqZ87OFPIIxApDHBqaU0opNwHggGLc9qjoddZGB1TSsaAqTCDZbRY5gZdWoK0/tukFUO3m61slc5byWLk4ro468MQ6a6MDSjt0WqRJ9przKzlGjAQeBWlg/f8vigQdcgpK7dGMpxf0WCiIrd3C2+iAFgfEJJnPDLjcle9acl3HSPKc3TywWpbwM8/mc+3gUgY6ZCg5hEYB5eFWnqHolRrKKpLK7Trl2tyJahwhNqDYS4qut/V1NRP7bhySSv68/thYY212QKs8NlZBswrQWQHfCPGdmdQV9hvil5IgHGZAQ0sb9TRWwsGXNFFNQ86i4q6iefJiYlesjQ5oedGjPZUoAeyITo5jRRpmwZYExAN2m1zNpN+i46RzJ2Y4ZH7dHVF7bhjzebreM210QNt9hTQCP/MYDMk6CHNCc6TgbN3ttGj3lHa+HfGrIuLZXB6XjI7tgiyfv1vZvtOspazXZwqHsXE8+EJBtpyBrbqGdBmAggTwRphjZfAr5qgdjSZUETZInIiusTY6oNLkZvDSo6kD6YgtEAfW55iuTIQm5qlmmgdWQ5EAB3NAGms/ImlnJWSVlRvXWBvdHAlLgY7Bm94EMopoTlEaQ2aBDeTGo1gJpCWzTgYUJvAzD9cQ3IAENKeYp0qpcAdhOx2/4k6+5QcJlgQ+aC954RcOBsAPY4+u86uMsZ8WSBVhj13JCbDz5AK1R7eSL4+YA9c4OJ99jZVY+Xg6Mg0ygZbHXvJSKqzUnr5tmW+vHvBLgffZYm0bd6iVirTDAMSaYliuldymI0MEYlDPprIFYDCpj8UESyPWvqGoiwWgvTCEm20pT0mSwC0cZJAw2V9CniipY68UWnWZuGCBTWW/EqxmFTrZdatoASS5mvJLgY3SMZpkjbXRAUUSDtmSYLmooDsJKYsI0O8DcFkAT7JfHW1/E5sfWVPUCuvxpACQzqzfwjtxQH//938f3/d934czZ85gOBziuc99Lj7xiU/0nzczvP71r8dNN92E4XCIO++8E5/5zGeu+BkXL17EXXfdhd3dXezv7+P7v//7MZvNTvzwvha0O4ryfIE0Y8IiLT/eaTINBi3tfYfsncKowOhqB1Tas5A7RgmE9b+vr0Fz5NKlS3jxi1+Moijwa7/2a/jUpz6Fn/7pn77CseYtb3kL3va2t+Fd73oXPvaxj2E8HuMlL3kJVqtjdb677roLn/zkJ/GBD3wA73vf+/Cf//N/xqtf/eoTP3waKtJE0WapNTeMSAMO22xIp6/VqoC0guKSR3nJUQnXBLoTES4UzApqCl91Xfrikr82Y+Q3v/nNuPnmm/GLv/iL/cduvfXW/v+bGd761rfiR3/0R/E93/M9AIB/9s/+GW644Qb863/9r/GKV7wCn/70p/H+978fv/7rv47nP//5AIC3v/3t+NN/+k/jp37qp3Du3Lkn/TyiQrnflSDd2rCP2ZEWsspNMIHLYDBfC2SWVR+yRjO0E8KiOGEnDLPuYXiib/u3//bf4vnPfz7+wl/4Czh79iy++Zu/GT//8z/ff/5zn/sczp8/f4X9z97eHm6//fYr7H/29/f7YALAnXfeCeccPvaxj33Jv/fL2f9YHhE31yUMRzXSvOj5R8ig29jQVk2S8LYv2FQOlwJczQhIFLLsEtB5g14TG8rf/d3fxTvf+U58wzd8A/79v//3+IEf+AH8zb/5N/FLv/RLAI7te76Uvc/l9j9nz5694vMhBJw+ffrE9j9QgQ0S3NJlrqcgHJC/iWGClYrx7qoXwtIKROhNEtIwO4Jl8RZfcxLqqsSRyLWo5VUV3/It34I3velN+OZv/ma8+tWvxqte9Sq8613vWusvf7Lry9r/hKzMAJr5oaMeegOWHuHQo65DT7PRbqyROUrdTN6vODIppg6SUyq5FpfSTTfd9EWOMs9+9rPx8MMPAzi27/lS9j6X2/9c7lQDADFGXLx48cT2P8hmKLob0dSBUJzCkEbKHFQIStaSr/nggkO7LLIyY8bgZ8EWiWwwp6VHOPLXph/64he/GA899NAVH/vt3/5tPOMZzwDAC+rGG2+8wv7n6OgIH/vYx66w/zk4OMCDDz7Yf80HP/hBqCpuv/32Ez18WPJmlkWedkrGK9WOM6RgSMmh3cuBHhAs5mah959Pw2NJIdd0gAiFXzOgJ7rlf+iHfggvetGL8KY3vQl/8S/+RXz84x/Hz/3cz+Hnfu7nAKC3pPzxH/9xfMM3fANuvfVWvO51r8O5c+fwvd/7vQC4o//Un/pT/VHRti3uuecevOIVrzjRDQ/QZ04GClSKyc4K8+kg55z0ldPCaAJQ51Sp5A6UXGYWR47yGg6QaabVRFLE1zVKPVFAv/VbvxW/+qu/invvvRc/9mM/hltvvRVvfetbcdddd/Vf83f+zt/BfD7Hq1/9ahwcHOCP//E/jve///1XWK695z3vwT333IPv+I7v6K2C3va2t5386QUIM49kgpQcdBngWwEGAptEuMMCBkJt2lMJ7iAP57OyWKvWqzOSTUdhW60My7PrXUqbbf/zU2+ETCrO4XcjnWkeKxDmgtX/VSN8vkQ810AOOE8OM0HzNQ1r/ZWng7dDz1+KI0M6FeEPAmxW4/99/f+9XfY/YeYYTACWBK6k71waGCyDwspRQwxoR1qoEmHfid/vWulFrV3DzpNE2U4vECALtgyIZdI6i/8XyLpNQJv5S6nC8W8bFFZmw+lAC0otM6MuV1BbOfXUzu/DUwoYLS+gNOHUE0K7H1eTy5RKO+4ba2biZXOVzoEWBppNX7eFQ7qeMRwFoYx9LukWDiKsdnygIJbUfL3TtIA/CNzZgRj7sJRjVXBntE5fM7Hf6CGdKPU/Y6XQRI4mXbwyWUEByf1QF3OzZKdFNEFxkeMS0hPRI/SOf/Z6z7TRO9Q1WSZo5dhI7gKaOF8HgPZgwPyyYsA0ZUXHnMybp41FB8NBk+k226iKk0qjQIsHJuNVBtNyN7YHVW823YEZXAQ0d6TSkBcVZTABGC0v/CQCsqVCWN2yYULdBo6TPVDfGOHzoM3VDnGs8LfOkC4z+wOAYkqkCRwdbqwA0sr/gaKy0QG1YX5vW8FyOoAtPWUuFh5pJ0FHCjvdAELTlDAXyDgCVdZ6GuRkfpLg0vGlBMF2Ah2kdtlkyvHMDMeCLNIKqkcDfEGJdWmYItksoBy1SOOcb0YgXKJIll8JiiET0LiNEhmh0wpthDaUSXrNOhso6hsixGmvb2cOQKlojipIEkIfOxq3GJpdhXOUIbomHfuvtkUzPwbMDJBG+gpHqgQ/o9KDX3HmLgZIsN5dVlQIuq0Mcch2YD0vKbGxjTJDABAeL5BGinZRwnYi4k52nF3knuciz44AdqcG7bF9hQFhSj59p/rgS+3ZJeusjQ6olqyG3MoRTpNIhE1DO+bHlwqtCPP2C2HHHuh1nbTILourrNSYA7mVEhld112Hylq+cT2txrwhjjN5QdGrf4sn9p4/IE86HaCeqRNFs7bUG1kSMUzSCHlKpSLuGNKYKOUOo9ScVkoLOwOmBSSPSGBA6P8DgDP55Hr+0zprowNKjYycJjmOO0zoVtOfk002o46CdseyfXr+nkyfcSt6f6TKYC1hOlv5yndLiwwKCxQgsAxeEAWpNXPKYJaXMhpvqEh7iVWS0cEWRqyolKmHiq+zNjqgOrA+vWkX5RW/TSesKlmBset9olDI0vG1FzZN/JJ1fleW6pAN6HXWRgfULcmKgzcMd1d9vZgGfK3TwHq+PHZi70xj5XFNb45B7fGhs8B8dhv7oRCDn/MSWS1Klo2xRNpNMBVIcnCjiDj2wFFAKnnOwhlcPlt9I9DgqIGnoJvN3G+nbpMG7kK/EFjt0U4r4kOPPPxOy46+ES9qlfaYT7fwWVGcvVBJwOAiO0/SUOBlK5XF/EqOS8REM1Qx5MaHHWOVXObA51tdIrJJFVkgokC9zwa0jSLL1DU79hv9ypMGY2j3OagzO+60p6MSKAHnEyTmTn0ERVwzi9llg2nzBDmYN7o1DA3r+qFv9A61wqCThDB1ecrJj4cjKocBeQQSsjaJsdFplR7b+Cqt1Vybd7tlq7VtbN9ZMKqEdylO9vHQ7PyVdhL8bkPV2yiUDsqoZehxEzmO6M4AgArhA9vO5gjEkHaIpZegQOvySAPQSYKf89fT0lCfUsQBgCgQ4050NXoLX1dnXGgmLKzL9dzsgOYmh5YG545HyK4WuIUnqLZ1VBsTKn+73UyZ8YBWDGaqiL/3S4G7fkW5y228lFzt4NUhnY5wifBwO6JDolYK84Jy2EJRHfM5W9ezPVyT5dgr4qEgxEitS6kBNnyH+jpfHrWDJQcJhjTKijYCyrGvir4ul5T1QUHCQ7tPmQwbJLQ73OHa+GPo+BprowNqLqc+kcM1U2H1M3N9685q319aqcq7sCDI1i15HPhLgciSIXulaaxb2m0yZDaHoSiy46kJ0o7CqsSRcFDilpIwq+oGcJm+aCAXFAaUB0yrpJXtNJvuRsEIhsW0gnQdeANfbRX4UrkzA+UwZByPZ++5i+9XgmLqjuXapm7ttGmjL6VOQQyalcQWubxJwpw0m/g5E0pnjOjqjazbDJePAYCQnZxONWcjiqP1nmmjd6ivcz65dJQPAqunkF0PZelhid0lHdATBIVCFoSDw8i5Lw/Y4kvDbKjqthTbFAfGcUXWafJ7TV/pSOt6ZrFrpUfloXHHFpWgpUUaWJ+TAoA7CiiPtrD09LVAS+2HdGkR4OtjQUFXO6BUSAsKCR46FHvcei43kV2bBbKyuQqmBcJCUO9vYaWURgoxTj6tcXBHvBI6E+rOUtI8oTlhBbTzMuND5TjZHzITcJHFQTuxLRVxEUDqDEkEhVjbUzwrSS8UhEEkrXvpWcu3rtfMcy1x+q6mkYA5oNyhVl6xJvFrowNqo9izOfyI6VCYcrQRT1EHL65yN2q/6SHffu4pXLBHNIlEYuxTZWiOKsCAdrTeM210QFF7ghSCUbMucreaB9OguYM4utDYMvRenWknZVVcOnbHnQQN6DVDfS0Iq6/w936FtdkBRb6ZnaGoqAqeSuu5neYBCdpDwrUCdZqB/nztz1JhnuqGMfdU13uejQ6oWzqkCWE3bR1ogdbJW+ROvB4VJHh1o+PVZVWQQ5/HSsoT0Mazsb+NpaeVVLJxhwE+KPycPHcT9Maofr/ppduKqUAmkalU7dhE0eP8Mw6N4oOVIo23MA+Vlk0PHR3LXnbjX6koWSlggOP1LU2ohYM4ya4MAPppqAVAOgecbQSL6UB7L6TYeOCwYAl5qkXn76kqtJpcUqRAG3byw1SO7dRNukYVqYyrLW0wE69Ekz9xRNZpaXCzAMsZgM4KxLHBT/Ov2jrYMCFODGmocA04myrp8akNlV2K+RZCcbobOo0U3meRFs09zVKhnYRTFv+XWqhum4VfXCNIWRfBN1lLr0iwImxnpdRb9qogzorc88yfqx3CYeCl1QpS9pCnqCiHclpZ7xomufEsngxnW/Ot3+iAIp+BNkqQKrEtlwUGO/khP24z9JvdJll6+COPdu8YByrIvp+F0ZdJcKxQdsK10QG1YBRXnXoUg9i7cbs2wxE1X0LZeVYDYOMIu6HuhQbEOHbWwJ3crzUlLzf6DAWItHMNb3l3FCgFXHHnOQDWEN6dxgo/c4QzXggQz15qMRW0u3T5MscStjhyfZl60rXRO7TLJdszEWYCO9PQnOpUhE4SmcnDXPIoCQooiE4WzbJDucxMg+w8a50X/Rb2Qy2L+oeDABwVgJKn5I6yAqMA2vp8NGRmRxJ2qTKMUTQfA3lAh8ibf13tu81+5Z3BjVuk2gG7LdlzKhC14zOwo8yAdb4fc8eyjwqkCggXCxRTIRZ/kGDOb+vU01EJjCQOuEkLveSpyxQlD+WOcUrtjiJNC2qGFtbrjmigQjgATkVNCH9cY234K6/AkmphPih0VlzWRGa9XowaaKD3kqSc9E9ooGIAZ/fLrNQYBW4UoUPdTokMt/JARQ+l4ahmwr4C0pjOiVoaYh0QloI4UYQZKyVbecpkjKjBTDVHvvKaSbdrP9NV/P2u+fLLDP7yRkHWoGh386ucvUBwWNDEL9fv9KrjrB7ZGzks2MZLA8CNIvuq20j8ancT3IKVTYqOI5HI193y8M0CXWU7rVE3jkACyguBI5AqWwAF41kcSXzYSqADgkH3IjWVy9w4Tgyiq7nr3E7b+ySHhcAieU2pzOi7RR6XBEBaZKKYHEPET/pIV/lXvKbLLT3EsVLy3qBVAoTDIAsGLY+FBqjHZLDLzkcLlMNIOwmpG4UMEsz7LTX5q5TpUWHwXpFqgmXTJNESrc78eAHCXoNUcu4OR/GWziAQylo/zGmpbpLtgNZYGx3QDq+klWJxNDgWIMjaS82ZlBvGgD42gK8FzcUBpHZo9y6bceQodCAzOjWs90wbHdCO5SFK7VAZxdxMJmemvOhhtcs69ugNUa1SSJu1R40uDH4lqE8ZinGDuKO0Ul9jbXRAteIgyMYRaRVg8wAtQQlgE3p/lNqPi+Moz+LjsUddx693jaA8EqTo+2bzOmujA9qVlDIPkKAIew2Jr/mqtYKOtDCapJoHqtNLNqRbBrBDK8cRmSBa+944dZ210QF1rRDoAOaPqSVgoTwUoFQKCGZJDGSb3tgGmlI7oB2zd5oh90zmnbFjdS1EXFJKeN3rXodbb70Vw+EQX//1X483vvGNuFwX+1ra/6QsMeRqga9ocmyOoi1Q3uTaeDRnEiUxo8A5atqj43oa0F7fZskhY1ZguDZDuje/+c145zvfiX/0j/4RPv3pT+PNb34z3vKWt+Dtb397/zXX0v7HHJkcWpGJjHmAOaZAiA5xzI93FBu/ELRHJWSQ4NpMmm0E/mLRQ28sOjQ3tog71yCx//CHP4zv+Z7vwUtf+lIAwDOf+Uz8i3/xL/Dxj3+cD/OHZP9T1zXq+rgW7NxqAJK7dJA162pBeeRQn8oXTmEY7a6wPNqBDRPSgHMRa7PhtGOw0+kW8niB8tBhVbJ95xfrHaIn2qEvetGL8MADD+C3f/u3AQD/43/8D/yX//Jf8F3f9V0A/vDsf76cW43lgZwobdR0pB2Du+8YNXXR8+QhwGB/BTeI/ZCu48hz7MEJKunfJ4nM8TrRDv2RH/kRHB0d4VnPeha890gp4Sd+4id6p4U/LPufe++9F6997Wv7fz86OsLNN9+McOSRrqPA1WDYYtn4jETmOWjekFrXIRfhmiwW+ESB0OFKA0vO5nRCO+FREQ782vC7EwX0l3/5l/Ge97wH733ve/Gc5zwHv/Ebv4HXvOY1OHfuHF75yleu9QBPZlVVhar64lsiVceXSNMEIBuqWEF4jqoAQ+7g8gnPQOfohqWg2SNQwgqqkBVHgvR1LXDke8DESdeJAvrDP/zD+JEf+RG84hWvAAA897nPxf/6X/8L9913H175yldeYf9z00039d/36KOP4nnPex6A9ex/vtxykTwjAsEE7rBg133pkIZKykzWGGn3FeUTDlZ7cjkHBpclMN1KYGBeOpsHhNwbXWed6AxdLBZw7spv8d5DlQf4tbb/kSiQaUCYOqSaQFmXcsI/YCuvGESORbIDQzFpSLa9xNadXx2/3fObE6R15IquqTlyoh36Z/7Mn8FP/MRP4JZbbsFznvMc/Pf//t/xMz/zM/irf/Wv8he8xvY/acjbXQsgDCLS3MMvHOINzEPjboI3oVLYOCFc8Fg1HlYq4jg3Vkp2o0a/59FGei9xHnWiR+nXiQL69re/Ha973evwgz/4g3jsscdw7tw5/LW/9tfw+te/vv+aa2n/47I3ctxLKL0ijhPSzGVpS4fB+YB2jzvSVBCHhmIQ0QJwlwLpMzOH+npykzprNRfX17HfbPuft7wRwQ+hlWFw0xzLCyNUjwVoYWhP0aTKxrFXwR2e91g9e0m805JNEL8ilTHM2LVv9xOqxzxwUOMzP/33tsv+x4YKC0yPltMKCEyZ4k7OMccRfkhte3g6epWDSBBEkqwmznSq3TGk0iDZbHXdM3SjA9p5G8NRO7nTDXIrBxnRaEpb13t5mjOsZgTh2yghThIFcAtDmGef5dyx306ZoYr+nG5BLxAJ1KzTG2rgUg6cAeUlB93Nt33jIK2DG0SICtoJqd89WMzlWdOakdnsId1hAe849m1nJWdDNaAXS+g4weYeoUpsLK8cxQY0SwRfrOBrQXlJEEeSlRwE+LoG6SDAL9d8pqv7K17j1WmOOFCbvjA0p5TCLlWCWwlizZmSZP0mScJmSkvxavMUHlAPNLuG9ogV2VYy6XSovVK4OANa+ivpwGAL4uuLYXushCsG7LTsME2Y+GvBRnVYZr0mT1XHrQyoNBk9VxrKYUsx1fq48jFniE1HWeRudMFgQeFWLnvV8fvr03RsQJRMtVnvmTY7oJ3Bnwe8V/gnyEmCoFdntCUnmnJDjXaXs3s4TkDDnNJD5oBiJn0yHydbOlOyYJCVo6WkV6R9MpIlstyECWSYrSkXAeUlB/HaixC2p5W7WThOaU6z3pSE7aTVSBLAA+H6FZbL8vjVDhwf60BhNaE60u3YVQdnZpdJC/TWP2HqjnFO2wh0CFO6LLSHFdpZSV+lueuxTAB6XXrk2bsbRfolLz00MKnvkCK+Ecgwrg1lBDY8oGmYd2OVONYw9NbmNENVyKWyN6yKY/p8AtyBxVR6Og1vfIM11MrbSqk2vpoCLD0phSWRyBYMMg8cFZ+mla+7jLyAgpDvVB2/7p3Zn1Rpe7XvrPsfR11QAMc9Tp8dFALLynh9czx3jwIEg2vZjG53OOwLcwGmBSCGdrzeM210QMUyJzMoAwX0dj4dsbYoEs/E2qM8cAhFghtHutNeNrxLJbmeJoSUb6c3cidAsPKcyzujI03gDhUDmlXIhE6y5tplAZ1SqCAOeVaK4VgJInuBbmViD2QmXKlwXiF1TuKj9FAbApeQAWEK8crGc2Dg1QOuBoaPsX3Xff1W9kOLgzyYW3hocrBs+Nd7znsjpt4BKAzFEYli2Gt7/dCwFLR72p+ZNlDu2msxU/pqW3GskB3KDHkxyNIjzMhJcuMWSQJCmZB8QDluEMdkIds8UEgwAs2+EsU3MqQB6AXiAbeNOxQGdu2zXLCVHIm4WqDzAn7qYcZBXnNUoTzMOqPBEHcV7U5WdiiJsxflzyzmsr26TeHQQ2cFUsuhW302wUXpqd/a8rz0R55usolqD2HaGQLyP4AYmyVWciZvW0lN7AyqnEFbcj7LJxwbx1WCDhTlqAGMyt+DJ4Ts5FIRbyIH1CXW7lA2laVUtDuKsKY640afoeYNKNhVKkY1Wimg05KBqT383KF2Q5QNYEuHdkJjKlOORHSgiJqHeEofJVtRJnPdBvNmB3SoEAfIMhugmrC+z240AABvrOG9wbxAR/n6voyYQPdZAJItLSNQnBxQDWDDX3l/5IGsFN6NkSWC86WO0FAl9jYFUA/4SUuDFRWEKa9yDYbV2Uy8TfyP0jx5bMMVa6MDmvZiD5o1BbDyEBUKtQhzSecvUw1XSrFJSyHBNFAUU86XIEbe/H6bdZ62kEmHbGqKgWIwargL881vBbvxaenhFwREmAd0EWATEsQkCtpdyguHhWOzxBniSLe09DTuQpl7pOQAsd5R1s8850sqaPaJBy2mAimVXz80WEUFMQ38Z5wYNfM6RMoaa7MDmllwVioZcI2Dq1mzpyF32c7ZWT9wa/b4GtsoQccpSwxzjOwiWSLIvshbmdi7FUEMfuoRlwFW0focWeW2mApmh0POjYJSYKB2vbcnwMlpV9e7CMgsX1TbOJfXYWIvc6K0nAA7R53+Z7trCFXMFZGjUWq29YVlH08FXJPHKZ7V19bOlMJhoH9HIxhOasggIY4N6XRLWuJK0B4M+kZydcn1Qq24TOG286TrjFZcI2vnoRud2ENB6Ysjh+V0AFt4VAcOsS2ZChXEe8Y6i7COHDWWg0KeKGkQmDxSaQQ6ZDNqOL+dIi4AufFxRyl94WjNG3cSrFD4lYM1nr3NLOJiKlmBLIu95J+TKo6R0VKrZCsbzHE/UidUAZcdvrTIRtI1E3cgm61Emv9ZpPBAd+mkkcKCIe6mXpHROgmNNdZGB9QtuY1sJ2Iwalhmdnr1l6004KTTL6j8QDzUMUnBLx13q0OPdI7baF1huez0TxRYzitY69jcyJNPiUIABIi3F6OkG5RNZy2ys3fOZ9PAejSfbuMr75sOBZL/mXFMWmSoowfMJGOgCBdv5wUHdrsJvibVRgeWdyqbJpLWd+/e6ICa47g4nm0ohGWs5TvxatcCRcW6HTW3nKw8wpRm1H7JXNTVAt+QP2+DdCz4ssba6IB2S6YBcVXAKo4vup6npNwnzWNkzeCHOCH4oUPq+QXPV1IYs+/8mgnlRueh5gA3z6XiijMl1wjUc59oZShHDaKUOQMAsN8ACxqjxgmDGXcIgnBt1sDcWpXw4soy0ZyhmPPCAVjxxOgRlpzdDx4/nnr21EMBdNwN9gCplK4MW6nB7A02TGy1BYVrHLSwPrfUwrLXMc/EdmJAzRwVYr3lefE45/SSBBb56rttPEPDzKN4gvU8W3nZ8mfhehw9wIsq7SSkkWJ845zy6tlQ2mf2B927rbe+WBcSvtFnaBoobKDw19XwISFWCfboAOZY32tFkFh56LAaGMLMYW+0xFzGKA49ybI5kFpyiAfj2eu2KaAdgdrmNWThkI6AeH2ELgTlYw10YKh3U/bpVCzGCpMIPy3w6OcDcNhAlwQ56DBTRFdEN+ugBRqHVDZX/F1Pdm0kvft3f/d38fVf//XX5O965JFH8LVf+7VP+us3coeePn0aAPDwww9jb2/vD/zzOpWdRx55pOfGmxmm0+mJVSY2MqCd7sne3t6JxAH+T2t3d/eKn7fOf6yNvuW/GtfTAb3KayMDWlUV3vCGN3xJcayn+udt5C3/1bw2cod+Na+nA3qV19MBvcrr6YBe5fV0QK/y2siAvuMd78Azn/lMDAYD3H777b1k8Vdaf//v/32IyBV/nvWsZ/WfX61WuPvuu3HmzBlMJhO87GUvw6OPPnryh7MNW/fff7+VZWm/8Au/YJ/85CftVa96le3v79ujjz76Fb/vDW94gz3nOc+xL3zhC/2fCxcu9J//63/9r9vNN99sDzzwgH3iE5+wF77whfaiF73oxM+3cQF9wQteYHfffXf/7yklO3funN13331f8fve8IY32B/7Y3/sS37u4ODAiqKwX/mVX+k/9ulPf9oA2Ec+8pETPd9GvfJN0+DBBx+8QjTbOYc777yzF83+Suszn/kMzp07h6/7uq/DXXfdhYcffhgA8OCDD6Jt2yt+7rOe9SzccsstT+rnXr42KqCPP/44UkpfUTT7y63bb78d7373u/H+978f73znO/G5z30Of+JP/AlMp1OcP38eZVlif3//xD/3f18b2b5bZ3XS8ADwTd/0Tbj99tvxjGc8A7/8y7+M4XB41f6ejdqh1113Hbz3X3T7PvrooycWxN7f38cf+SN/BJ/97Gdx4403omkaHBwc/IF/7kYFtCxL3HbbbVeIZqsqHnjggV40+8mu2WyG3/md38FNN92E2267DUVRXPFzH3roITz88MMn/rkbd8vff//9VlWVvfvd77ZPfepT9upXv9r29/ft/PnzX/H7/vbf/tv2n/7Tf7LPfe5z9l//63+1O++806677jp77LHHzIxp0y233GIf/OAH7ROf+ITdcccddscdd5z4+TYuoGZmb3/72+2WW26xsiztBS94gX30ox/9P37Py1/+crvpppusLEv7mq/5Gnv5y19un/3sZ/vPL5dL+8Ef/EE7deqUjUYj+7N/9s/aF77whRM/29P90Ku8NuoM3YT1dECv8no6oFd5PR3Qq7yeDuhVXk8H9CqvpwN6ldfTAb3K6+mAXuX1dECv8no6oFd5/X/nrdVSyy9JDgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(longest_shakespeare, return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract LayerNorm inputs\n",
    "ln_inputs = outputs[-1][-1]\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
