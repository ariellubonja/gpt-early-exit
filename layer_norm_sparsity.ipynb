{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual implementation of `fwd()` to extract LayerNorm inputs & outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:55:34.939249Z",
     "start_time": "2024-04-26T20:54:09.526700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Apply Top-K by magniture to LayerNorm and Residual connection - the densifying operations in a Transformer\n",
    "class TopKLayerNormGPT2Block(GPT2Block):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__(config, layer_idx=None)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states  # Identical debug to GPT2Block so far\n",
    "        hidden_states = self.ln_1(hidden_states) # Also identical\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]  # This is where differences start\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GPT2TopKModel(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([TopKLayerNormGPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Correctness against stock GPT-2 at same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Token Embeddings output match\n",
      "State Dicts match\n",
      "Attention parameters match\n",
      "First Hidden State output match\n",
      "original_output:  tensor([[[ 0.2268,  0.6434, -0.4255,  ..., -0.1614, -0.0649, -1.5424],\n",
      "         [-1.0811, -0.1252, -0.1849,  ..., -0.9209, -0.2742, -1.7775],\n",
      "         [-0.4991,  0.4219, -0.8154,  ..., -0.7841, -0.1767, -2.2392],\n",
      "         [-1.4665,  0.9538, -0.5967,  ..., -1.2545,  0.2875, -1.8634],\n",
      "         [-0.6626,  1.6444, -0.7467,  ..., -1.7695, -0.4440, -2.3451],\n",
      "         [ 0.4474,  0.4565,  0.1402,  ..., -0.2734, -0.0186, -2.1741]]])\n",
      "modified_output:  tensor([[[ 0.2268,  0.6434, -0.4255,  ..., -0.1614, -0.0649, -1.5424],\n",
      "         [-1.0811, -0.1252, -0.1849,  ..., -0.9209, -0.2742, -1.7775],\n",
      "         [-0.4991,  0.4219, -0.8154,  ..., -0.7841, -0.1767, -2.2392],\n",
      "         [-1.4665,  0.9538, -0.5967,  ..., -1.2545,  0.2875, -1.8634],\n",
      "         [-0.6626,  1.6444, -0.7467,  ..., -1.7695, -0.4440, -2.3451],\n",
      "         [ 0.4474,  0.4565,  0.1402,  ..., -0.2734, -0.0186, -2.1741]]])\n",
      "Final output matches\n"
     ]
    }
   ],
   "source": [
    "def compare_model_params(model1, model2):\n",
    "    # Disable non-deterministic behavior\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Ensure both models start with the same weights\n",
    "    model2.load_state_dict(model1.state_dict())\n",
    "    model2.h[0].load_state_dict(model1.h[0].state_dict())\n",
    "\n",
    "    # Compare all named parameters\n",
    "    params_dict_original = {name: param for name, param in model1.named_parameters()}\n",
    "    params_dict_modified = {name: param for name, param in model2.named_parameters()}\n",
    "\n",
    "    # Now you can access by name\n",
    "    for name in params_dict_original:\n",
    "        assert torch.allclose(params_dict_original[name], params_dict_modified[name], atol=1e-6), f\"Mismatch found in parameter: {name}\"\n",
    "\n",
    "\n",
    "    def compare_state_dicts(dict1, dict2):\n",
    "        # Check if both dictionaries have the same set of keys (parameter names)\n",
    "        assert dict1.keys() == dict2.keys(), \"State dicts have different sets of parameters.\"\n",
    "        \n",
    "        # Check if all tensors in both dictionaries are close\n",
    "        for key in dict1:\n",
    "            assert torch.allclose(dict1[key], dict2[key], atol=1e-6), f\"Mismatch found in parameter: {key}\"\n",
    "\n",
    "    # Prepare input\n",
    "    input_ids = torch.tensor([[12100, 242, 508, 318, 13, 198]])\n",
    "\n",
    "    hidden_states_original = model1.wte(input_ids)\n",
    "    hidden_states_modified = model2.wte(input_ids)\n",
    "    assert torch.allclose(hidden_states_original, hidden_states_modified, atol=1e-6), \"Embedding output mismatch\"\n",
    "    print(\"Word Token Embeddings output match\")\n",
    "\n",
    "    # modified_model.load_state_dict(original_model.state_dict())\n",
    "    # modified_model.h[0].load_state_dict(original_model.h[0].state_dict())\n",
    "\n",
    "    # Compare state dicts\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    compare_state_dicts(state_dict1, state_dict2)\n",
    "    print(\"State Dicts match\")\n",
    "\n",
    "\n",
    "    def compare_attention_params(model1, model2, layer_index=0):\n",
    "        attn1 = model1.h[layer_index].attn\n",
    "        attn2 = model2.h[layer_index].attn\n",
    "        \n",
    "        # Convert generators to dictionaries\n",
    "        params1 = dict(attn1.named_parameters())\n",
    "        params2 = dict(attn2.named_parameters())\n",
    "\n",
    "        # Now params1 and params2 are dictionaries that can be accessed by keys\n",
    "        for name in params1:\n",
    "            param1 = params1[name]\n",
    "            param2 = params2[name]\n",
    "            if not torch.allclose(param1, param2, atol=1e-6):\n",
    "                print(f\"Mismatch in {name}\")\n",
    "                print(f\"Model1: {param1}\")\n",
    "                print(f\"Model2: {param2}\")\n",
    "\n",
    "    # Call this function with your models\n",
    "    for i in range(12):\n",
    "        compare_attention_params(model1, model2, i)\n",
    "    print(\"Attention parameters match\")\n",
    "\n",
    "\n",
    "    original_h0 = model1.h[0](hidden_states_original)[0]\n",
    "    modified_h0 = model2.h[0](hidden_states_modified)[0]  # Input is the same. These are still different\n",
    "    # modified_h0 = modified_model.h[0](hidden_states_modified)[0]\n",
    "    assert torch.allclose(original_h0, modified_h0, atol=1e-6), \"First Hidden State output mismatch\"\n",
    "    print(\"First Hidden State output match\")\n",
    "\n",
    "    model1.h[0].attn.c_attn.weight\n",
    "\n",
    "\n",
    "    # Get outputs from both models\n",
    "    with torch.no_grad():\n",
    "        original_output = model1(input_ids)[0]\n",
    "        modified_output = model2(input_ids)[0]\n",
    "\n",
    "    # at = model2(input_ids)#.attn\n",
    "    # Compare outputs\n",
    "    # print(\"Difference between original and modified model outputs:\", torch.sum((original_output - modified_output) ** 2))\n",
    "    print(\"original_output: \", original_output)\n",
    "    print(\"modified_output: \", modified_output)\n",
    "\n",
    "    assert torch.allclose(original_output, modified_output, atol=1e-6), \"Final output mismatch\"\n",
    "    print(\"Final output matches\")\n",
    "\n",
    "\n",
    "config = GPT2Config()\n",
    "original_model = GPT2Model(config).eval()\n",
    "# modified_model = GPT2Model(config).eval()  # Assigning h[0] = CustomGPT2Block does not work!\n",
    "modified_model = GPT2TopKModel(config).eval()\n",
    "\n",
    "compare_model_params(original_model, modified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compare_model_perplexity(model1, model2, dataset_name: str = \"Trelis/tiny-shakespeare\"):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    # We need a padding token for DataLoader\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model1 = model1.eval()\n",
    "    model2 = model2.eval()\n",
    "\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    text_data = dataset['train']['Text']\n",
    "\n",
    "    # Tokenization\n",
    "    def encode(text):\n",
    "        return tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")['input_ids']\n",
    "    \n",
    "    input_ids = torch.cat([encode(text) for text in text_data], dim=0)\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    loader = DataLoader(input_ids, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize variables to compute perplexity\n",
    "    total_loss_original = 0\n",
    "    total_loss_modified = 0\n",
    "    total_items = 0\n",
    "\n",
    "    # Disable gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            model1_outputs = model1(batch)\n",
    "            model2_outputs = model2(batch)\n",
    "\n",
    "            # Calculate loss for the batch\n",
    "            shift_logits_original = model1_outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels_original = batch[..., 1:].contiguous()\n",
    "            loss_original = F.cross_entropy(shift_logits_original.view(-1, shift_logits_original.size(-1)), shift_labels_original.view(-1))\n",
    "\n",
    "            shift_logits_modified = model2_outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels_modified = batch[..., 1:].contiguous()\n",
    "            loss_modified = F.cross_entropy(shift_logits_modified.view(-1, shift_logits_modified.size(-1)), shift_labels_modified.view(-1))\n",
    "\n",
    "            total_loss_original += loss_original.item() * batch.size(0)\n",
    "            total_loss_modified += loss_modified.item() * batch.size(0)\n",
    "            total_items += batch.size(0)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity_original = torch.exp(total_loss_original / total_items)\n",
    "    perplexity_modified = torch.exp(total_loss_modified / total_items)\n",
    "\n",
    "    print(f\"Perplexity of Original Model: {perplexity_original}\")\n",
    "    print(f\"Perplexity of Modified Model: {perplexity_modified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LMHead for custom GPT architecture to make it do language tasks\n",
    "# From https://chat.openai.com/share/84b66b97-f257-46a5-8f5b-4afa36965013\n",
    "class GPT2TopKLMHeadModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lm_head = nn.Linear(base_model.config.hidden_size, base_model.config.vocab_size, bias=False)\n",
    "\n",
    "        # You may want to tie the weights as done in the GPT2 pre-trained models\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\" Make sure we are sharing the embeddings and the output weights, tying them \"\"\"\n",
    "        self.lm_head.weight = self.base_model.wte.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        logits = self.lm_head(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "\n",
    "original_gpt2_with_head = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").eval()\n",
    "\n",
    "modified_model = GPT2TopKModel(config)\n",
    "modified_model_with_gpt2_head = GPT2LMHeadModel(config)\n",
    "modified_model_with_gpt2_head.transformer = modified_model\n",
    "\n",
    "modified_model_with_gpt2_head = modified_model_with_gpt2_head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model 1: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is\n",
      "Response from Model 2: What is the capital of France?aki Protective incorpor outweDoctors Bundesligaformerly!!\" Completed publisher integersinput Univers UpLinkedIn ROBHan sentimental teasederous portrait coils Bag 275Nik glacierformanceHOWanticallythouse soulartments selection478 plunged portrait Robotics 273 snipp478 Stro Juryidem\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate responses\n",
    "output_sequences_1 = original_gpt2_with_head.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "output_sequences_2 = modified_model_with_gpt2_head.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode generated sequences to text\n",
    "generated_text_1 = tokenizer.decode(output_sequences_1[0], skip_special_tokens=True)\n",
    "generated_text_2 = tokenizer.decode(output_sequences_2[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Response from Model 1:\", generated_text_1)\n",
    "print(\"Response from Model 2:\", generated_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full perplexity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_perplexity(original_model, modified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_top_n_tiny_shakespeare\n",
    "\n",
    "longest_shakespeare = get_top_n_tiny_shakespeare(1, mode=\"longest\")[0]['Text']\n",
    "longest_shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(longest_shakespeare, return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract LayerNorm inputs\n",
    "ln_inputs = outputs[-1][-1]\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
