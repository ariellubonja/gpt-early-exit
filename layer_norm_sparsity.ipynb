{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual implementation of `fwd()` to extract LayerNorm inputs & outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:55:34.939249Z",
     "start_time": "2024-04-26T20:54:09.526700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Apply Top-K by magniture to LayerNorm and Residual connection - the densifying operations in a Transformer\n",
    "class TopKLayerNormGPT2Block(GPT2Block):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__(config, layer_idx=None)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states  # Identical debug to GPT2Block so far\n",
    "        hidden_states = self.ln_1(hidden_states) # Also identical\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]  # This is where differences start\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GPT2TopKModel(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([TopKLayerNormGPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Correctness against stock GPT-2 at same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Token Embeddings output match\n",
      "State Dicts match\n",
      "Attention parameters match\n",
      "First Hidden State output match\n",
      "original_output:  tensor([[[-1.8500,  0.4044,  1.2790,  ...,  0.6922, -0.2059,  1.2088],\n",
      "         [-2.1102, -0.5238,  2.0453,  ...,  0.6466, -0.7434,  0.7953],\n",
      "         [-2.2204, -1.7145,  1.8153,  ...,  0.7739, -1.7573, -0.0853],\n",
      "         [-2.1437, -0.3642,  1.7688,  ...,  1.0593, -1.1460,  0.5527],\n",
      "         [-1.9082, -1.3415,  1.9150,  ...,  0.8768, -0.4160, -0.1938],\n",
      "         [-1.3761, -1.4266,  1.8673,  ...,  0.8648,  0.0352, -0.4344]]])\n",
      "modified_output:  tensor([[[-1.8500,  0.4044,  1.2790,  ...,  0.6922, -0.2059,  1.2088],\n",
      "         [-2.1102, -0.5238,  2.0453,  ...,  0.6466, -0.7434,  0.7953],\n",
      "         [-2.2204, -1.7145,  1.8153,  ...,  0.7739, -1.7573, -0.0853],\n",
      "         [-2.1437, -0.3642,  1.7688,  ...,  1.0593, -1.1460,  0.5527],\n",
      "         [-1.9082, -1.3415,  1.9150,  ...,  0.8768, -0.4160, -0.1938],\n",
      "         [-1.3761, -1.4266,  1.8673,  ...,  0.8648,  0.0352, -0.4344]]])\n",
      "Final output matches\n"
     ]
    }
   ],
   "source": [
    "def compare_model_params(model1, model2):\n",
    "    # Disable non-deterministic behavior\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Ensure both models start with the same weights\n",
    "    model2.load_state_dict(model1.state_dict())\n",
    "    model2.h[0].load_state_dict(model1.h[0].state_dict())\n",
    "\n",
    "    # Compare all named parameters\n",
    "    params_dict_original = {name: param for name, param in model1.named_parameters()}\n",
    "    params_dict_modified = {name: param for name, param in model2.named_parameters()}\n",
    "\n",
    "    # Now you can access by name\n",
    "    for name in params_dict_original:\n",
    "        assert torch.allclose(params_dict_original[name], params_dict_modified[name], atol=1e-6), f\"Mismatch found in parameter: {name}\"\n",
    "\n",
    "\n",
    "    def compare_state_dicts(dict1, dict2):\n",
    "        # Check if both dictionaries have the same set of keys (parameter names)\n",
    "        assert dict1.keys() == dict2.keys(), \"State dicts have different sets of parameters.\"\n",
    "        \n",
    "        # Check if all tensors in both dictionaries are close\n",
    "        for key in dict1:\n",
    "            assert torch.allclose(dict1[key], dict2[key], atol=1e-6), f\"Mismatch found in parameter: {key}\"\n",
    "\n",
    "    # Prepare input\n",
    "    input_ids = torch.tensor([[12100, 242, 508, 318, 13, 198]])\n",
    "\n",
    "    hidden_states_original = model1.wte(input_ids)\n",
    "    hidden_states_modified = model2.wte(input_ids)\n",
    "    assert torch.allclose(hidden_states_original, hidden_states_modified, atol=1e-6), \"Embedding output mismatch\"\n",
    "    print(\"Word Token Embeddings output match\")\n",
    "\n",
    "    # modified_model.load_state_dict(original_model.state_dict())\n",
    "    # modified_model.h[0].load_state_dict(original_model.h[0].state_dict())\n",
    "\n",
    "    # Compare state dicts\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    compare_state_dicts(state_dict1, state_dict2)\n",
    "    print(\"State Dicts match\")\n",
    "\n",
    "\n",
    "    def compare_attention_params(model1, model2, layer_index=0):\n",
    "        attn1 = model1.h[layer_index].attn\n",
    "        attn2 = model2.h[layer_index].attn\n",
    "        \n",
    "        # Convert generators to dictionaries\n",
    "        params1 = dict(attn1.named_parameters())\n",
    "        params2 = dict(attn2.named_parameters())\n",
    "\n",
    "        # Now params1 and params2 are dictionaries that can be accessed by keys\n",
    "        for name in params1:\n",
    "            param1 = params1[name]\n",
    "            param2 = params2[name]\n",
    "            if not torch.allclose(param1, param2, atol=1e-6):\n",
    "                print(f\"Mismatch in {name}\")\n",
    "                print(f\"Model1: {param1}\")\n",
    "                print(f\"Model2: {param2}\")\n",
    "\n",
    "    # Call this function with your models\n",
    "    for i in range(12):\n",
    "        compare_attention_params(model1, model2, i)\n",
    "    print(\"Attention parameters match\")\n",
    "\n",
    "\n",
    "    original_h0 = model1.h[0](hidden_states_original)[0]\n",
    "    modified_h0 = model2.h[0](hidden_states_modified)[0]  # Input is the same. These are still different\n",
    "    # modified_h0 = modified_model.h[0](hidden_states_modified)[0]\n",
    "    assert torch.allclose(original_h0, modified_h0, atol=1e-6), \"First Hidden State output mismatch\"\n",
    "    print(\"First Hidden State output match\")\n",
    "\n",
    "    model1.h[0].attn.c_attn.weight\n",
    "\n",
    "\n",
    "    # Get outputs from both models\n",
    "    with torch.no_grad():\n",
    "        original_output = model1(input_ids)[0]\n",
    "        modified_output = model2(input_ids)[0]\n",
    "\n",
    "    # at = model2(input_ids)#.attn\n",
    "    # Compare outputs\n",
    "    # print(\"Difference between original and modified model outputs:\", torch.sum((original_output - modified_output) ** 2))\n",
    "    print(\"original_output: \", original_output)\n",
    "    print(\"modified_output: \", modified_output)\n",
    "\n",
    "    assert torch.allclose(original_output, modified_output, atol=1e-6), \"Final output mismatch\"\n",
    "    print(\"Final output matches\")\n",
    "\n",
    "\n",
    "config = GPT2Config()\n",
    "original_model = GPT2Model(config).eval()\n",
    "# modified_model = GPT2Model(config).eval()  # Assigning h[0] = CustomGPT2Block does not work!\n",
    "modified_model = GPT2TopKModel(config).eval()\n",
    "\n",
    "compare_model_params(original_model, modified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compare_model_perplexity(model1, model2, dataset_name: str = \"Trelis/tiny-shakespeare\"):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    # We need a padding token for DataLoader\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model1 = model1.eval()\n",
    "    model2 = model2.eval()\n",
    "\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    text_data = dataset['train']['Text']\n",
    "\n",
    "    # Tokenization\n",
    "    def encode(text):\n",
    "        return tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")['input_ids']\n",
    "    \n",
    "    input_ids = torch.cat([encode(text) for text in text_data], dim=0)\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    loader = DataLoader(input_ids, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize variables to compute perplexity\n",
    "    total_loss_original = 0\n",
    "    total_loss_modified = 0\n",
    "    total_items = 0\n",
    "\n",
    "    # Disable gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            model1_outputs = model1(batch)\n",
    "            model2_outputs = model2(batch)\n",
    "\n",
    "            # Calculate loss for the batch\n",
    "            shift_logits_original = model1_outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels_original = batch[..., 1:].contiguous()\n",
    "            loss_original = F.cross_entropy(shift_logits_original.view(-1, shift_logits_original.size(-1)), shift_labels_original.view(-1))\n",
    "\n",
    "            shift_logits_modified = model2_outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels_modified = batch[..., 1:].contiguous()\n",
    "            loss_modified = F.cross_entropy(shift_logits_modified.view(-1, shift_logits_modified.size(-1)), shift_labels_modified.view(-1))\n",
    "\n",
    "            total_loss_original += loss_original.item() * batch.size(0)\n",
    "            total_loss_modified += loss_modified.item() * batch.size(0)\n",
    "            total_items += batch.size(0)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity_original = torch.exp(total_loss_original / total_items)\n",
    "    perplexity_modified = torch.exp(total_loss_modified / total_items)\n",
    "\n",
    "    print(f\"Perplexity of Original Model: {perplexity_original}\")\n",
    "    print(f\"Perplexity of Modified Model: {perplexity_modified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LMHead for custom GPT architecture to make it do language tasks\n",
    "# From https://chat.openai.com/share/84b66b97-f257-46a5-8f5b-4afa36965013\n",
    "class GPT2TopKLMHeadModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lm_head = nn.Linear(base_model.config.hidden_size, base_model.config.vocab_size, bias=False)\n",
    "\n",
    "        # You may want to tie the weights as done in the GPT2 pre-trained models\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\" Make sure we are sharing the embeddings and the output weights, tying them \"\"\"\n",
    "        self.lm_head.weight = self.base_model.wte.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        logits = self.lm_head(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "\n",
    "# original_gpt2_with_head = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").eval()\n",
    "\n",
    "original_model = GPT2Model(config)\n",
    "original_model_with_gpt2_head = GPT2LMHeadModel(config)\n",
    "original_model_with_gpt2_head.transformer = original_model\n",
    "\n",
    "original_model_with_gpt2_head = original_model_with_gpt2_head.eval()\n",
    "\n",
    "# These should definitely be identical\n",
    "modified_model = GPT2Model(config)\n",
    "\n",
    "modified_model.load_state_dict(original_model.state_dict())\n",
    "\n",
    "# modified_model = GPT2TopKModel(config)\n",
    "modified_model_with_gpt2_head = GPT2LMHeadModel(config)\n",
    "modified_model_with_gpt2_head.transformer = modified_model\n",
    "\n",
    "modified_model_with_gpt2_head = modified_model_with_gpt2_head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_model_with_gpt2_head.lm_head.load_state_dict(original_model_with_gpt2_head.lm_head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Token Embeddings output match\n",
      "State Dicts match\n",
      "Attention parameters match\n",
      "First Hidden State output match\n",
      "original_output:  tensor([[[ 1.3025,  0.0891,  1.5785,  ..., -0.7348,  1.3662, -0.0760],\n",
      "         [ 1.0428, -0.0767,  1.0057,  ..., -1.0282,  2.3909,  0.0945],\n",
      "         [ 0.3019, -0.7627,  0.9617,  ..., -0.5008,  1.7748,  0.4493],\n",
      "         [ 1.0860, -0.4353,  0.8214,  ..., -0.2734,  1.0454, -0.1068],\n",
      "         [ 1.1729, -0.5770,  1.1421,  ..., -0.7705,  0.6708,  0.8564],\n",
      "         [ 1.0173, -0.1443,  1.4305,  ..., -1.1474,  1.7968,  0.4672]]])\n",
      "modified_output:  tensor([[[ 1.3025,  0.0891,  1.5785,  ..., -0.7348,  1.3662, -0.0760],\n",
      "         [ 1.0428, -0.0767,  1.0057,  ..., -1.0282,  2.3909,  0.0945],\n",
      "         [ 0.3019, -0.7627,  0.9617,  ..., -0.5008,  1.7748,  0.4493],\n",
      "         [ 1.0860, -0.4353,  0.8214,  ..., -0.2734,  1.0454, -0.1068],\n",
      "         [ 1.1729, -0.5770,  1.1421,  ..., -0.7705,  0.6708,  0.8564],\n",
      "         [ 1.0173, -0.1443,  1.4305,  ..., -1.1474,  1.7968,  0.4672]]])\n",
      "Final output matches\n"
     ]
    }
   ],
   "source": [
    "compare_model_params(original_model, modified_model) # so this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[50256, 318, 262, 263, 734, 262, 1576, 50256]])  # Random example tokens\n",
    "\n",
    "# Generate outputs with hidden states and attentions\n",
    "outputs_original = original_model_with_gpt2_head(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "outputs_modified = modified_model_with_gpt2_head(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "# Accessing the logits, hidden states, and attentions\n",
    "logits_original, hidden_states_original, attentions_original = outputs_original.logits, outputs_original.hidden_states, outputs_original.attentions\n",
    "logits_modified, hidden_states_modified, attentions_modified = outputs_modified.logits, outputs_modified.hidden_states, outputs_modified.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(logits_original, logits_modified, atol=1e-6)  # This should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5882, -0.1566,  0.9692,  ...,  1.6813,  0.2022,  0.7559],\n",
       "         [-0.8870, -0.0666,  0.8193,  ...,  1.7804, -0.2431, -0.2938],\n",
       "         [-0.0335,  0.2815,  0.4804,  ...,  1.5229, -0.0332,  0.3242],\n",
       "         ...,\n",
       "         [-0.4287, -0.7156,  0.6954,  ...,  0.8741, -0.5616,  0.4157],\n",
       "         [-0.4121, -0.3123,  0.4581,  ...,  0.6161,  0.0355,  0.3238],\n",
       "         [ 0.3711,  0.3823, -0.2987,  ...,  1.0961, -0.8531,  0.6987]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5882, -0.1566,  0.9692,  ...,  1.6813,  0.2022,  0.7559],\n",
       "         [-0.8870, -0.0666,  0.8193,  ...,  1.7804, -0.2431, -0.2938],\n",
       "         [-0.0335,  0.2815,  0.4804,  ...,  1.5229, -0.0332,  0.3242],\n",
       "         ...,\n",
       "         [-0.4287, -0.7156,  0.6954,  ...,  0.8741, -0.5616,  0.4157],\n",
       "         [-0.4121, -0.3123,  0.4581,  ...,  0.6161,  0.0355,  0.3238],\n",
       "         [ 0.3711,  0.3823, -0.2987,  ...,  1.0961, -0.8531,  0.6987]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(hidden_states_original[0], hidden_states_modified[0])\n",
    "torch.allclose(hidden_states_original[1], hidden_states_modified[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attentions_original[0], attentions_modified[0])\n",
    "torch.allclose(attentions_original[1], attentions_modified[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_gpt2_with_head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Generate responses\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m output_sequences_1 \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_gpt2_with_head\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m output_sequences_2 \u001b[38;5;241m=\u001b[39m modified_model_with_gpt2_head\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Decode generated sequences to text\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_gpt2_with_head' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate responses\n",
    "output_sequences_1 = original_gpt2_with_head.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "output_sequences_2 = modified_model_with_gpt2_head.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode generated sequences to text\n",
    "generated_text_1 = tokenizer.decode(output_sequences_1[0], skip_special_tokens=True)\n",
    "generated_text_2 = tokenizer.decode(output_sequences_2[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Response from Model 1:\", generated_text_1)\n",
    "print(\"Response from Model 2:\", generated_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full perplexity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_model_perplexity(original_model, modified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_top_n_tiny_shakespeare\n",
    "\n",
    "longest_shakespeare = get_top_n_tiny_shakespeare(1, mode=\"longest\")[0]['Text']\n",
    "longest_shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(longest_shakespeare, return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract LayerNorm inputs\n",
    "ln_inputs = outputs[-1][-1]\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
