{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual implementation of `fwd()` to extract LayerNorm inputs & outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:55:34.939249Z",
     "start_time": "2024-04-26T20:54:09.526700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# Apply Top-K by magniture to LayerNorm and Residual connection - the densifying operations in a Transformer\n",
    "class TopKLayerNormGPT2Block(GPT2Block):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__(config, layer_idx=None)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states  # Identical debug to GPT2Block so far\n",
    "        hidden_states = self.ln_1(hidden_states) # Also identical\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]  # This is where differences start\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GPT2TopKModel(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([TopKLayerNormGPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Correctness against stock GPT-2 at same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Token Embeddings output match\n",
      "State Dicts match\n",
      "Attention parameters match\n",
      "First Hidden State output match\n",
      "original_output:  tensor([[[-1.8500,  0.4044,  1.2790,  ...,  0.6922, -0.2059,  1.2088],\n",
      "         [-2.1102, -0.5238,  2.0453,  ...,  0.6466, -0.7434,  0.7953],\n",
      "         [-2.2204, -1.7145,  1.8153,  ...,  0.7739, -1.7573, -0.0853],\n",
      "         [-2.1437, -0.3642,  1.7688,  ...,  1.0593, -1.1460,  0.5527],\n",
      "         [-1.9082, -1.3415,  1.9150,  ...,  0.8768, -0.4160, -0.1938],\n",
      "         [-1.3761, -1.4266,  1.8673,  ...,  0.8648,  0.0352, -0.4344]]])\n",
      "modified_output:  tensor([[[-1.8500,  0.4044,  1.2790,  ...,  0.6922, -0.2059,  1.2088],\n",
      "         [-2.1102, -0.5238,  2.0453,  ...,  0.6466, -0.7434,  0.7953],\n",
      "         [-2.2204, -1.7145,  1.8153,  ...,  0.7739, -1.7573, -0.0853],\n",
      "         [-2.1437, -0.3642,  1.7688,  ...,  1.0593, -1.1460,  0.5527],\n",
      "         [-1.9082, -1.3415,  1.9150,  ...,  0.8768, -0.4160, -0.1938],\n",
      "         [-1.3761, -1.4266,  1.8673,  ...,  0.8648,  0.0352, -0.4344]]])\n",
      "Final output matches\n"
     ]
    }
   ],
   "source": [
    "def compare_model_params(model1, model2):\n",
    "    # Disable non-deterministic behavior\n",
    "    torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Ensure both models start with the same weights\n",
    "    model2.load_state_dict(model1.state_dict())\n",
    "    model2.h[0].load_state_dict(model1.h[0].state_dict())\n",
    "\n",
    "    # Compare all named parameters\n",
    "    params_dict_original = {name: param for name, param in model1.named_parameters()}\n",
    "    params_dict_modified = {name: param for name, param in model2.named_parameters()}\n",
    "\n",
    "    # Now you can access by name\n",
    "    for name in params_dict_original:\n",
    "        assert torch.allclose(params_dict_original[name], params_dict_modified[name], atol=1e-6), f\"Mismatch found in parameter: {name}\"\n",
    "\n",
    "\n",
    "    def compare_state_dicts(dict1, dict2):\n",
    "        # Check if both dictionaries have the same set of keys (parameter names)\n",
    "        assert dict1.keys() == dict2.keys(), \"State dicts have different sets of parameters.\"\n",
    "        \n",
    "        # Check if all tensors in both dictionaries are close\n",
    "        for key in dict1:\n",
    "            assert torch.allclose(dict1[key], dict2[key], atol=1e-6), f\"Mismatch found in parameter: {key}\"\n",
    "\n",
    "    # Prepare input\n",
    "    input_ids = torch.tensor([[12100, 242, 508, 318, 13, 198]])\n",
    "\n",
    "    hidden_states_original = model1.wte(input_ids)\n",
    "    hidden_states_modified = model2.wte(input_ids)\n",
    "    assert torch.allclose(hidden_states_original, hidden_states_modified, atol=1e-6), \"Embedding output mismatch\"\n",
    "    print(\"Word Token Embeddings output match\")\n",
    "\n",
    "    # modified_model.load_state_dict(original_model.state_dict())\n",
    "    # modified_model.h[0].load_state_dict(original_model.h[0].state_dict())\n",
    "\n",
    "    # Compare state dicts\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    compare_state_dicts(state_dict1, state_dict2)\n",
    "    print(\"State Dicts match\")\n",
    "\n",
    "\n",
    "    def compare_attention_params(model1, model2, layer_index=0):\n",
    "        attn1 = model1.h[layer_index].attn\n",
    "        attn2 = model2.h[layer_index].attn\n",
    "        \n",
    "        # Convert generators to dictionaries\n",
    "        params1 = dict(attn1.named_parameters())\n",
    "        params2 = dict(attn2.named_parameters())\n",
    "\n",
    "        # Now params1 and params2 are dictionaries that can be accessed by keys\n",
    "        for name in params1:\n",
    "            param1 = params1[name]\n",
    "            param2 = params2[name]\n",
    "            if not torch.allclose(param1, param2, atol=1e-6):\n",
    "                print(f\"Mismatch in {name}\")\n",
    "                print(f\"Model1: {param1}\")\n",
    "                print(f\"Model2: {param2}\")\n",
    "\n",
    "    # Call this function with your models\n",
    "    for i in range(12):\n",
    "        compare_attention_params(model1, model2, i)\n",
    "    print(\"Attention parameters match\")\n",
    "\n",
    "\n",
    "    original_h0 = model1.h[0](hidden_states_original)[0]\n",
    "    modified_h0 = model2.h[0](hidden_states_modified)[0]  # Input is the same. These are still different\n",
    "    # modified_h0 = modified_model.h[0](hidden_states_modified)[0]\n",
    "    assert torch.allclose(original_h0, modified_h0, atol=1e-6), \"First Hidden State output mismatch\"\n",
    "    print(\"First Hidden State output match\")\n",
    "\n",
    "    model1.h[0].attn.c_attn.weight\n",
    "\n",
    "\n",
    "    # Get outputs from both models\n",
    "    with torch.no_grad():\n",
    "        original_output = model1(input_ids)[0]\n",
    "        modified_output = model2(input_ids)[0]\n",
    "\n",
    "    # at = model2(input_ids)#.attn\n",
    "    # Compare outputs\n",
    "    # print(\"Difference between original and modified model outputs:\", torch.sum((original_output - modified_output) ** 2))\n",
    "    print(\"original_output: \", original_output)\n",
    "    print(\"modified_output: \", modified_output)\n",
    "\n",
    "    assert torch.allclose(original_output, modified_output, atol=1e-6), \"Final output mismatch\"\n",
    "    print(\"Final output matches\")\n",
    "\n",
    "\n",
    "config = GPT2Config()\n",
    "original_model = GPT2Model(config).eval()\n",
    "# modified_model = GPT2Model(config).eval()  # Assigning h[0] = CustomGPT2Block does not work!\n",
    "modified_model = GPT2TopKModel(config).eval()\n",
    "\n",
    "compare_model_params(original_model, modified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compare_model_perplexity(model1, model2, dataset_name: str = \"Trelis/tiny-shakespeare\"):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    # We need a padding token for DataLoader\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model1 = model1.eval()\n",
    "    model2 = model2.eval()\n",
    "\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    text_data = dataset['train']['Text']\n",
    "\n",
    "    # Tokenization\n",
    "    def encode(text):\n",
    "        return tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")['input_ids']\n",
    "    \n",
    "    input_ids = torch.cat([encode(text) for text in text_data], dim=0)\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    loader = DataLoader(input_ids, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize variables to compute perplexity\n",
    "    total_loss_original = 0\n",
    "    total_loss_modified = 0\n",
    "    total_items = 0\n",
    "\n",
    "    # Disable gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            model1_outputs = model1(batch)\n",
    "            model2_outputs = model2(batch)\n",
    "\n",
    "            # Calculate loss for the batch\n",
    "            shift_logits_original = model1_outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels_original = batch[..., 1:].contiguous()\n",
    "            loss_original = F.cross_entropy(shift_logits_original.view(-1, shift_logits_original.size(-1)), shift_labels_original.view(-1))\n",
    "\n",
    "            shift_logits_modified = model2_outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels_modified = batch[..., 1:].contiguous()\n",
    "            loss_modified = F.cross_entropy(shift_logits_modified.view(-1, shift_logits_modified.size(-1)), shift_labels_modified.view(-1))\n",
    "\n",
    "            total_loss_original += loss_original.item() * batch.size(0)\n",
    "            total_loss_modified += loss_modified.item() * batch.size(0)\n",
    "            total_items += batch.size(0)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity_original = torch.exp(total_loss_original / total_items)\n",
    "    perplexity_modified = torch.exp(total_loss_modified / total_items)\n",
    "\n",
    "    print(f\"Perplexity of Original Model: {perplexity_original}\")\n",
    "    print(f\"Perplexity of Modified Model: {perplexity_modified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LMHead for custom GPT architecture to make it do language tasks\n",
    "# From https://chat.openai.com/share/84b66b97-f257-46a5-8f5b-4afa36965013\n",
    "class GPT2TopKLMHeadModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lm_head = nn.Linear(base_model.config.hidden_size, base_model.config.vocab_size, bias=False)\n",
    "\n",
    "        # You may want to tie the weights as done in the GPT2 pre-trained models\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\" Make sure we are sharing the embeddings and the output weights, tying them \"\"\"\n",
    "        self.lm_head.weight = self.base_model.wte.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        logits = self.lm_head(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "\n",
    "# original_gpt2_with_head = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").eval()\n",
    "\n",
    "original_model = GPT2Model(config)\n",
    "original_model_with_gpt2_head = GPT2LMHeadModel(config)\n",
    "original_model_with_gpt2_head.transformer = original_model\n",
    "\n",
    "original_model_with_gpt2_head = original_model_with_gpt2_head.eval()\n",
    "\n",
    "# These should definitely be identical\n",
    "modified_model = GPT2Model(config)\n",
    "\n",
    "modified_model.load_state_dict(original_model.state_dict())\n",
    "\n",
    "# modified_model = GPT2TopKModel(config)\n",
    "modified_model_with_gpt2_head = GPT2LMHeadModel(config)\n",
    "modified_model_with_gpt2_head.transformer = modified_model\n",
    "\n",
    "modified_model_with_gpt2_head = modified_model_with_gpt2_head.eval()\n",
    "\n",
    "modified_model_with_gpt2_head.lm_head.load_state_dict(original_model_with_gpt2_head.lm_head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_model_with_gpt2_head.lm_head.load_state_dict(original_model_with_gpt2_head.lm_head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Token Embeddings output match\n",
      "State Dicts match\n",
      "Attention parameters match\n",
      "First Hidden State output match\n",
      "original_output:  tensor([[[ 1.3025,  0.0891,  1.5785,  ..., -0.7348,  1.3662, -0.0760],\n",
      "         [ 1.0428, -0.0767,  1.0057,  ..., -1.0282,  2.3909,  0.0945],\n",
      "         [ 0.3019, -0.7627,  0.9617,  ..., -0.5008,  1.7748,  0.4493],\n",
      "         [ 1.0860, -0.4353,  0.8214,  ..., -0.2734,  1.0454, -0.1068],\n",
      "         [ 1.1729, -0.5770,  1.1421,  ..., -0.7705,  0.6708,  0.8564],\n",
      "         [ 1.0173, -0.1443,  1.4305,  ..., -1.1474,  1.7968,  0.4672]]])\n",
      "modified_output:  tensor([[[ 1.3025,  0.0891,  1.5785,  ..., -0.7348,  1.3662, -0.0760],\n",
      "         [ 1.0428, -0.0767,  1.0057,  ..., -1.0282,  2.3909,  0.0945],\n",
      "         [ 0.3019, -0.7627,  0.9617,  ..., -0.5008,  1.7748,  0.4493],\n",
      "         [ 1.0860, -0.4353,  0.8214,  ..., -0.2734,  1.0454, -0.1068],\n",
      "         [ 1.1729, -0.5770,  1.1421,  ..., -0.7705,  0.6708,  0.8564],\n",
      "         [ 1.0173, -0.1443,  1.4305,  ..., -1.1474,  1.7968,  0.4672]]])\n",
      "Final output matches\n"
     ]
    }
   ],
   "source": [
    "compare_model_params(original_model, modified_model) # so this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[50256, 318, 262, 263, 734, 262, 1576, 50256]])  # Random example tokens\n",
    "\n",
    "# Generate outputs with hidden states and attentions\n",
    "outputs_original = original_model_with_gpt2_head(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "outputs_modified = modified_model_with_gpt2_head(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "# Accessing the logits, hidden states, and attentions\n",
    "logits_original, hidden_states_original, attentions_original = outputs_original.logits, outputs_original.hidden_states, outputs_original.attentions\n",
    "logits_modified, hidden_states_modified, attentions_modified = outputs_modified.logits, outputs_modified.hidden_states, outputs_modified.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(logits_original, logits_modified, atol=1e-6)  # This should be True\n",
    "torch.allclose(hidden_states_original[0], hidden_states_modified[0])\n",
    "torch.allclose(hidden_states_original[1], hidden_states_modified[1])\n",
    "torch.allclose(attentions_original[0], attentions_modified[0])\n",
    "torch.allclose(attentions_original[1], attentions_modified[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5882, -0.1566,  0.9692,  ...,  1.6813,  0.2022,  0.7559],\n",
       "         [-0.8870, -0.0666,  0.8193,  ...,  1.7804, -0.2431, -0.2938],\n",
       "         [-0.0335,  0.2815,  0.4804,  ...,  1.5229, -0.0332,  0.3242],\n",
       "         ...,\n",
       "         [-0.4287, -0.7156,  0.6954,  ...,  0.8741, -0.5616,  0.4157],\n",
       "         [-0.4121, -0.3123,  0.4581,  ...,  0.6161,  0.0355,  0.3238],\n",
       "         [ 0.3711,  0.3823, -0.2987,  ...,  1.0961, -0.8531,  0.6987]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5882, -0.1566,  0.9692,  ...,  1.6813,  0.2022,  0.7559],\n",
       "         [-0.8870, -0.0666,  0.8193,  ...,  1.7804, -0.2431, -0.2938],\n",
       "         [-0.0335,  0.2815,  0.4804,  ...,  1.5229, -0.0332,  0.3242],\n",
       "         ...,\n",
       "         [-0.4287, -0.7156,  0.6954,  ...,  0.8741, -0.5616,  0.4157],\n",
       "         [-0.4121, -0.3123,  0.4581,  ...,  0.6161,  0.0355,  0.3238],\n",
       "         [ 0.3711,  0.3823, -0.2987,  ...,  1.0961, -0.8531,  0.6987]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_model(model1_with_head, model2_with_head):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "    prompt = \"What is the capital of France?\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate responses\n",
    "    output_sequences_1 = model1_with_head.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "    output_sequences_2 = model2_with_head.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "    # Decode generated sequences to text\n",
    "    generated_text_1 = tokenizer.decode(output_sequences_1[0], skip_special_tokens=True)\n",
    "    generated_text_2 = tokenizer.decode(output_sequences_2[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Response from Model 1:\", generated_text_1)\n",
    "    print(\"Response from Model 2:\", generated_text_2)\n",
    "\n",
    "    input_ids = torch.tensor([[50256, 318, 262, 263, 734, 262, 1576, 50256]])  # Random example tokens\n",
    "\n",
    "    # Generate outputs with hidden states and attentions\n",
    "    outputs_original = model1_with_head(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "    outputs_modified = model2_with_head(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "    # Accessing the logits, hidden states, and attentions\n",
    "    logits_original, hidden_states_original, attentions_original = outputs_original.logits, outputs_original.hidden_states, outputs_original.attentions\n",
    "    logits_modified, hidden_states_modified, attentions_modified = outputs_modified.logits, outputs_modified.hidden_states, outputs_modified.attentions\n",
    "\n",
    "    print(\"Comparing outputs of the two models with torch.allclose()\")\n",
    "    assert torch.allclose(logits_original, logits_modified, atol=1e-6), \"Logits DO NOT match\" \n",
    "    assert torch.allclose(hidden_states_original[0], hidden_states_modified[0]), \"Hidden States DO NOT match\"\n",
    "    assert torch.allclose(hidden_states_original[1], hidden_states_modified[1]), \"Hidden States DO NOT match\"\n",
    "    assert torch.allclose(attentions_original[0], attentions_modified[0]), \"Attentions DO NOT match\"\n",
    "    assert torch.allclose(attentions_original[1], attentions_modified[1]), \"Attentions DO NOT match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model 1: What is the capital of France?MajorMajorMajor Duty TelecommunicationsMajorather TelecommunicationsRPG 234RPG TestsRPG mouthHourRPG mouth maintenance420 deteriorating Duty Telecommunications420TexRPGorth petsoptions Trent Anonymous Duty Documents bip Documents TelecommunicationsCoach Erica tool slaves contingent restrictionDb Telecommunications\n",
      "Response from Model 2: What is the capital of France?ontontontontracticalontracticalontracticalracticalracticalracticalractical applicableont burgeront burgerontontontracticalont burger capabilitiesont 287ontracticalracticalracticalracticalracticalracticalGirlontontontontontontontractical\n"
     ]
    }
   ],
   "source": [
    "sanity_check_model(original_model_with_gpt2_head, modified_model_with_gpt2_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try initializing from Pretrained model. The previous output is gibberish, though identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "\n",
    "original_gpt2_with_head = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").eval()\n",
    "\n",
    "# These should definitely be identical\n",
    "# modified_model = GPT2Model(config)\n",
    "modified_model = GPT2TopKModel(config)\n",
    "\n",
    "# Crucial to copy the state if we want same starting parameters\n",
    "modified_model.load_state_dict(original_gpt2_with_head.transformer.state_dict())\n",
    "\n",
    "modified_model_with_gpt2_head = GPT2LMHeadModel(config)\n",
    "modified_model_with_gpt2_head.transformer = modified_model\n",
    "\n",
    "modified_model_with_gpt2_head = modified_model_with_gpt2_head.eval()\n",
    "\n",
    "modified_model_with_gpt2_head.lm_head.load_state_dict(original_gpt2_with_head.lm_head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model 1: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is\n",
      "Response from Model 2: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is\n",
      "Comparing outputs of the two models with torch.allclose()\n"
     ]
    }
   ],
   "source": [
    "sanity_check_model(original_gpt2_with_head, modified_model_with_gpt2_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full perplexity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompare_model_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_gpt2_with_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified_model_with_gpt2_head\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mcompare_model_perplexity\u001b[0;34m(model1, model2, dataset_name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m---> 33\u001b[0m         model1_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         model2_outputs \u001b[38;5;241m=\u001b[39m model2(batch)\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# Calculate loss for the batch\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1119\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         output_attentions,\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:654\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    652\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 654\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:574\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 574\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compare_model_perplexity(original_gpt2_with_head, modified_model_with_gpt2_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting & Heatmapping LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_top_n_tiny_shakespeare\n",
    "\n",
    "longest_shakespeare = get_top_n_tiny_shakespeare(1, mode=\"longest\")[0]['Text']\n",
    "longest_shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(longest_shakespeare, return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract LayerNorm inputs\n",
    "ln_inputs = outputs[-1][-1]\n",
    "\n",
    "plt.imshow(ln_inputs[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
