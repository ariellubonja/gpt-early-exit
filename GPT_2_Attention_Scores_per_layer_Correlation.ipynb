{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WNbCeIUc7i4y",
        "pNh8E2Mhoy4Y",
        "PsCJpojTg2fH",
        "9JEZN5sL1N5l"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Original code"
      ],
      "metadata": {
        "id": "WNbCeIUc7i4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DejaVu Section 3.3 does exactly this"
      ],
      "metadata": {
        "id": "NleeKhkWA9Z3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWM2wWQ4sMv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c759f62-19f6-47cf-fb80-5c19f98ed4a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2Model.from_pretrained('gpt2-medium', output_attentions=True, output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"ariel is handsome\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "wfZ_xITazOW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "attentions = outputs.attentions\n",
        "hidden_states = outputs.hidden_states"
      ],
      "metadata": {
        "id": "pAMQCOCBzP55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_hidden_state = hidden_states[4]"
      ],
      "metadata": {
        "id": "gV7m7RxblfHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the lm_head to project the hidden states to the vocabulary space\n",
        "lm_head = model.lm_head\n",
        "logits = lm_head(selected_hidden_state)\n",
        "\n",
        "# Focus on the last token's logits for next token prediction\n",
        "next_token_logits = logits[:, -1, :]\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "import torch.nn.functional as F\n",
        "probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "# Get the most probable next token ID\n",
        "predicted_token_id = probabilities.argmax(dim=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id.tolist())\n",
        "\n",
        "print(f\"Predicted next token: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "4RLjC2t3lj5I",
        "outputId": "540aaa3c-1864-4c83-f868-4821c60eb7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GPT2Model' object has no attribute 'lm_head'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5e26557df93>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply the lm_head to project the hidden states to the vocabulary space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Focus on the last token's logits for next token prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPT2Model' object has no attribute 'lm_head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model with the LM head\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', output_hidden_states=True)"
      ],
      "metadata": {
        "id": "pioZbHz-mS-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode some input text\n",
        "inputs = tokenizer(\"Y/N: the capital of france is paris:   \", return_tensors=\"pt\")\n",
        "\n",
        "print(\"Input IDs:\", inputs['input_ids'])\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist()))"
      ],
      "metadata": {
        "id": "qtPa-5L5mDOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model outputs\n",
        "outputs = model(**inputs)\n",
        "hidden_states = outputs.hidden_states\n",
        "\n",
        "# Select the output from an earlier block, for instance, the 5th block\n",
        "selected_hidden_state = hidden_states[4]  # Using zero indexing; adjust as necessary\n",
        "\n",
        "# Apply the lm_head to project the hidden states to the vocabulary space\n",
        "logits = model.lm_head(selected_hidden_state)\n",
        "\n",
        "print(\"logits: \", logits)"
      ],
      "metadata": {
        "id": "j-XkDriCnEhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "id": "YOXRkdGTnQs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on the last token's logits for next token prediction\n",
        "next_token_logits = logits[:, -1, :]\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "import torch.nn.functional as F\n",
        "probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "print(\"probabilities: \", probabilities)"
      ],
      "metadata": {
        "id": "Px-jU6sZnOCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities.shape"
      ],
      "metadata": {
        "id": "jFNEoDd1nlbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_token_id"
      ],
      "metadata": {
        "id": "tIUDvUvhnpjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities[0][predicted_token_id.item()]"
      ],
      "metadata": {
        "id": "6nl-BlXNnrKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the most probable next token ID\n",
        "predicted_token_id = probabilities.argmax(dim=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id.tolist())\n",
        "\n",
        "print(f\"Predicted next token: {predicted_token}\")"
      ],
      "metadata": {
        "id": "UyEhw8f-niOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The capital of France is Paris: Y/N?\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "num_tokens_to_generate = 25\n",
        "\n",
        "for _ in range(num_tokens_to_generate):\n",
        "    # Get model outputs\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Only use the logits from the last token position\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "    # Append the predicted token ID to the input sequence\n",
        "    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
        "\n",
        "# Decode the input IDs to a string\n",
        "generated_text = tokenizer.decode(input_ids[0])\n",
        "\n",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "id": "DwAmbYdqo5xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(231)"
      ],
      "metadata": {
        "id": "eAstlwMQoSu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT-2 regular, 13 hidden states\n",
        "\n",
        "- GPT-2-medium, 25 hidden states"
      ],
      "metadata": {
        "id": "Lc4Y_uh2kvXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(hidden_states)"
      ],
      "metadata": {
        "id": "31tpt0-8hML7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states[12].shape"
      ],
      "metadata": {
        "id": "gT2qtCFrkdBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24 `attentions` bcs. there are 24 attention blocks in gpt-2-medium"
      ],
      "metadata": {
        "id": "pNh8E2Mhoy4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(attentions)"
      ],
      "metadata": {
        "id": "yzCsgL__og1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attentions[0].shape"
      ],
      "metadata": {
        "id": "Kok_zWWfzaVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attentions[1].shape"
      ],
      "metadata": {
        "id": "TKXMyqc9zRMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attentions[0][0].shape"
      ],
      "metadata": {
        "id": "ghvf0HPMzR8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Scores at first layer, for all heads"
      ],
      "metadata": {
        "id": "PsCJpojTg2fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attentions[0][0].shape[0]"
      ],
      "metadata": {
        "id": "AW5gLxtvv3H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Number of rows and columns for the subplot grid\n",
        "n_rows = 4\n",
        "n_cols = 4\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 10)) # Adjust figsize as needed\n",
        "\n",
        "# Flatten the axes array for easy indexing\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(attentions[0][0].shape[0]):\n",
        "    # Select the appropriate subplot\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Plot the attention map on the chosen subplot\n",
        "    im = ax.imshow(attentions[0][0][i].detach().numpy())\n",
        "\n",
        "    # Optional: Add a colorbar and set other properties\n",
        "    fig.colorbar(im, ax=ax)\n",
        "\n",
        "# Adjust layout to prevent overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VLTEuUaz2W3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson Correlation between rows, across blocks\n",
        "\n",
        "Rows because rows represent attention scores per token, and softmax makes them add up to 1. Columns don't mean much"
      ],
      "metadata": {
        "id": "e4dzaxd5uh_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "WV9lm9auxGYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_head_0_layer_0 = attentions[0][0][0]  # Layer, Batch, attn. Head\n",
        "attn_head_0_layer_1 = attentions[1][0][0]\n",
        "attn_head_0_layer_0.shape"
      ],
      "metadata": {
        "id": "Cw1JrbH6vTlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_head_0_layer_1"
      ],
      "metadata": {
        "id": "Y1qxqLmlwUBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_head_0_layer_0[0,:]"
      ],
      "metadata": {
        "id": "j8scwmgFwVBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearsons_r_1_2 = []\n",
        "for row in range(attn_head_0_layer_0.shape[0]):\n",
        "    left_layer_vec = attn_head_0_layer_0[row][:].detach().numpy() # Tells PyTorch to forget about gradient info\n",
        "    right_layer_vec = attn_head_0_layer_1[row][:].detach().numpy()\n",
        "    pearsons_r_1_2.append(pearsonr(left_layer_vec, right_layer_vec))"
      ],
      "metadata": {
        "id": "n6gwiwmAvKKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearsons_r_1_2"
      ],
      "metadata": {
        "id": "_QO3Sty6w-J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(attentions)"
      ],
      "metadata": {
        "id": "A0yJcz1Cx0bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do this for all trasnformer blocks\n",
        "# Calculate correlation of neighboring transformer blocks\n",
        "# Correlation is row-wise\n",
        "\n",
        "# TODO This is for first head ONLY!\n",
        "\n",
        "def calculate_pearsons_per_head(head_nr):\n",
        "    all_pearsons = []\n",
        "    for block in range(len(attentions) - 1):\n",
        "        pearsons_r = []\n",
        "\n",
        "        left_layer = attentions[block][0][head_nr]\n",
        "        right_layer = attentions[block + 1][0][head_nr]\n",
        "\n",
        "        for row in range(left_layer.shape[0]):\n",
        "            left_layer_vec = left_layer[row][:].detach().numpy() # .detach().numpy() Tells PyTorch to forget about gradient info\n",
        "            right_layer_vec = right_layer[row][:].detach().numpy()\n",
        "            pearsons_r.append(pearsonr(left_layer_vec, right_layer_vec))\n",
        "\n",
        "        all_pearsons.append(pearsons_r)\n",
        "\n",
        "    return all_pearsons"
      ],
      "metadata": {
        "id": "47sHJu3Pxuwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_pearsons)"
      ],
      "metadata": {
        "id": "oBRJppz6zY2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pearsons[0]"
      ],
      "metadata": {
        "id": "kYJ8wcP6zaXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearsons_statistic = np.array(all_pearsons)[:,:,0]\n",
        "pearsons_p_value = np.array(all_pearsons)[:,:,1]\n",
        "\n",
        "pearsons_statistic"
      ],
      "metadata": {
        "id": "DsdmPKVvzMV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(pearsons_statistic)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "ho-0G1N4zPuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(pearsons_p_value)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "RZ9Dbhbr0qJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's calculate corr. between a layer and all previous layers"
      ],
      "metadata": {
        "id": "9JEZN5sL1N5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pearsons_autoregressive(head_nr: int = 0):\n",
        "    ar_pearsons_r = []\n",
        "    ar_pearsons_p_val = []\n",
        "    for block in range(len(attentions) - 1, 1, -1):\n",
        "        block_pearsons_r = []\n",
        "        block_pearsons_p_val = []\n",
        "\n",
        "        right_layer = attentions[block][0][head_nr]\n",
        "\n",
        "        for l in range(0, block):\n",
        "            pearsons_r = []\n",
        "            pearsons_p_val = []\n",
        "            left_layer = attentions[l][0][head_nr]\n",
        "\n",
        "            for row in range(left_layer.shape[0]):\n",
        "                left_layer_vec = left_layer[row][:].detach().numpy() # .detach().numpy() Tells PyTorch to forget about gradient info\n",
        "                right_layer_vec = right_layer[row][:].detach().numpy()\n",
        "\n",
        "                pearsons = pearsonr(left_layer_vec, right_layer_vec)\n",
        "\n",
        "                pearsons_r.append(pearsons[0])\n",
        "                pearsons_p_val.append(pearsons[1])\n",
        "\n",
        "            block_pearsons_r.append(pearsons_r)\n",
        "            block_pearsons_p_val.append(pearsons_p_val)\n",
        "\n",
        "        ar_pearsons_r.append(block_pearsons_r)\n",
        "        ar_pearsons_p_val.append(block_pearsons_p_val)\n",
        "\n",
        "\n",
        "    return ar_pearsons_r, ar_pearsons_p_val"
      ],
      "metadata": {
        "id": "tX8YTnKg1Rzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar_pearsons_r, ar_pearsons_p_val = calculate_pearsons_autoregressive()"
      ],
      "metadata": {
        "id": "896_n63A2qRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ar_pearsons_r[0])"
      ],
      "metadata": {
        "id": "2TZNkHTr2scd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ar_pearsons_r)"
      ],
      "metadata": {
        "id": "ytBG9i9U456_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar_pearsons_r"
      ],
      "metadata": {
        "id": "kuJ5cCRI3MEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(ar_pearsons_r[0])\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "9ewi3zRC5QC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(ar_pearsons_r[21])\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "8dd-xzs_6D8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar_pearsons_r[0]"
      ],
      "metadata": {
        "id": "6qizD-wd6sgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ar_pearsons_r[0])"
      ],
      "metadata": {
        "id": "Jq9N_9Fe5q0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(attentions)"
      ],
      "metadata": {
        "id": "hfT0WaPE5sF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaned code"
      ],
      "metadata": {
        "id": "qmSqLQV27bd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model with the LM head\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', output_hidden_states=True)"
      ],
      "metadata": {
        "id": "xrzjTZ9Q73XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode some input text\n",
        "inputs = \"Y/N: the capital of france is paris:\"\n",
        "input2 = \"testing: the united states is a country in \""
      ],
      "metadata": {
        "id": "2YroMs8V76TO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict token for different inputs"
      ],
      "metadata": {
        "id": "XYVMMix36tvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_from_early_exit(input_prompt, exit_layer=2, print_debug_stmts=False, num_tokens_to_generate = 25):\n",
        "    # Exit model from the early transformer blocks instead of computing all few dozen\n",
        "    # This should give a good estimate of the final token, at a fraction of the cost\n",
        "    inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
        "\n",
        "    if print_debug_stmts:\n",
        "        print(\"Input IDs:\", inputs['input_ids'])\n",
        "        print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist()))\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    attentions = outputs.attentions\n",
        "    hidden_states = outputs.hidden_states\n",
        "    print(\"Nr. of hidden states: \", len(outputs.hidden_states))\n",
        "\n",
        "    early_exit_state = outputs.hidden_states[exit_layer]\n",
        "\n",
        "    logits = model.lm_head(selected_hidden_state)\n",
        "    probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    predicted_token_id = probabilities.argmax(dim=-1)\n",
        "    predicted_token = tokenizer.decode(predicted_token_id.tolist())\n",
        "\n",
        "    print(f\"Predicted next token: {predicted_token}\")"
      ],
      "metadata": {
        "id": "N3w7I0os65w1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_from_early_exit(input2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd2QBFnM8cle",
        "outputId": "cd2fdab9-dfe7-490b-f4b7-e292a3b16bed"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nr. of hidden states:  13\n",
            "Predicted next token:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(hidden2))\n",
        "\n",
        "# Select the output from an earlier block, for instance, the 5th block\n",
        "selected_hidden_state = hidden_states[1]  # Using zero indexing; adjust as necessary\n",
        "another_hidden_state = hidden_states[12]\n",
        "\n",
        "# Apply the lm_head to project the hidden states to the vocabulary space\n",
        "logits = model.lm_head(selected_hidden_state)\n",
        "logits12 = model.lm_head(another_hidden_state)\n",
        "\n",
        "# assert [logits[i]==logits12[i] for i in range(len(logits))]\n",
        "# print(\"logits: \", logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0adkwtEn79Jm",
        "outputId": "62915c04-3360-4cc7-cfbb-dedd221447ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5DV6_GQ8C33",
        "outputId": "0082a647-bb07-4269-f791-bae0ab0a99c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 13, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on the last token's logits for next token prediction\n",
        "next_token_logits = logits[:, -1, :]\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "import torch.nn.functional as F\n",
        "probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "print(\"probabilities: \", probabilities)"
      ],
      "metadata": {
        "id": "KoXbyvAP8F6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a7d54a-6355-4bba-e0ee-cff199ad7086"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probabilities:  tensor([[1.9838e-10, 1.7317e-08, 8.4721e-14,  ..., 2.5343e-24, 2.1858e-22,\n",
            "         3.0620e-09]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities.shape"
      ],
      "metadata": {
        "id": "qtnSOrMP8GTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a368b84-7994-438e-ce64-b7e2b11913dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the most probable next token ID\n",
        "predicted_token_id = probabilities.argmax(dim=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id.tolist())\n",
        "\n",
        "print(f\"Predicted next token: {predicted_token}\")"
      ],
      "metadata": {
        "id": "sW8YnURv8QdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500227be-f004-4e82-952a-edb3df66661f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next token:  the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The capital of France is Paris: Y/N?\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "num_tokens_to_generate = 25\n",
        "\n",
        "for _ in range(num_tokens_to_generate):\n",
        "    # Get model outputs\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Only use the logits from the last token position\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "    # Append the predicted token ID to the input sequence\n",
        "    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
        "\n",
        "# Decode the input IDs to a string\n",
        "generated_text = tokenizer.decode(input_ids[0])\n",
        "\n",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "id": "suqrIcoT8Str",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56284e0e-068a-49d5-85b0-7f7a4a06ec78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: The capital of France is Paris: Y/N?\n",
            "\n",
            "The French capital is Paris: Y/N? The French capital is Paris: Y/N? The French capital\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states[12].shape"
      ],
      "metadata": {
        "id": "ekJzjggn8dOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae1573f-ccd2-475b-dc3c-55fe4c4b3f1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 13, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the tokenizer and model with the LM head\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', output_hidden_states=True)\n",
        "\n",
        "# Encode some input text\n",
        "inputs = tokenizer(\"Y/N: the capital of france is paris:   \", return_tensors=\"pt\")\n",
        "\n",
        "# Get model outputs\n",
        "outputs = model(**inputs)\n",
        "hidden_states = outputs.hidden_states\n",
        "\n",
        "# Select the output from an earlier block, for instance, the 5th block\n",
        "selected_hidden_state = hidden_states[12]  # Using zero indexing; adjust as necessary\n",
        "\n",
        "# Apply the lm_head to project the hidden states to the vocabulary space\n",
        "logits = model.lm_head(selected_hidden_state)\n",
        "\n",
        "# Focus on the last token's logits for next token prediction\n",
        "next_token_logits = logits[:, -1, :]\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "print(probabilities)\n",
        "print(len(probabilities))\n",
        "\n",
        "# get sorted list of indices\n",
        "# ranked_indices = torch.argsort(probabilities)\n",
        "values, indices = probabilities.topk(3)\n",
        "print(values)\n",
        "print(indices)\n",
        "\n",
        "# Get the most probable next tokens\n",
        "for i in indices:\n",
        "  predicted_token = tokenizer.decode(i)\n",
        "  print(\"token: \", predicted_token)\n"
      ],
      "metadata": {
        "id": "Rq9lh21v_M5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48db38be-7882-4d2e-b74e-affae9fb2b9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.6054e-04, 4.2071e-04, 1.3583e-04,  ..., 2.0722e-07, 1.0056e-06,\n",
            "         3.7878e-03]], grad_fn=<SoftmaxBackward0>)\n",
            "1\n",
            "tensor([[0.2658, 0.1268, 0.0381]], grad_fn=<TopkBackward0>)\n",
            "tensor([[ 220, 1849,  198]])\n",
            "token:   Â \n",
            "\n"
          ]
        }
      ]
    }
  ]
}