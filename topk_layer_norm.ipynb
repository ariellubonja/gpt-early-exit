{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's actually apply Top-K and compare Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from torch import nn\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "import copy\n",
    "\n",
    "class ShowIntermediateOutputsGPT2Block(GPT2Block):\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__(config, layer_idx=None)\n",
    "        self.intermediate_outputs = {}\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        # Dictionary to store all intermediate outputs\n",
    "        residual = hidden_states  # Identical debug to GPT2Block so far\n",
    "\n",
    "        self.intermediate_outputs['initial_residual'] = residual.detach().clone()\n",
    "        self.intermediate_outputs['initial_hidden_states'] = hidden_states.detach().clone()\n",
    "\n",
    "        hidden_states = self.ln_1(hidden_states) # Also identical\n",
    "        \n",
    "        # Apply Top-K to hidden_states\n",
    "        \n",
    "    \n",
    "        # Store initial hidden states\n",
    "        self.intermediate_outputs['post_ln1_residual'] = residual.detach().clone()\n",
    "        self.intermediate_outputs['post_ln1_hidden_states'] = hidden_states.detach().clone()\n",
    "\n",
    "        \n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]  # Why is attn_output and outputs different? What do they mean?\n",
    "\n",
    "        self.intermediate_outputs['attn_projection_output'] = attn_output.detach().clone()\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "        self.intermediate_outputs['post_attn_residual_hidden_states'] = hidden_states.detach().clone()\n",
    "\n",
    "        # Where is * W0? Is it inside self.attn?\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        self.intermediate_outputs['post_cross_attn_residual'] = residual.detach().clone()\n",
    "        self.intermediate_outputs['post_cross_attn_hidden_states'] = hidden_states.detach().clone()\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "\n",
    "        self.intermediate_outputs['post_ln2_residual'] = residual.detach().clone()\n",
    "        self.intermediate_outputs['post_ln2_hidden_states'] = hidden_states.detach().clone()\n",
    "\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        self.intermediate_outputs['post_feed_fwd_hidden_states'] = hidden_states.detach().clone()\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "        self.intermediate_outputs['post_feed_fwd_residual_hidden_states'] = hidden_states.detach().clone()\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)\n",
    "\n",
    "\n",
    "class GPT2IntermediateOutputsModel(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.h = nn.ModuleList([ShowIntermediateOutputsGPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        self.all_intermediate_outputs = []"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
